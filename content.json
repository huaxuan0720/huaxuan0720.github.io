{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Adaboost算法","text":"前言 提升方法（boosting）是一种常用的机器学习方法，应用十分广泛，而且效果非常好，近几年的很多比赛的优胜选手都或多或少使用了提升方法用以提高自己的成绩。 提升方法的本质是通过对每一个训练样本赋予一个权重，并通过改变这些样本的权重，来学习多个分类器，并按照一定的算法将这些分类器组合在一起，通常是线性组合，因为单个分类器往往效果有限，因此组合多个分类器往往会提高模型的性能。 一、提升方法简介 提升方法（boosting）实际上是集成学习方法的一种，其基于这样的一种朴素思想：对于一个复杂的任务来说，将多个“专家”（这里的“专家”本意是指各种机器学习模型，可以较好地满足实际问题的需要）的意见进行适当的整合，进而得出最后的综合的判断，比其中任何一个单一的“专家”给出的判断要好。实际上，也就是“三个臭皮匠顶过诸葛亮”的意思。因此，boousting的本意就是寻找到合适的“臭皮匠”。 在实际的数据处理的过程中，我们往往可以很容易地发现各种各样的弱机器学习模型，这些模型仅仅比随机猜测好一些，但是还远远不能满足实际作业的精度要求。但是要寻找到一个单一的十分强大的机器学习模型往往会十分困难，虽然可以很好的满足要求，但是寻找这样的模型并不容易。不过好在，我们可以通过整合之前发现的弱机器学习模型，来进行综合考虑，从而形成一个可以媲美单一的强大的机器学习模型。这些弱机器学习模型往往被称之为“弱学习方法”，强机器学习模型往往被称之为“强学习方法”。 二、AdaBoost算法 1、AdaBoost算法的过程 2、AdaBoost的使用","link":"/2019/05/10/Note1-AdaBoost/"},{"title":"YOLOv3源码阅读：get_kmeans.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master\\get_kmeans.py 这里函数的主要作用是使用kmeans聚类产生若干个anchors中心，在训练的时候使用这些作为一种先验条件。这里的聚类主要是对目标检测框的尺寸进行聚类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177# coding: utf-8# This script is modified from https://github.com/lars76/kmeans-anchor-boxesfrom __future__ import division, print_functionimport numpy as np# 计算IOU，box一个长度为2的数组，表示box的尺寸，clusters表示的是若干集群的中心，同样也是尺寸。def iou(box, clusters): \"\"\" Calculates the Intersection over Union (IoU) between a box and k clusters. param: box: tuple or array, shifted to the origin (i. e. width and height) clusters: numpy array of shape (k, 2) where k is the number of clusters return: numpy array of shape (k, 0) where k is the number of clusters \"\"\" x = np.minimum(clusters[:, 0], box[0]) y = np.minimum(clusters[:, 1], box[1]) if np.count_nonzero(x == 0) &gt; 0 or np.count_nonzero(y == 0) &gt; 0: raise ValueError(\"Box has no area\") intersection = x * y box_area = box[0] * box[1] cluster_area = clusters[:, 0] * clusters[:, 1] iou_ = intersection / (box_area + cluster_area - intersection + 1e-10) return iou_def avg_iou(boxes, clusters): \"\"\" Calculates the average Intersection over Union (IoU) between a numpy array of boxes and k clusters. param: boxes: numpy array of shape (r, 2), where r is the number of rows clusters: numpy array of shape (k, 2) where k is the number of clusters return: average IoU as a single float \"\"\" # 计算平均IOU return np.mean([np.max(iou(boxes[i], clusters)) for i in range(boxes.shape[0])])# 这个函数并未在任何地方被使用def translate_boxes(boxes): \"\"\" Translates all the boxes to the origin. param: boxes: numpy array of shape (r, 4) return: numpy array of shape (r, 2) \"\"\" new_boxes = boxes.copy() for row in range(new_boxes.shape[0]): new_boxes[row][2] = np.abs(new_boxes[row][2] - new_boxes[row][0]) new_boxes[row][3] = np.abs(new_boxes[row][3] - new_boxes[row][1]) return np.delete(new_boxes, [0, 1], axis=1)def kmeans(boxes, k, dist=np.median): \"\"\" Calculates k-means clustering with the Intersection over Union (IoU) metric. param: boxes: numpy array of shape (r, 2), where r is the number of rows k: number of clusters dist: distance function return: numpy array of shape (k, 2) \"\"\" # rows表示的是数据集中一共有多少个标注框 rows = boxes.shape[0] # 初始化统计距离的矩阵和每一个标注框的所属集群编号， # 这里使用last cluster记录下一轮循环开始时标注框的集群编号，如果在这某一轮的迭代中不发生改变则算法已经收敛。 distances = np.empty((rows, k)) last_clusters = np.zeros((rows,)) np.random.seed() # the Forgy method will fail if the whole array contains the same rows # 随机选择几个数据作为初始的集群中心 clusters = boxes[np.random.choice(rows, k, replace=False)] # 循环 while True: # 对每一个标注框,计算其与每个集群中心的距离,这里的距离采用的是(1 - 标注框与集群中心的IOU)来表示, # IOU数值越大, 则(1- IOU)越小， 则表示距离越接近. for row in range(rows): distances[row] = 1 - iou(boxes[row], clusters) # 对每个标注框选择与其距离最接近的集群中心的标号作为所属类别的编号。 nearest_clusters = np.argmin(distances, axis=1) # 如果在这轮循环中所有的标注框的所属类别不再变化，则说明算法已经收敛，可以跳出循环。 if (last_clusters == nearest_clusters).all(): break # 对每一类集群，取出所有属于该集群的数据，并按照给定的方法计算集群的中心， # 这里默认采用中位数的方法来计算集群中心 for cluster in range(k): clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0) # 更新每一个标注框所属的集群类别。 last_clusters = nearest_clusters # 返回所有的集群中心 return clustersdef parse_anno(annotation_path): # 打开数据标记的文件 anno = open(annotation_path, 'r') # 用以储存最后的提取出的所有的高度和宽度的结果， result = [] # 对每一个标记图片 for line in anno: # 根据空格将数据行进行分割 s = line.strip().split(' ') # 按照数据的标记规则，每一行的第一个数据是编号，第二个数据是图片地址，从第三个开始才是标记框的信息。 s = s[2:] # 当前图片的标记框的数目，每个标记框包含五个信息，四个坐标信息和一个类别信息 box_cnt = len(s) // 5 # 分别处理每一个标记框的信息，并提取标记框的高度和宽度，存入result 列表。 for i in range(box_cnt): x_min, y_min, x_max, y_max = float(s[i*5+1]), float(s[i*5+2]), float(s[i*5+3]), float(s[i*5+4]) width = x_max - x_min height = y_max - y_min assert width &gt; 0 assert height &gt; 0 result.append([width, height]) # 将list变为numpy的数组 result = np.asarray(result) # 返回 return resultdef get_kmeans(anno, cluster_num=9): # 使用kmeans算法计算需要的anchors anchors = kmeans(anno, cluster_num) # 计算平均IOU ave_iou = avg_iou(anno, anchors) # 格式化为int类型 anchors = anchors.astype('int').tolist() # 按照面积大小排序， anchors = sorted(anchors, key=lambda x: x[0] * x[1]) # 返回 return anchors, ave_iouif __name__ == '__main__': annotation_path = \"./data/my_data/train.txt\" anno_result = parse_anno(annotation_path) anchors, ave_iou = get_kmeans(anno_result, 9) # 格式化输出anchors数据 anchor_string = '' for anchor in anchors: anchor_string += '{},{}, '.format(anchor[0], anchor[1]) anchor_string = anchor_string[:-2] print('anchors are:') print(anchor_string) print('the average iou is:') print(ave_iou)","link":"/2019/05/22/Note10-YOLOv3-part02/"},{"title":"YOLOv3源码阅读：layer_utils.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master_utils.py 这里函数的主要作用是对卷积等操作做出一定的个性化封装，方便代码的编写。主要包括： 卷积的封装 darknet网络结构的定义 resize的定义，默认是最近邻方法 在主体网络的基础上做的YOLO的附加的卷积操作，为后面的特征融合做准备 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# coding: utf-8from __future__ import division, print_functionimport numpy as npimport tensorflow as tfslim = tf.contrib.slimdef conv2d(inputs, filters, kernel_size, strides=1): # 对conv2d做一定的个性化封装，方便代码的编写和阅读 def _fixed_padding(inputs, kernel_size): pad_total = kernel_size - 1 pad_beg = pad_total // 2 pad_end = pad_total - pad_beg padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]], mode='CONSTANT') return padded_inputs if strides &gt; 1: inputs = _fixed_padding(inputs, kernel_size) inputs = slim.conv2d(inputs, filters, kernel_size, stride=strides, padding=('SAME' if strides == 1 else 'VALID')) return inputsdef darknet53_body(inputs): \"\"\" darknet的主体网络框架 :param inputs: :return: 三张不同尺度的特征图 \"\"\" def res_block(inputs, filters): shortcut = inputs net = conv2d(inputs, filters * 1, 1) net = conv2d(net, filters * 2, 3) net = net + shortcut return net # first two conv2d layers net = conv2d(inputs, 32, 3, strides=1) net = conv2d(net, 64, 3, strides=2) # res_block * 1 net = res_block(net, 32) net = conv2d(net, 128, 3, strides=2) # res_block * 2 for i in range(2): net = res_block(net, 64) net = conv2d(net, 256, 3, strides=2) # res_block * 8 for i in range(8): net = res_block(net, 128) route_1 = net net = conv2d(net, 512, 3, strides=2) # res_block * 8 for i in range(8): net = res_block(net, 256) route_2 = net net = conv2d(net, 1024, 3, strides=2) # res_block * 4 for i in range(4): net = res_block(net, 512) route_3 = net return route_1, route_2, route_3def yolo_block(inputs, filters): \"\"\" 在darknet主体网络提取特征的基础上增加的若干卷积层，为了后面的特征融合做准备 :param inputs: :param filters: :return: \"\"\" net = conv2d(inputs, filters * 1, 1) net = conv2d(net, filters * 2, 3) net = conv2d(net, filters * 1, 1) net = conv2d(net, filters * 2, 3) net = conv2d(net, filters * 1, 1) route = net net = conv2d(net, filters * 2, 3) return route, netdef upsample_layer(inputs, out_shape): \"\"\" 这一部分主要是对特征图进行resize，默认使用最近邻方法 :param inputs: :param out_shape: :return: \"\"\" new_height, new_width = out_shape[1], out_shape[2] # NOTE: here height is the first # TODO: Do we need to set `align_corners` as True? inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width), name='upsampled') return inputs","link":"/2019/05/22/Note12-YOLOv3-part04/"},{"title":"YOLOv3源码阅读：nms_utils.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master_utils.py 这一部分代码主要是非最大值抑制（NMS）的实现，原理都是相同，过程大致如下： - 首先按照目标的置信度从大到小排序 - 取出当前最大的置信度的目标框 - 计算剩下的目标框和取出的目标框的iou - 依次检查iou的大小，如果iou高于一定的阈值，则说明对应的目标框被取出的目标框抑制了，因此只留下iou小于一定阈值的框。 - 重复2~4步骤，直至处理完所有的目标框 - 返回所有取出的目标框，就是NMS的结果 需要注意的是，NMS只针对于一类类别的数据，如果有多个类别，则需要分别处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139# coding: utf-8from __future__ import division, print_functionimport numpy as npimport tensorflow as tfdef gpu_nms(boxes, scores, num_classes, max_boxes=50, score_thresh=0.5, nms_thresh=0.5): \"\"\" Perform NMS on GPU using TensorFlow. params: boxes: tensor of shape [1, 10647, 4] # 10647=(13*13+26*26+52*52)*3, for input 416*416 image scores: tensor of shape [1, 10647, num_classes], score=conf*prob num_classes: total number of classes max_boxes: integer, maximum number of predicted boxes you'd like, default is 50 score_thresh: if [ highest class probability score &lt; score_threshold] then get rid of the corresponding box nms_thresh: real value, \"intersection over union\" threshold used for NMS filtering \"\"\" boxes_list, label_list, score_list = [], [], [] max_boxes = tf.constant(max_boxes, dtype='int32') # since we do nms for single image, then reshape it boxes = tf.reshape(boxes, [-1, 4]) # '-1' means we don't konw the exact number of boxes score = tf.reshape(scores, [-1, num_classes]) # Step 1: Create a filtering mask based on \"box_class_scores\" by using \"threshold\". mask = tf.greater_equal(score, tf.constant(score_thresh)) # Step 2: Do non_max_suppression for each class for i in range(num_classes): # Step 3: Apply the mask to scores, boxes and pick them out filter_boxes = tf.boolean_mask(boxes, mask[:, i]) filter_score = tf.boolean_mask(score[:, i], mask[:, i]) nms_indices = tf.image.non_max_suppression(boxes=filter_boxes, scores=filter_score, max_output_size=max_boxes, iou_threshold=nms_thresh, name='nms_indices') label_list.append(tf.ones_like(tf.gather(filter_score, nms_indices), 'int32') * i) boxes_list.append(tf.gather(filter_boxes, nms_indices)) score_list.append(tf.gather(filter_score, nms_indices)) boxes = tf.concat(boxes_list, axis=0) score = tf.concat(score_list, axis=0) label = tf.concat(label_list, axis=0) return boxes, score, labeldef py_nms(boxes, scores, max_boxes=50, iou_thresh=0.5): \"\"\" Pure Python NMS baseline. Arguments: boxes: shape of [-1, 4], the value of '-1' means that dont know the exact number of boxes scores: shape of [-1,] max_boxes: representing the maximum of boxes to be selected by non_max_suppression iou_thresh: representing iou_threshold for deciding to keep boxes \"\"\" assert boxes.shape[1] == 4 and len(scores.shape) == 1 # 下面几行的代码主要是用于求解每个box的面积，然后按照每个box的score的大小进行排序 x1 = boxes[:, 0] y1 = boxes[:, 1] x2 = boxes[:, 2] y2 = boxes[:, 3] areas = (x2 - x1) * (y2 - y1) # 按照每个box的score大小进行排序，这里返回的是排序之后的box的index。 # 本质上order储存的是需要处理的box的索引 order = scores.argsort()[::-1] # keep用于储存保留下来的box的索引index keep = [] # 如果还存在没有被处理的box的索引 while order.size &gt; 0: # 由于之前进行了排序，所以order的第一个肯定是score最高的 i = order[0] # 将这个索引保存起来 keep.append(i) # 下面的代码主要是求解第一个box和剩下的所有的box的IOU， # 因为第一个是目标box，所以在order的选取上需要加上[1:]，取遍剩下的所有的box xx1 = np.maximum(x1[i], x1[order[1:]]) yy1 = np.maximum(y1[i], y1[order[1:]]) xx2 = np.minimum(x2[i], x2[order[1:]]) yy2 = np.minimum(y2[i], y2[order[1:]]) w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) inter = w * h # IOU计算 ovr = inter / (areas[i] + areas[order[1:]] - inter) # 将和目标box的IOU小于一定阈值的box的索引取出，因为高于这一阈值的box都已经被目标box抑制了 inds = np.where(ovr &lt;= iou_thresh)[0] # 然后更新我们的order，重复下一轮循环。 order = order[inds + 1] # 最后返回给定数目的box的索引 return keep[:max_boxes]def cpu_nms(boxes, scores, num_classes, max_boxes=50, score_thresh=0.5, iou_thresh=0.5): \"\"\" Perform NMS on CPU. Arguments: boxes: shape [1, 10647, 4] scores: shape [1, 10647, num_classes] \"\"\" boxes = boxes.reshape(-1, 4) scores = scores.reshape(-1, num_classes) # Picked bounding boxes picked_boxes, picked_score, picked_label = [], [], [] for i in range(num_classes): indices = np.where(scores[:, i] &gt;= score_thresh) filter_boxes = boxes[indices] filter_scores = scores[:, i][indices] if len(filter_boxes) == 0: continue # do non_max_suppression on the cpu indices = py_nms(filter_boxes, filter_scores, max_boxes=max_boxes, iou_thresh=iou_thresh) picked_boxes.append(filter_boxes[indices]) picked_score.append(filter_scores[indices]) picked_label.append(np.ones(len(indices), dtype='int32') * i) if len(picked_boxes) == 0: return None, None, None boxes = np.concatenate(picked_boxes, axis=0) score = np.concatenate(picked_score, axis=0) label = np.concatenate(picked_label, axis=0) return boxes, score, label","link":"/2019/05/22/Note13-YOLOv3-part05/"},{"title":"Softmax、Cross Entropy及其反向传播（求导）","text":"前言 分类问题是深度学习中的最基本的问题，而分类问题中，我们使用最多的就是利用\\(softmax\\)函数并结合\\(cross \\; entropy\\)计算最后的损失值\\(Loss\\)。所以我们有必要对其进行一定的了解，并进行其求导的操作，这里的求导就相当于进行反向传播。 一、参数设置 假设我们输入一个样本，并经过一定的操作，例如卷积，全连接，dropout等一系列操作，最后，我们可以得到关于这个样本的一个长度为\\(n\\)的特征向量，拿到这个特征向量之后，我们往往需要对其使用\\(softmax\\)函数进行归一化，使之满足一个概率分布的基本条件（即所有的概率分量之和为1），然后我们使用\\(cross \\; entropy\\)来计算最后的损失值。所以，我们不妨假设我们经过深度学习模型前向传播之后的特征向量为\\(x\\)，所以，我们有： \\[ x = [x_1, x_2, x_3, \\cdots, x_n] \\] 根据\\(softmax\\)的定义，我们会得到以下的特征向量经过变化之后的概率分布，有： \\[ softmax(x) = [\\frac{e^{x_1}}{\\sum_{i=1}^n e^{x_i}}, \\frac{e^{x_2}}{\\sum_{i=1}^n e^{x_i}}, \\frac{e^{x_3}}{\\sum_{i=1}^n e^{x_i}}, \\cdots, \\frac{e^{x_n}}{\\sum_{i=1}^n e^{x_i}}] \\] 很容易验证上面的结果满足一个概率分布的基本条件（即所有的概率分量之和为1）。 为了表述方便，我们将上面的式子表示为： \\[ softmax(x) = [a_1, a_2, a_3, \\cdots, a_n] \\] 其中，\\(a_k = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}, \\; \\;(1 \\leq k \\leq n)\\)。 我们对上面的结果和实际上给的标签\\(y\\)进行一定的计算，可以得到最后的损失值\\(Loss\\)，然后我们根据这个损失值\\(Loss\\)进行反向传播，因此，我们不妨假设标签\\(y\\)为如下的形式： \\[ y = [y_1, y_2, y_3, \\cdots, y_n] \\] 根据交叉熵\\(cross \\; entropy\\)的定义，最后我们的损失值为： \\[ Loss = -\\sum_{i = 1}^n y_i \\; ln \\;a_i \\] 二、Softmax求导 可以发现，我们改动\\(x\\)的任何一个分量，最后的结果会影响到每一个分量，所以，当我们需要求\\(x\\)的一个分量\\(x_k\\)的偏导数时，我们需要对上面的每一个分量\\(a_i\\)对\\(x_k\\)求偏导数。 但是，很容易可以发现上面的\\(a\\)的分量可以分成两类，一类是\\(a_k\\)，另一类是\\(a_i\\)，其中\\(i \\neq k\\)，所以不同的\\(a\\)的分量有不同的对\\(x_k\\)的偏导数求法。 \\[ [\\frac{\\partial a_1}{\\partial x_k}, \\frac{\\partial a_2}{\\partial x_k}, \\cdots, \\frac{\\partial a_k}{\\partial x_k}, \\cdots, \\frac{\\partial a_n}{\\partial x_k}] \\] 所以，对于偏导数\\(\\frac{\\partial a_k}{\\partial x_k}\\)来说，我们的计算过程如下： \\[ \\frac{\\partial a_k}{\\partial x_k} = \\frac{\\partial \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}}{\\partial x_k} = \\frac{e^{x_k}(\\sum_{i=1}^n e^{x_i}) - e^{x_k}(e^{x_k})}{(\\sum_{i=1}^n e^{x_i})^2} = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}} \\cdot (1 - \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}) = a_k \\cdot (1 - a_k) \\] 对于其他的\\(\\frac{\\partial a_j}{\\partial x_k}\\)，其中\\(j \\neq k\\)来说，它的偏导数计算过程如下： \\[ \\frac{\\partial a_j}{\\partial x_k} = \\frac{\\partial \\frac{e^{x_j}}{\\sum_{i=1}^n e^{x_i}}}{\\partial x_k} = \\frac{- e^{x_j}(e^{x_k})}{(\\sum_{i=1}^n e^{x_i})^2} = - \\frac{e^{x_j}}{\\sum_{i=1}^n e^{x_i}} \\cdot \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}} = - a_j \\cdot a_k \\] 假设我们从上一层接收到的误差为\\(\\delta = [\\frac{\\partial L}{\\partial a_1}, \\frac{\\partial L}{\\partial a_2},\\frac{\\partial L}{\\partial a_3}, \\cdots, \\frac{\\partial L}{\\partial a_n}]\\)，那么，根据求导的链式法则，我们需要计算的偏导数\\(\\frac{\\partial L}{\\partial x_k}\\)有： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_k} &amp;= \\sum_{i = 1}^n \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial x_k} \\\\ &amp;= \\sum_{i = 1, i \\neq k}^n \\frac{\\partial L}{\\partial a_i} \\cdot \\frac{\\partial a_i}{\\partial x_k} + \\frac{\\partial L}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial x_k} \\\\ &amp;= \\sum_{i = 1, i \\neq k}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i \\cdot a_k) + \\frac{\\partial L}{\\partial a_k} \\cdot (a_k \\cdot (1 - a_k)) \\\\ &amp;= a_k \\{\\sum_{i = 1, i \\neq k}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i) + \\frac{\\partial L}{\\partial a_k} \\cdot (1 - a_k) \\} \\\\ &amp;= a_k \\{\\sum_{i = 1, i \\neq k}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i) + \\frac{\\partial L}{\\partial a_k} \\cdot (- a_k) + \\frac{\\partial L}{\\partial a_k} \\} \\\\ &amp;= a_k \\{\\sum_{i = 1}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i)+ \\frac{\\partial L}{\\partial a_k} \\} \\end{aligned} \\] 到此为止，我们计算出了\\(softmax\\)层的反向传播的计算公式，下一步就是讨论交叉熵的反向传播（误差计算）。 三、交叉熵\\(cross \\; entropy\\)的误差计算 本质上，我们的交叉熵有如下的计算公式： \\[ Loss = -\\sum_{i = 1}^n y_i \\; ln \\;a_i \\] 根据对数的求导公式，我们很容易有： \\[ \\frac{\\partial L}{\\partial a_i} = - y_i \\frac{1}{a_i} \\] 我们下面就将上面的公式代入\\(softmax\\)的反向传播的公式中。 四、综合 经过前面的计算，我们有： \\[ \\frac{\\partial L}{\\partial x_k} = a_k \\{\\sum_{i = 1}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i)+ \\frac{\\partial L}{\\partial a_k} \\} \\] 接着，我们将\\(\\frac{\\partial L}{\\partial a_i} = - y_i \\frac{1}{a_i}\\)代入上面的式子中，即有： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_k} &amp;= a_k \\{\\sum_{i = 1}^n \\frac{\\partial L}{\\partial a_i} \\cdot (- a_i)+ \\frac{\\partial L}{\\partial a_k} \\} \\\\ &amp;= a_k \\{\\sum_{i = 1}^n - y_i \\frac{1}{a_i} (-a_i) + - y_k \\frac{1}{a_k} \\} \\\\ &amp;= a_k \\{ \\sum_{i = 1}^n y_i + - y_k \\frac{1}{a_k} \\} \\\\ &amp;= a_k (\\sum_{i = 1}^n y_i) + a_k (- y_k \\frac{1}{a_k}) \\\\ &amp;= a_k (\\sum_{i = 1}^n y_i) - y_k \\end{aligned} \\] 考虑到在绝大多数情况下，给定的标签\\(y\\)往往是一个one-hot的向量，即\\(y\\)的所有分量中有且仅有一个分量的值是1，剩下的所有分量的值都是0，所以，我们有：\\(\\sum_{i = 1}^n y_i = 1\\)。将这个式子代入上面的公式，我们可以得到一个十分简单的表达： \\[ \\frac{\\partial L}{\\partial x_k} = a_k (\\sum_{i = 1}^n y_i) - y_k = a_k - y_k = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}} - y_k \\] 需要注意的是上面这个简单表达的使用条件： 使用softmax函数计算概率分布。 使用交叉熵计算最后的损失值。 标签必须是one-hot向量。 五、总结 总的来说，softmax和交叉熵的组合可以优化神经网络的表达，同时也不会引入过多的反向传播的计算开销。","link":"/2019/06/03/Note20-Softmax-and-Cross-Entropy/"},{"title":"支持向量机以及核函数","text":"","link":"/2019/06/15/Note23-SVM-Kernel-Function/"},{"title":"支持向量回归SVR","text":"前言 在之前都是在讨论SVM，即支持向量机在分类中的使用，但是有很多的问题是回归问题，给出的标签值是一个连续值，能不能使用SVM或者其他的关于SVM的技术来处理分类问题呢，实际上是可以的，这个就是使用支持向量回归模型（Support Vector Regression， SVR）。 一、SVR的模型定义 在很多情况下，回归问题采用的损失函数是","link":"/2019/06/15/Note24-SVR/"},{"title":"PCA背后的公式推导","text":"一、PCA 在之前讲述过如何使用PCA以及背后的基本的几何原理。但是并没有给出详细的数学推导，因此，在这里补上PCA的详细数学推导过程。 二、预备知识 1. 矩阵的特征值分解 这一部分是线性代数的基本内容。 2. Frobenius 范数 在线性代数中会有很有的范数来进行约束，Frobenius范数就是其中之一，它的定义十分简单，就是对矩阵中的每个元素进行平方，再进行求和，最后对计算得到的和进行开方。Frobenius范数通常记作\\(||A||_F\\)，用公式表达如下： \\[ Frobenius(A) = ||A||_F = \\sqrt{\\sum_i\\sum_j A_{i, j}^2} \\tag{1} \\] 其中，\\(A\\)表示一个矩阵，\\(A_{i, j}\\)表示的是矩阵中第\\(i\\)行第\\(j\\)列的元素。 如果将矩阵是做一个多维空间中的一个点的话，那么Frobenius范数就代表这个点和原点之间的距离。 3. 矩阵的迹（Trace） 在线性代数中，还有一个十分朴素但却十分重要的概念就是矩阵的迹（Trace），它表示的是矩阵的对角线上的元素的和， 记作\\(Tr(A)\\)，用公式表达如下： \\[ Tr(A) = \\sum_i A_{i, i} \\tag{2} \\] 矩阵的迹（trace）有时候可以简化矩阵的计算，在线性代数中有很重要的用处。 4.矩阵的迹（trace）和Frobenius范数的关系 在《深度学习》（中文版）（即有人称之为“花书”）中，第29页给出了一个公式： \\[ ||A||_F = \\sqrt{Tr(AA^T)} \\tag{3} \\] 在这里，我们简单证明一下。 假设我们的矩阵\\(A\\)可以表示为列向量的集合，即： \\[ A = \\begin{bmatrix} \\vec{a_1}&amp; \\vec{a_2}&amp; \\cdots&amp; \\vec{a_n} \\end{bmatrix} \\] 其中，每一项都是一个长度\\(m\\)的列向量，即\\(\\vec{a_i} \\in \\Bbb{R^m}\\)。所以\\(A \\in \\Bbb{R^{m \\times n}}\\)。 有上面的表示法，我们可以得到\\(A^T\\)，\\(A^T \\in \\Bbb{R^{n \\times m}}\\)： \\[ A^T = \\begin{bmatrix} \\vec{a_1}^T \\\\ \\vec{a_2}^T \\\\ \\vdots \\\\ \\vec{a_n}^T \\end{bmatrix} \\] 于是我们可以得到： \\[ \\begin{aligned} A^T \\cdot A=&amp;\\begin{bmatrix} \\vec{a_1}^T \\\\ \\vec{a_2}^T \\\\ \\vdots \\\\ \\vec{a_n}^T \\end{bmatrix} \\cdot \\begin{bmatrix} \\vec{a_1}&amp; \\vec{a_2}&amp; \\cdots&amp; \\vec{a_n} \\end{bmatrix} \\\\ =&amp; \\begin{bmatrix} \\vec{a_1}^T \\cdot \\vec{a_1}&amp; \\vec{a_1}^T \\cdot \\vec{a_2}&amp; \\cdots&amp; \\vec{a_1}^T \\cdot \\vec{a_n} \\\\ \\vec{a_2}^T \\cdot \\vec{a_1}&amp; \\vec{a_2}^T \\cdot \\vec{a_2}&amp; \\cdots&amp; \\vec{a_2}^T \\cdot \\vec{a_n} \\\\ \\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots\\\\ \\vec{a_n}^T \\cdot \\vec{a_1}&amp; \\vec{a_n}^T \\cdot \\vec{a_2}&amp; \\cdots&amp; \\vec{a_n}^T \\cdot \\vec{a_n} \\end{bmatrix} \\end{aligned} \\tag{4} \\] 所以： \\[ Tr(A^TA) = \\sum_{i=1}^n \\vec{a_i}^T \\cdot \\vec{a_i} \\tag{5} \\] 我们考察其中的某一个\\(\\vec{a_i}^T \\cdot \\vec{a_i}\\)。由于\\(\\vec{a_i}\\)表示的是长度为\\(m\\)的列向量，表示的是矩阵\\(A\\)的第\\(i\\)列，因此，我们有： \\[ \\begin{aligned} \\vec{a_i}^T \\cdot \\vec{a_i} &amp;=\\begin{bmatrix} A_{1, i} \\\\ A_{2, i} \\\\ \\vdots \\\\ A_{m, i} \\end{bmatrix} \\cdot \\begin{bmatrix} A_{1, i} &amp; A_{2, i}&amp; \\cdots &amp; A_{m, i} \\end{bmatrix} \\\\ &amp;= \\sum_{k = 1}^m A_{k, i} ^2 \\end{aligned} \\tag{6} \\] 将\\(\\vec{a_i}^T \\cdot \\vec{a_i} =\\sum_{k = 1}^m A_{k, i} ^2\\)代入式子（5），有： \\[ \\begin{aligned} Tr(A^TA) &amp;= \\sum_{i=1}^n \\vec{a_i}^T \\cdot \\vec{a_i} \\\\ &amp;= \\sum_{i= 1}^n \\sum_{k = 1}^m A_{k, i}^2 \\\\ &amp;= \\sum_{i= 1}^n \\sum_{j = 1}^m A_{j, i}^2 \\\\ &amp;= \\sum_{j = 1}^m \\sum_{i= 1}^n A_{j, i}^2 \\\\ &amp;= ||A||_F^2 \\end{aligned} \\tag{7} \\] 考虑到对于一个矩阵，有：\\(Tr(A^T A) = Tr(A A^T)\\)，故： \\[ ||A||_F = \\sqrt{Tr(A^T A)} = \\sqrt{Tr(A A^T)} \\tag{8} \\] 命题得证。 三、详细推导过程 1. 编码和解码 在之前也讲述过PCA的求解过程，甚至也进行了代码的编写，但是并没有给出严格的数学证明，只是从感性上进行理解，即挑选若干个坐标的基，使得每个基上的样本的方差尽可能大，从而达到数据降维的目的。假设我们的每一个样本点都是一个列向量，长度为\\(n\\)，不妨记作\\(x_i\\)，\\(x_i \\in \\Bbb{R^n}\\)。一共收集到了\\(m\\)个样本数据，则这些矩阵可以组成我们的样本矩阵\\(A\\)，如下： \\[ A = \\begin{bmatrix}x_0 &amp; x_1 &amp; \\cdots &amp; x_m \\end{bmatrix}, A \\in \\Bbb{R^{n \\times m }} \\] 按照之前的说明，我们求解\\(\\Sigma= AA^T\\)，\\(\\Sigma\\)表示的是样本矩阵的协方差矩阵，接着我们对该协方差矩阵进行特征分解，会产生\\(n\\)个特征值，我们选择较大的若干个特征值，并选择对应的特征向量，组成一个对应的变换矩阵\\(S\\)，然后对数据矩阵应用这个变换矩阵，有\\(R = A S^T\\)，则这个\\(R\\)矩阵就是我们最后求得的结果，这个结果会存在一定的精度损失，不过一般可以忽略不记。 如果我们此时在矩阵的右侧重新乘以变换矩阵\\(S\\)，则结果会还原成我们的原始的数据矩阵，即： \\[ RS = AS^T S = A \\] 从上面的过程中可以看出，我们完全可以将PCA看作一个线性的编码和解码的过程，因此，我们所需要的就是求解编码和解码的矩阵信息。 2.编码矩阵和解码矩阵的关系 现在开始，让我们从头开始推导PCA的相关公式。根据我们的经验，线性变换是所有变换中最简单的，因此我们就从最简单的线性变换开始。 假设我们的数据矩阵依旧表示成上面的形式，如果我们收集数据维度太多，不便于进一步分析，我们希望可以对数据进行一定的线性压缩，这种压缩是可逆的，存在一个解码矩阵可以将数据还原成原始的数据。 我们考虑其中的一个数据向量\\(x_i\\)，\\(x_i \\in \\Bbb{R^m}\\)。编码压缩之后的数据向量为\\(c_i\\)，\\(c_i \\in \\Bbb{R^l}\\)，其中\\(l &lt; m\\)。我们可以将这个编码过程表示成一个函数，即\\(f(x_i) = c_i\\)。同时也可以将解码函数表示成一个函数，即","link":"/2019/08/14/Note25-PCAMathDerivation/"},{"title":"YOLOv3源码阅读：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master\\test_single_image.py 需要注意的是，我们默认输入图片尺寸为\\([416, 416]\\)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# coding: utf-8from __future__ import division, print_functionimport tensorflow as tfimport numpy as npimport argparseimport cv2from utils.misc_utils import parse_anchors, read_class_namesfrom utils.nms_utils import gpu_nmsfrom utils.plot_utils import get_color_table, plot_one_boxfrom model import yolov3# 设置命令行参数，具体可参见每一个命令行参数的含义parser = argparse.ArgumentParser(description=\"YOLO-V3 test single image test procedure.\")parser.add_argument(\"input_image\", type=str, help=\"The path of the input image.\")parser.add_argument(\"--anchor_path\", type=str, default=\"./data/yolo_anchors.txt\", help=\"The path of the anchor txt file.\")parser.add_argument(\"--new_size\", nargs='*', type=int, default=[416, 416], help=\"Resize the input image with `new_size`, size format: [width, height]\")parser.add_argument(\"--class_name_path\", type=str, default=\"./data/coco.names\", help=\"The path of the class names.\")parser.add_argument(\"--restore_path\", type=str, default=\"./data/darknet_weights/yolov3.ckpt\", help=\"The path of the weights to restore.\")args = parser.parse_args()# 处理anchors，这些anchors是通过数据聚类获得，一共9个，shape为：[9, 2]。# 需要注意的是，最后一个维度的顺序是[width, height]args.anchors = parse_anchors(args.anchor_path)# 处理classes， 这里是将所有的class的名称提取了出来，组成了一个列表args.classes = read_class_names(args.class_name_path)# 类别的数目args.num_class = len(args.classes)# 根据类别的数目为每一个类别分配不同的颜色，以便展示color_table = get_color_table(args.num_class)# 读取图片img_ori = cv2.imread(args.input_image)# 获取图片的尺寸height_ori, width_ori = img_ori.shape[:2]# resize，根据之前设定的尺寸值进行resize，默认是[416, 416]，还是[width, height]的顺序img = cv2.resize(img_ori, tuple(args.new_size))# 对图片像素进行一定的数据处理img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)img = np.asarray(img, np.float32)img = img[np.newaxis, :] / 255.# TF会话with tf.Session() as sess: # 输入的placeholder，用于输入图片 input_data = tf.placeholder(tf.float32, [1, args.new_size[1], args.new_size[0], 3], name='input_data') # 定义一个YOLOv3的类，在后面可以用来做模型建立以及loss计算等操作，参数分别是类别的数目和anchors yolo_model = yolov3(args.num_class, args.anchors) with tf.variable_scope('yolov3'): # 对图片进行正向传播，返回多张特征图 pred_feature_maps = yolo_model.forward(input_data, False) # 对这些特征图进行处理，获得计算出的bounding box以及属于前景的概率已经每一个类别的概率分布 pred_boxes, pred_confs, pred_probs = yolo_model.predict(pred_feature_maps) # 将两个概率值分别相乘就可以获得最终的概率值 pred_scores = pred_confs * pred_probs # 对这些bounding boxes和概率值进行非最大抑制（NMS）就可以获得最后的bounding boxes和与其对应的概率值以及标签 boxes, scores, labels = gpu_nms(pred_boxes, pred_scores, args.num_class, max_boxes=30, score_thresh=0.4, nms_thresh=0.5) # Saver类，用以保存和恢复模型 saver = tf.train.Saver() # 恢复模型参数 saver.restore(sess, args.restore_path) # 运行graph，获得对应tensors的具体数值，这里是[boxes, scores, labels]，对应于NMS之后获得的结果 boxes_, scores_, labels_ = sess.run([boxes, scores, labels], feed_dict={input_data: img}) # rescale the coordinates to the original image # 将坐标重新映射到原始图片上，因为前面的计算都是在resize之后的图片上进行的，所以需要进行映射 boxes_[:, 0] *= (width_ori/float(args.new_size[0])) boxes_[:, 2] *= (width_ori/float(args.new_size[0])) boxes_[:, 1] *= (height_ori/float(args.new_size[1])) boxes_[:, 3] *= (height_ori/float(args.new_size[1])) # 输出 print(\"box coords:\") print(boxes_) print('*' * 30) print(\"scores:\") print(scores_) print('*' * 30) print(\"labels:\") print(labels_) # 绘制并展示，保存最后的结果 for i in range(len(boxes_)): x0, y0, x1, y1 = boxes_[i] plot_one_box(img_ori, [x0, y0, x1, y1], label=args.classes[labels_[i]], color=color_table[labels_[i]]) cv2.imshow('Detection result', img_ori) cv2.imwrite('detection_result.jpg', img_ori) cv2.waitKey(0)","link":"/2019/05/22/Note9-YOLOv3-part01/"},{"title":"Welcome","text":"WELCOME","link":"/2019/05/09/Welcome/"},{"title":"YOLOv3源码阅读：train.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master.py 这一部分代码主要是训练模型的入口，按照要求准备号训练数据之后，就可以从这里开始训练了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271# coding: utf-8from __future__ import division, print_functionimport tensorflow as tfimport numpy as npimport loggingfrom tqdm import trangeimport argsfrom utils.data_utils import get_batch_datafrom utils.misc_utils import shuffle_and_overwrite, make_summary, config_learning_rate, config_optimizer, AverageMeterfrom utils.eval_utils import evaluate_on_cpu, evaluate_on_gpu, get_preds_gpu, voc_eval, parse_gt_recfrom utils.nms_utils import gpu_nmsfrom model import yolov3# setting loggers# 设置日志记录logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s', datefmt='%a, %d %b %Y %H:%M:%S', filename=args.progress_log_path, filemode='w')# setting placeholders# 整个网络的数据输入入口# 是否是训练阶段，针对BN等操作有用is_training = tf.placeholder(tf.bool, name=\"phase_train\")# 这个数据输入入口未被使用，原因不明handle_flag = tf.placeholder(tf.string, [], name='iterator_handle_flag')# register the gpu nms operation here for the following evaluation scheme# 为了后面的模型评估的计算，这里首先定义好在gpu上的nms操作pred_boxes_flag = tf.placeholder(tf.float32, [1, None, None])pred_scores_flag = tf.placeholder(tf.float32, [1, None, None])gpu_nms_op = gpu_nms(pred_boxes_flag, pred_scores_flag, args.class_num, args.nms_topk, args.score_threshold, args.nms_threshold)################### tf.data pipeline################### 输入输入流，我们是从一个文本文件读入数据，因此，可以使用TextLineDataset类来帮助数据读入train_dataset = tf.data.TextLineDataset(args.train_file)# 随机打乱train_dataset = train_dataset.shuffle(args.train_img_cnt)# 设定batch sizetrain_dataset = train_dataset.batch(args.batch_size)# 自定义输入的返回格式，因为文本文件中的数据不一定就是正式的使用数据，可以自定义真正的数据读取操作train_dataset = train_dataset.map( lambda x: tf.py_func(get_batch_data, inp=[x, args.class_num, args.img_size, args.anchors, 'train', args.multi_scale_train, args.use_mix_up], Tout=[tf.int64, tf.float32, tf.float32, tf.float32, tf.float32]), num_parallel_calls=args.num_threads)# 预先读取train_dataset = train_dataset.prefetch(args.prefetech_buffer)# 和训练数据的读取类似,这里读取的是验证集的数据val_dataset = tf.data.TextLineDataset(args.val_file)val_dataset = val_dataset.batch(1)val_dataset = val_dataset.map( lambda x: tf.py_func(get_batch_data, inp=[x, args.class_num, args.img_size, args.anchors, 'val', False, False], Tout=[tf.int64, tf.float32, tf.float32, tf.float32, tf.float32]), num_parallel_calls=args.num_threads)val_dataset.prefetch(args.prefetech_buffer)# 定义迭代器iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)train_init_op = iterator.make_initializer(train_dataset)val_init_op = iterator.make_initializer(val_dataset)# get an element from the chosen dataset iterator# 利用迭代器获取数据.由于之前我们自定义了数据的读取方式,这里返回的正是我们希望的数据image_ids, image, y_true_13, y_true_26, y_true_52 = iterator.get_next()y_true = [y_true_13, y_true_26, y_true_52]# tf.data pipeline will lose the data `static` shape, so we need to set it manually# 手动设置shapeimage_ids.set_shape([None])image.set_shape([None, None, None, 3])for y in y_true: y.set_shape([None, None, None, None, None])################### Model definition################### 模型定义,这一部分和预测时的一致.yolo_model = yolov3(args.class_num, args.anchors, args.use_label_smooth, args.use_focal_loss, args.batch_norm_decay, args.weight_decay)with tf.variable_scope('yolov3'): pred_feature_maps = yolo_model.forward(image, is_training=is_training)# 计算损失loss = yolo_model.compute_loss(pred_feature_maps, y_true)# 计算预测的结果y_pred = yolo_model.predict(pred_feature_maps)# 正则化的损失l2_loss = tf.losses.get_regularization_loss()# setting restore parts and vars to update# 定义Saver,saver_to_restore = tf.train.Saver(var_list=tf.contrib.framework.get_variables_to_restore(include=args.restore_part))update_vars = tf.contrib.framework.get_variables_to_restore(include=args.update_part)# 这一部分是为了tensor board可视化做的准备,主要是一些曲线,反映loss的变化tf.summary.scalar('train_batch_statistics/total_loss', loss[0])tf.summary.scalar('train_batch_statistics/loss_xy', loss[1])tf.summary.scalar('train_batch_statistics/loss_wh', loss[2])tf.summary.scalar('train_batch_statistics/loss_conf', loss[3])tf.summary.scalar('train_batch_statistics/loss_class', loss[4])tf.summary.scalar('train_batch_statistics/loss_l2', l2_loss)tf.summary.scalar('train_batch_statistics/loss_ratio', l2_loss / loss[0])# global stepglobal_step = tf.Variable(float(args.global_step), trainable=False, collections=[tf.GraphKeys.LOCAL_VARIABLES])# 是否使用warm up,默认是True,主要是定义学习率的方法上有些区别if args.use_warm_up: learning_rate = tf.cond(tf.less(global_step, args.train_batch_num * args.warm_up_epoch), lambda: args.learning_rate_init * global_step / (args.train_batch_num * args.warm_up_epoch), lambda: config_learning_rate(args, global_step - args.train_batch_num * args.warm_up_epoch))else: learning_rate = config_learning_rate(args, global_step)tf.summary.scalar('learning_rate', learning_rate)#if not args.save_optimizer: saver_to_save = tf.train.Saver() saver_best = tf.train.Saver()# 优化器optimizer = config_optimizer(args.optimizer_name, learning_rate)if args.save_optimizer: saver_to_save = tf.train.Saver() saver_best = tf.train.Saver()# set dependencies for BN ops# 为BN操作设置依赖update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)with tf.control_dependencies(update_ops): train_op = optimizer.minimize(loss[0] + l2_loss, var_list=update_vars, global_step=global_step)# 设置会话Sessionwith tf.Session() as sess: # 初始化全局的variable sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()]) saver_to_restore.restore(sess, args.restore_path) merged = tf.summary.merge_all() writer = tf.summary.FileWriter(args.log_dir, sess.graph) print('\\n----------- start to train -----------\\n') best_mAP = -np.Inf # 开始循环训练 for epoch in range(args.total_epoches): sess.run(train_init_op) # 定义记录数据的类，主要是保存当前为止的所有数据的均值 loss_total, loss_xy, loss_wh, loss_conf, loss_class = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter() # 对每一个bacth size for i in trange(args.train_batch_num): _, summary, __y_pred, __y_true, __loss, __global_step, __lr = sess.run( [train_op, merged, y_pred, y_true, loss, global_step, learning_rate], feed_dict={is_training: True}) writer.add_summary(summary, global_step=__global_step) # 更新均值 loss_total.update(__loss[0], len(__y_pred[0])) loss_xy.update(__loss[1], len(__y_pred[0])) loss_wh.update(__loss[2], len(__y_pred[0])) loss_conf.update(__loss[3], len(__y_pred[0])) loss_class.update(__loss[4], len(__y_pred[0])) # 每隔一段时间进行模型的评估，这里主要计算的是recall和precision # 这里计算的是训练集上的评估结果 if __global_step % args.train_evaluation_step == 0 and __global_step &gt; 0: # recall, precision = evaluate_on_cpu(__y_pred, __y_true, args.class_num, args.nms_topk, args.score_threshold, args.eval_threshold) recall, precision = evaluate_on_gpu(sess, gpu_nms_op, pred_boxes_flag, pred_scores_flag, __y_pred, __y_true, args.class_num, args.eval_threshold) info = \"Epoch: {}, global_step: {} | loss: total: {:.2f}, xy: {:.2f}, wh: {:.2f}, conf: {:.2f}, class: {:.2f} | \".format( epoch, int(__global_step), loss_total.average, loss_xy.average, loss_wh.average, loss_conf.average, loss_class.average) info += 'Last batch: rec: {:.3f}, prec: {:.3f} | lr: {:.5g}'.format(recall, precision, __lr) print(info) logging.info(info) writer.add_summary(make_summary('evaluation/train_batch_recall', recall), global_step=__global_step) writer.add_summary(make_summary('evaluation/train_batch_precision', precision), global_step=__global_step) if np.isnan(loss_total.average): print('****' * 10) raise ArithmeticError( 'Gradient exploded! Please train again and you may need modify some parameters.') # 重置相关的均值记录类 tmp_total_loss = loss_total.average loss_total.reset() loss_xy.reset() loss_wh.reset() loss_conf.reset() loss_class.reset() # 保存模型 # NOTE: this is just demo. You can set the conditions when to save the weights. if epoch % args.save_epoch == 0 and epoch &gt; 0: if tmp_total_loss &lt;= 2.: saver_to_save.save(sess, args.save_dir + 'model-epoch_{}_step_{}_loss_{:.4f}_lr_{:.5g}'.format(epoch, int(__global_step), loss_total.last_avg, __lr)) # 验证集用以评估模型，这一部分和前面类似 # switch to validation dataset for evaluation if epoch % args.val_evaluation_epoch == 0 and epoch &gt; 0: sess.run(val_init_op) val_loss_total, val_loss_xy, val_loss_wh, val_loss_conf, val_loss_class = \\ AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter() val_preds = [] for j in trange(args.val_img_cnt): __image_ids, __y_pred, __loss = sess.run([image_ids, y_pred, loss], feed_dict={is_training: False}) pred_content = get_preds_gpu(sess, gpu_nms_op, pred_boxes_flag, pred_scores_flag, __image_ids, __y_pred) val_preds.extend(pred_content) val_loss_total.update(__loss[0]) val_loss_xy.update(__loss[1]) val_loss_wh.update(__loss[2]) val_loss_conf.update(__loss[3]) val_loss_class.update(__loss[4]) # calc mAP # 计算mAP rec_total, prec_total, ap_total = AverageMeter(), AverageMeter(), AverageMeter() gt_dict = parse_gt_rec(args.val_file, args.img_size) info = '======&gt; Epoch: {}, global_step: {}, lr: {:.6g} &lt;======\\n'.format(epoch, __global_step, __lr) for ii in range(args.class_num): npos, nd, rec, prec, ap = voc_eval(gt_dict, val_preds, ii, iou_thres=args.eval_threshold, use_07_metric=False) info += 'EVAL: Class {}: Recall: {:.4f}, Precision: {:.4f}, AP: {:.4f}\\n'.format(ii, rec, prec, ap) rec_total.update(rec, npos) prec_total.update(prec, nd) ap_total.update(ap, 1) mAP = ap_total.avg info += 'EVAL: Recall: {:.4f}, Precison: {:.4f}, mAP: {:.4f}\\n'.format(rec_total.avg, prec_total.avg, mAP) info += 'EVAL: loss: total: {:.2f}, xy: {:.2f}, wh: {:.2f}, conf: {:.2f}, class: {:.2f}\\n'.format( val_loss_total.avg, val_loss_xy.avg, val_loss_wh.avg, val_loss_conf.avg, val_loss_class.avg) print(info) logging.info(info) if mAP &gt; best_mAP: best_mAP = mAP saver_best.save(sess, args.save_dir + 'best_model_Epoch_{}_step_{}_mAP_{:.4f}_loss_{:.4f}_lr_{:.7g}'.format( epoch, __global_step, best_mAP, val_loss_total.last_avg, __lr)) writer.add_summary(make_summary('evaluation/val_mAP', mAP), global_step=epoch) writer.add_summary(make_summary('evaluation/val_recall', rec_total.last_avg), global_step=epoch) writer.add_summary(make_summary('evaluation/val_precision', prec_total.last_avg), global_step=epoch) writer.add_summary(make_summary('validation_statistics/total_loss', val_loss_total.last_avg), global_step=epoch) writer.add_summary(make_summary('validation_statistics/loss_xy', val_loss_xy.last_avg), global_step=epoch) writer.add_summary(make_summary('validation_statistics/loss_wh', val_loss_wh.last_avg), global_step=epoch) writer.add_summary(make_summary('validation_statistics/loss_conf', val_loss_conf.last_avg), global_step=epoch) writer.add_summary(make_summary('validation_statistics/loss_class', val_loss_class.last_avg), global_step=epoch)","link":"/2019/05/22/Note13-YOLOv3-part06/"},{"title":"YOLOv3源码阅读：data_utils.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master_utils.py 这一部分代码主要是准备训练用的数据。算得上是YOLO模型中另一个十分重要的部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317# coding: utf-8from __future__ import division, print_functionimport numpy as npimport cv2import sysfrom utils.data_aug import *import randomPY_VERSION = sys.version_info[0]iter_cnt = 0def parse_line(line): ''' Given a line from the training/test txt file, return parsed info. return: line_idx: int64 pic_path: string. boxes: shape [N, 4], N is the ground truth count, elements in the second dimension are [x_min, y_min, x_max, y_max] labels: shape [N]. class index. ''' \"\"\" 这一部分代码的主要功能是对给定的数据字符串进行处理，提取出其中的有效信息，包括如下： 1. 图片索引 2. 图片路径 3. 每一个目标框的坐标， 4. 每一个目标框的label \"\"\" if 'str' not in str(type(line)): line = line.decode() # 按照空格划分数据 s = line.strip().split(' ') # 第一个数据是图片索引 line_idx = int(s[0]) # 第二个数据是图片的路径 pic_path = s[1] # 去除掉前两个数据之后，剩下的就和目标框有关系了 s = s[2:] # 每一个目标框都包含五个数据，4个坐标信息和1个label信息，因此数据总数除以5之后就是目标框的总数目 box_cnt = len(s) // 5 # 存储数据的list boxes = [] labels = [] # 对每一个目标框 for i in range(box_cnt): # 提取出label以及四个坐标数据 label, x_min, y_min, x_max, y_max = int(s[i * 5]), float(s[i * 5 + 1]), float(s[i * 5 + 2]), float( s[i * 5 + 3]), float(s[i * 5 + 4]) boxes.append([x_min, y_min, x_max, y_max]) labels.append(label) # numpy处理一下 boxes = np.asarray(boxes, np.float32) labels = np.asarray(labels, np.int64) # 返回 return line_idx, pic_path, boxes, labelsdef process_box(boxes, labels, img_size, class_num, anchors): ''' Generate the y_true label, i.e. the ground truth feature_maps in 3 different scales. params: boxes: [N, 5] shape, float32 dtype. `x_min, y_min, x_max, y_mix, mixup_weight`. labels: [N] shape, int64 dtype. class_num: int64 num. anchors: [9, 4] shape, float32 dtype. ''' \"\"\" 这一部分是数据预处理中最重要的一部分,因为这里才是生成最后的y true的地方 \"\"\" # anchor的编号,分别对应于每一个不同尺寸的特征图, # 大尺寸的特征图对应的anchor是6,7,8,中尺寸的特征图对应的是3,4,5,小尺寸的对应的是0,1,2 anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] # convert boxes form: # shape: [N, 2] # (x_center, y_center) # 计算目标框的中心坐标 box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2 # (width, height) # 计算目标框的大小 box_sizes = boxes[:, 2:4] - boxes[:, 0:2] # [13, 13, 3, 5+num_class+1] `5` means coords and labels. `1` means mix up weight. # 储存数据的矩阵,初始全部数据都是0,分别对应的是三个不同尺寸的特征图 # 矩阵的shape: [height, width , 3, 4 + 1 + num_class + 1], # 最后一维的第一个1表示的是前景后景的标志位,最后一个1表示的是mix up的权重. y_true_13 = np.zeros((img_size[1] // 32, img_size[0] // 32, 3, 6 + class_num), np.float32) y_true_26 = np.zeros((img_size[1] // 16, img_size[0] // 16, 3, 6 + class_num), np.float32) y_true_52 = np.zeros((img_size[1] // 8, img_size[0] // 8, 3, 6 + class_num), np.float32) # mix up weight default to 1. # mix up的权重默认值设置为1 y_true_13[..., -1] = 1. y_true_26[..., -1] = 1. y_true_52[..., -1] = 1. # 将他们放在一起,可以统一操作 y_true = [y_true_13, y_true_26, y_true_52] # [N, 1, 2] # 扩展一维,shape: [N, 1, 2] # 需要注意的是,这里的N表示的是目标框的数目,而不是样本的数据. box_sizes = np.expand_dims(box_sizes, 1) # broadcast tricks # [N, 1, 2] &amp; [9, 2] ==&gt; [N, 9, 2] # 使用numpy的广播机制,很容易计算出目标框和anchor之间的交集部分. mins = np.maximum(- box_sizes / 2, - anchors / 2) maxs = np.minimum(box_sizes / 2, anchors / 2) # [N, 9, 2] whs = maxs - mins # [N, 9] # 计算每一个目标框和每一个anchor的iou值 iou = (whs[:, :, 0] * whs[:, :, 1]) / ( box_sizes[:, :, 0] * box_sizes[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10) # [N] # 计算每一个目标框和某一个anchor之间的最佳iou值,并返回最佳iou值对应的下标索引 best_match_idx = np.argmax(iou, axis=1) # 这个字典是为了后续的计算方便才定义的 ratio_dict = {1.: 8., 2.: 16., 3.: 32.} for i, idx in enumerate(best_match_idx): # idx: 0,1,2 ==&gt; 2; 3,4,5 ==&gt; 1; 6,7,8 ==&gt; 0 # 根据上面的下标索引,下面的代码可以计算出该目标框应该对应与哪一张特征图. # 因为不同的anchor对应于不同尺寸的特征图, # 所以如果一个目标框和其中一个anchor具有最大的iou,那么我们应该将该目标框和这个anchor对应的特征图联系起来. feature_map_group = 2 - idx // 3 # scale ratio: 0,1,2 ==&gt; 8; 3,4,5 ==&gt; 16; 6,7,8 ==&gt; 32 # 这里就是利用了前面定义的字典,方便的获取缩放倍数 ratio = ratio_dict[np.ceil((idx + 1) / 3.)] # 计算目标框的中心.这里是指缩放之后的中心 x = int(np.floor(box_centers[i, 0] / ratio)) y = int(np.floor(box_centers[i, 1] / ratio)) # 根据特征图的编号,获取anchor的下标索引 k = anchors_mask[feature_map_group].index(idx) # 类别标记 c = labels[i] # print(feature_map_group, '|', y,x,k,c) # 分别将数据添加到合适的位置,其中需要注意的是k的使用,它表明的是目标框对应的是哪个anchor # 目标框的中心 y_true[feature_map_group][y, x, k, :2] = box_centers[i] # 目标框的尺寸 y_true[feature_map_group][y, x, k, 2:4] = box_sizes[i] # 前景标记 y_true[feature_map_group][y, x, k, 4] = 1. # 类别标记 y_true[feature_map_group][y, x, k, 5 + c] = 1. # mix up权重 y_true[feature_map_group][y, x, k, -1] = boxes[i, -1] # 当我们处理好所有的目标框之后就返回 return y_true_13, y_true_26, y_true_52def parse_data(line, class_num, img_size, anchors, mode): ''' param: line: a line from the training/test txt file class_num: totol class nums. img_size: the size of image to be resized to. [width, height] format. anchors: anchors. mode: 'train' or 'val'. When set to 'train', data_augmentation will be applied. ''' # 如果line不是一个list，说明这里的line是一个str if not isinstance(line, list): # 直接处理即可，返回图片索引，图片路径，以及gt目标框的坐标和对应的labels img_idx, pic_path, boxes, labels = parse_line(line) # 根据图片路径读取图片 img = cv2.imread(pic_path) # expand the 2nd dimension, mix up weight default to 1. # 扩展矩阵的维度，这里主要是在每一行的末尾添加一个表示mix up权重的信息，此处默认设置为1 boxes = np.concatenate((boxes, np.full(shape=(boxes.shape[0], 1), fill_value=1., dtype=np.float32)), axis=-1) else: # the mix up case # 如果line表示的是一个list，说明需要使用mix up策略 # 处理第一张图片 _, pic_path1, boxes1, labels1 = parse_line(line[0]) # 读取第一张图片 img1 = cv2.imread(pic_path1) # 处理第二张图片 img_idx, pic_path2, boxes2, labels2 = parse_line(line[1]) # 读取第二张图片 img2 = cv2.imread(pic_path2) # 将他们混合在一起 img, boxes = mix_up(img1, img2, boxes1, boxes2) labels = np.concatenate((labels1, labels2)) # 如果是训练阶段，则会做一些数据增强的操作，如随机颜色抖动，随机裁剪，随机翻转等操作 if mode == 'train': # random color jittering # NOTE: applying color distort may lead to bad performance sometimes # img = random_color_distort(img) # random expansion with prob 0.5 if np.random.uniform(0, 1) &gt; 0.5: img, boxes = random_expand(img, boxes, 2) # random cropping h, w, _ = img.shape boxes, crop = random_crop_with_constraints(boxes, (w, h)) x0, y0, w, h = crop img = img[y0: y0+h, x0: x0+w] # resize with random interpolation h, w, _ = img.shape interp = np.random.randint(0, 5) img, boxes = resize_with_bbox(img, boxes, img_size[0], img_size[1], interp) # random horizontal flip h, w, _ = img.shape img, boxes = random_flip(img, boxes, px=0.5) else: img, boxes = resize_with_bbox(img, boxes, img_size[0], img_size[1], interp=1) # 将颜色的通道顺序进行更改 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) # 规范化数据至0~1 # the input of yolo_v3 should be in range 0~1 img = img / 255. # 将给出的gt 目标框进行处理，返回对应的gt矩阵，用以后面的损失计算。 y_true_13, y_true_26, y_true_52 = process_box(boxes, labels, img_size, class_num, anchors) # 返回 return img_idx, img, y_true_13, y_true_26, y_true_52def get_batch_data(batch_line, class_num, img_size, anchors, mode, multi_scale=False, mix_up=False, interval=10): ''' generate a batch of imgs and labels param: batch_line: a batch of lines from train/val.txt files class_num: num of total classes. img_size: the image size to be resized to. format: [width, height]. anchors: anchors. shape: [9, 2]. mode: 'train' or 'val'. if set to 'train', data augmentation will be applied. multi_scale: whether to use multi_scale training, img_size varies from [320, 320] to [640, 640] by default. Note that it will take effect only when mode is set to 'train'. interval: change the scale of image every interval batches. Note that it's indeterministic because of the multi threading. ''' # 全局的计数器 global iter_cnt # multi_scale training # 是否使用多种尺寸进行训练， 默认是False if multi_scale and mode == 'train': # 设置随机数种子 random.seed(iter_cnt // interval) # 设定选择范围，并随机采样 random_img_size = [[x * 32, x * 32] for x in range(10, 20)] img_size = random.sample(random_img_size, 1)[0] # 计数器加1 iter_cnt += 1 # 用以保存数据的list img_idx_batch, img_batch, y_true_13_batch, y_true_26_batch, y_true_52_batch = [], [], [], [], [] # mix up strategy # 是否使用mix up策略，默认是False if mix_up and mode == 'train': mix_lines = [] batch_line = batch_line.tolist() for idx, line in enumerate(batch_line): if np.random.uniform(0, 1) &lt; 0.5: mix_lines.append([line, random.sample(batch_line[:idx] + batch_line[idx+1:], 1)[0]]) else: mix_lines.append(line) batch_line = mix_lines # 对一个batch中的数据，这里的line一般指的是一行文本数据 for line in batch_line: # 处里数据中的信息，主要是数据索引（一般用不上）图片的像素矩阵，不同特征图所对应的gt信息。 img_idx, img, y_true_13, y_true_26, y_true_52 = parse_data(line, class_num, img_size, anchors, mode) # 附加到这些list的末尾 img_idx_batch.append(img_idx) img_batch.append(img) y_true_13_batch.append(y_true_13) y_true_26_batch.append(y_true_26) y_true_52_batch.append(y_true_52) # 使用numpy处理一下 img_idx_batch, img_batch, y_true_13_batch, y_true_26_batch, y_true_52_batch = np.asarray(img_idx_batch, np.int64), np.asarray(img_batch), np.asarray(y_true_13_batch), np.asarray(y_true_26_batch), np.asarray(y_true_52_batch) # 返回 return img_idx_batch, img_batch, y_true_13_batch, y_true_26_batch, y_true_52_batch","link":"/2019/05/22/Note13-YOLOv3-part07/"},{"title":"代码实现二维平面上的卷积及其反向传播","text":"前言 在前面的叙述中，我们都是在二维平面上做卷积操作，并在此基础上进行了反向传播算法的推导和计算，但是，如果仅仅限于理论怕是很难验证我们算法的是否是真的可以让结果收敛，因此，这一篇文章中，我们就通过代码来验证一下算法在实际过程中的收敛情况。 必须要说明的是，之前讲解的算法都是十分朴素的，具有很大的改进空间，代码的实现也是基于这种朴素的计算过程，因此，并未对算法进行优化。 一、代码 在代码中，我们实现了一个20x20大小的矩阵的卷积操作，并在给定的卷积核的情况下计算出了我们需要的结果，然后随机初始化卷积核，希望通过训练，让卷积的结果尽可能和在给定的卷积核的情况下一致。 代码的中每一个函数的详细功能都做了注释。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225# import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt# 学习率learning_rate = 0.00003def convolution(x, kernel, stride): \"\"\" 二维平面上的卷积，padding为VALID :param x: 被卷积的特征矩阵，是一个二维矩阵 :param kernel: 卷积核参数，为一个二维矩阵 :param stride: 步长信息，一个正整数 :return: 卷积之后的矩阵信息 \"\"\" assert len(x.shape) == 2 assert len(kernel.shape) == 2 assert type(stride) is int assert (x.shape[0] - kernel.shape[0]) % stride == 0 and (x.shape[1] - kernel.shape[1]) % stride == 0 result = np.zeros([(x.shape[0] - kernel.shape[0]) // stride + 1, (x.shape[1] - kernel.shape[1]) // stride + 1]) for i in range(0, x.shape[0] - kernel.shape[0] + 1, stride): for j in range(0, x.shape[1] - kernel.shape[1] + 1, stride): sum = 0 for p in range(kernel.shape[0]): for k in range(kernel.shape[1]): sum += x[i + p][j + k] * kernel[p][k] result[i // stride][j // stride] = sum return result# 对矩阵的上下左右分别进行填补0的操作。def padding_zeros(x, left_right, top_bottom): \"\"\" 对矩阵的外围进行填补0的操作。 :param x: 一个二维矩阵 :param left_right: 一个长度为2的数组，分别表示左侧和右侧需要填补的0的层数 :param top_bottom: 一个长度为2的数组，分别表示上侧和下侧需要填补的0的层数 :return: 填补之后的矩阵 \"\"\" assert len(x.shape) == 2 assert len(left_right) == 2 and len(top_bottom) == 2 new_x = np.zeros([top_bottom[0] + top_bottom[1] + x.shape[0], left_right[0] + left_right[1] + x.shape[1]]) new_x[top_bottom[0]: top_bottom[0] + x.shape[0], left_right[0]: left_right[0] + x.shape[1]] = x return new_xdef insert_zeros(x, stride): \"\"\" 在矩阵的每两个相邻元素之间插入一定数目的0 :param x: 一个二维矩阵 :param stride: 一个非负数 :return: 插入0之后的矩阵 \"\"\" assert len(x.shape) == 2 assert type(stride) is int and stride &gt;= 0 new_x = np.zeros([(x.shape[0] - 1) * stride + x.shape[0], (x.shape[1] - 1) * stride + x.shape[1]]) for i in range(x.shape[0]): for j in range(x.shape[1]): new_x[i * (stride + 1)][j * (stride + 1)] = x[i][j] return new_xdef rotate_180_degree(x): \"\"\" 将矩阵旋转180°，这一步主要是针对卷积核而言。 :param x: 需要被旋转的矩阵 :return: 旋转之后的矩阵 \"\"\" assert len(x.shape) == 2 return np.rot90(np.rot90(x))class conv(object): def __init__(self, kernel, stride=1, bias=None): \"\"\" 表示卷积的类 :param kernel: 卷积核参数，可以是一个整数，表示卷积核尺寸，也可以是一个二维的矩阵。 :param stride: 一个正整数，表示步长信息 :param bias: 偏置量，一个浮点数 \"\"\" if type(kernel) is int: self.kernel = np.random.normal(0, 1, [kernel, kernel]) self.kernel_size = kernel elif type(kernel) is np.ndarray and len(kernel.shape) == 2: assert kernel.shape[0] == kernel.shape[1] self.kernel = kernel self.kernel_size = kernel.shape[0] self.bias = np.random.normal(0, 1) if bias is None else bias self.stride = stride self.x = None def forward(self, x): \"\"\" 前向传播的计算 :param x: 输入矩阵，是一个二维矩阵 :return: 经过卷积并加上偏置量之后的结果 \"\"\" self.x = x return convolution(x, self.kernel, stride=self.stride) + self.bias def backward(self, error): \"\"\" 卷积的反向传播过程 :param error: 接收到的上一层传递来的误差矩阵 :return: 应该传递给下一层的误差矩阵 \"\"\" # 首先在矩阵的每两个元素之间插入合适数目的0 error_inserted = insert_zeros(error, stride=self.stride - 1) # 在上面的矩阵外围填补上合适数目的0 error_ = padding_zeros(error_inserted, [self.kernel_size - 1, self.kernel_size - 1], [self.kernel_size - 1, self.kernel_size - 1]) # 将卷积核旋转180° kernel = self.kernel.copy() kernel = rotate_180_degree(kernel) # 将上面的两个矩阵进行卷积操作，步长为1，求得需要传递给下一层的误差矩阵 error_ = convolution(error_, kernel, 1) # 参数更新 # 将输入矩阵和插入0的矩阵进行步长为1的卷积，得到卷积核的更新梯度 kernel_gradient = convolution(self.x, error_inserted, 1) # 将误差矩阵中每个元素相加得到偏置量的更新梯度 bias_gradient = np.sum(error_inserted) # 利用学习率更新参数 self.kernel -= kernel_gradient * learning_rate self.bias -= bias_gradient * learning_rate # 返回误差矩阵 return error_class sigmoid(object): def __init__(self): self.x = None def forward(self, x): self.x = x return 1.0/(1.0+np.exp(self.x)) def backward(self, error): s = 1.0/(1.0+np.exp(self.x)) return error * s * (1 - s)class relu(object): def __init__(self): self.x = None def forward(self, x): self.x = x return np.maximum(x, 0) def backward(self, error): return error * (self.x &gt; 0)if __name__ == '__main__': map = np.random.normal(1, 1, (20, 20)) print(\"*\"*30) print(\"feature map:\") print(np.round(map, 3)) kernel1 = np.array([[0, 0, 1], [0, 2, 0], [1, 0, 1]], dtype=np.float32) kernel2 = np.array([[1, 0, 0, 1], [1, 0, 0, 1], [0, -1, -1, 0], [0, 1, 1, 0]], dtype=np.float32) print(\"*\"*30) print(\"kernel 1:\\n\", kernel1) print(\"*\" * 30) print(\"kernel 2:\\n\", kernel2) # 这里我们建立两层卷积，卷积核为给定的数值：kernel1和kernel2 conv1 = conv(kernel1, 1, 0) map1 = conv1.forward(map) conv2 = conv(kernel2, 2, 0) target = conv2.forward(map1) print(\"*\"*30) print(\"our target feature:\\n\", np.round(target, 3)) # 建立同样的两层网络，只不过卷积核为随机产生的数值 print(\"\\nBuilding our model...\") conv1 = conv(3, 1, 0) map1 = conv1.forward(map) conv2 = conv(4, 2, 0) map2 = conv2.forward(map1) loss_collection = [] print(\"\\nStart training ...\") for loop in range(2000): # 损失值我们使用简单的差平方和来计算。 loss_value = np.sum(np.square(map2 - target)) loss_collection.append(loss_value) if loop % 100 == 99: print(loop, \": \", loss_value) # 根据损失函数得到最末端的误差矩阵，并逐级向前传递 error_ = 2 * (map2 - target) error_ = conv2.backward(error_) error_ = conv1.backward(error_) # 参数更新之后需要重新进行正向传播，以获得新的输出矩阵 map1 = conv1.forward(map) map2 = conv2.forward(map1) print(\"*\"*30) print(\"Our reconstructed map:\") print(map2) plt.plot(np.arange(len(loss_collection)), loss_collection) plt.show() 二、结果 代码运行如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145******************************feature map:[[-1.493e+00 1.429e+00 4.150e-01 2.294e+00 9.450e-01 1.043e+00 1.115e+00 1.470e+00 1.999e+00 8.890e-01 -1.190e-01 2.020e-01 2.630e-01 1.916e+00 1.292e+00 8.600e-02 2.161e+00 1.212e+00 1.296e+00 3.067e+00] [-9.290e-01 4.850e-01 1.980e+00 2.666e+00 1.507e+00 2.475e+00 7.210e-01 6.480e-01 8.730e-01 2.068e+00 -8.910e-01 7.600e-01 4.270e-01 5.770e-01 5.800e-01 7.930e-01 1.076e+00 1.472e+00 2.294e+00 1.200e-01] [ 1.356e+00 -8.810e-01 1.626e+00 -6.420e-01 4.580e-01 4.310e-01 1.639e+00 5.760e-01 1.101e+00 -1.714e+00 1.070e-01 1.200e+00 1.862e+00 -2.820e-01 2.282e+00 1.285e+00 2.940e-01 8.400e-01 3.070e-01 9.910e-01] [ 4.910e-01 3.052e+00 4.760e-01 7.610e-01 1.366e+00 1.263e+00 2.353e+00 -2.200e-02 -5.740e-01 2.357e+00 2.029e+00 1.604e+00 -2.630e-01 7.030e-01 1.383e+00 1.858e+00 1.831e+00 2.824e+00 9.800e-01 2.470e+00] [ 1.521e+00 3.700e-01 7.260e-01 3.117e+00 -7.180e-01 1.144e+00 1.514e+00 1.212e+00 1.540e+00 1.202e+00 1.326e+00 9.830e-01 7.290e-01 2.144e+00 1.885e+00 3.084e+00 7.270e-01 1.983e+00 1.162e+00 6.660e-01] [ 1.758e+00 1.107e+00 2.234e+00 3.171e+00 1.200e+00 1.257e+00 1.659e+00 1.835e+00 -1.299e+00 1.283e+00 1.034e+00 4.140e-01 1.478e+00 1.042e+00 3.028e+00 9.400e-01 2.014e+00 -9.420e-01 1.578e+00 -4.270e-01] [ 3.420e-01 -9.200e-02 1.097e+00 -1.641e+00 3.155e+00 2.114e+00 8.570e-01 9.910e-01 1.147e+00 1.006e+00 1.065e+00 -4.620e-01 1.560e+00 1.895e+00 -8.060e-01 7.630e-01 3.091e+00 1.262e+00 -1.840e-01 1.220e-01] [ 3.828e+00 2.010e+00 2.188e+00 6.040e-01 1.420e+00 9.900e-02 1.033e+00 7.460e-01 2.611e+00 1.258e+00 9.510e-01 9.210e-01 7.440e-01 7.300e-01 2.780e-01 1.717e+00 2.000e-01 1.595e+00 1.389e+00 1.656e+00] [ 3.350e-01 8.200e-01 3.018e+00 1.879e+00 -2.100e-02 -6.250e-01 1.573e+00 6.390e-01 1.452e+00 -4.680e-01 1.200e+00 1.833e+00 1.526e+00 1.907e+00 1.785e+00 2.051e+00 8.190e-01 1.600e-01 2.093e+00 9.290e-01] [ 9.590e-01 -1.270e-01 2.545e+00 1.877e+00 1.014e+00 5.830e-01 1.600e+00 1.722e+00 1.240e-01 1.378e+00 1.370e-01 1.201e+00 6.540e-01 6.960e-01 1.701e+00 -8.420e-01 1.627e+00 -1.674e+00 -7.400e-02 1.127e+00] [ 7.030e-01 3.290e-01 1.841e+00 1.385e+00 3.680e-01 1.174e+00 1.875e+00 8.580e-01 8.540e-01 1.696e+00 -2.330e-01 -8.310e-01 1.079e+00 1.012e+00 5.040e-01 1.632e+00 2.550e-01 -2.090e-01 8.530e-01 -5.290e-01] [ 4.100e-01 1.175e+00 1.506e+00 -8.640e-01 1.488e+00 1.113e+00 7.790e-01 8.280e-01 1.074e+00 -5.560e-01 -2.760e-01 2.561e+00 1.135e+00 1.375e+00 -8.140e-01 -8.700e-02 -1.120e-01 5.220e-01 -8.720e-01 5.610e-01] [ 1.845e+00 1.736e+00 -3.690e-01 1.212e+00 1.269e+00 1.774e+00 -1.240e+00 -3.770e-01 4.360e-01 9.160e-01 2.761e+00 -1.470e-01 1.095e+00 3.250e-01 -2.150e-01 -2.332e+00 9.290e-01 1.240e+00 5.680e-01 1.836e+00] [-4.660e-01 6.710e-01 1.191e+00 2.862e+00 3.050e-01 1.861e+00 4.020e-01 1.327e+00 7.300e-01 6.270e-01 2.682e+00 3.048e+00 3.630e-01 -3.140e-01 -3.890e-01 2.380e+00 -5.500e-01 1.491e+00 3.220e-01 4.970e-01] [ 7.290e-01 2.541e+00 1.011e+00 -3.260e-01 1.789e+00 6.700e-02 3.560e-01 1.883e+00 2.602e+00 4.550e-01 1.768e+00 3.350e-01 1.590e-01 2.364e+00 3.659e+00 7.420e-01 2.127e+00 2.639e+00 5.050e-01 3.774e+00] [ 2.620e+00 -5.000e-03 1.250e+00 1.045e+00 1.090e+00 1.300e-02 1.816e+00 1.499e+00 -6.680e-01 1.314e+00 5.150e-01 1.111e+00 4.190e-01 1.564e+00 1.990e+00 3.700e-01 -2.270e-01 2.090e-01 1.865e+00 -8.290e-01] [-8.800e-02 2.931e+00 9.130e-01 4.410e-01 -2.790e-01 2.880e-01 -2.830e-01 1.576e+00 1.374e+00 1.437e+00 7.210e-01 2.540e-01 7.540e-01 7.900e-02 8.110e-01 2.291e+00 4.300e-02 1.173e+00 2.160e-01 1.258e+00] [ 6.200e-01 -7.400e-02 -6.030e-01 -2.650e-01 6.640e-01 2.018e+00 -1.574e+00 2.530e-01 1.260e+00 8.890e-01 -2.350e-01 1.029e+00 1.000e-03 2.490e-01 1.300e-02 7.250e-01 1.890e-01 2.984e+00 9.790e-01 9.130e-01] [ 1.443e+00 3.438e+00 8.000e-01 3.240e-01 8.100e-01 4.750e-01 1.041e+00 5.130e-01 1.329e+00 -4.180e-01 2.237e+00 3.070e-01 1.488e+00 1.637e+00 -2.300e-01 1.642e+00 2.034e+00 -9.670e-01 1.079e+00 1.006e+00] [ 2.653e+00 4.840e-01 4.790e-01 -1.493e+00 4.030e-01 -5.900e-02 3.610e-01 -9.410e-01 -6.060e-01 2.276e+00 4.180e+00 3.480e-01 -4.930e-01 1.783e+00 -4.270e-01 1.799e+00 2.069e+00 1.685e+00 1.172e+00 1.912e+00]]******************************kernel 1: [[0. 0. 1.] [0. 2. 0.] [1. 0. 1.]]******************************kernel 2: [[ 1. 0. 0. 1.] [ 1. 0. 0. 1.] [ 0. -1. -1. 0.] [ 0. 1. 1. 0.]]******************************our target feature: [[26.553 18.88 18.124 18.16 8.258 23.685 24.708 24.097] [20.64 31.311 14.439 25.2 16.984 30.628 19.429 32.712] [29.07 19.528 25.637 14.765 27.227 21.041 27.518 10.76 ] [22.549 24.207 16.272 20.756 14.157 24.426 12.429 19.15 ] [16.412 28.147 14.357 18.035 25.29 12.691 9.955 13.582] [19.198 13.485 16.587 13.285 12.428 16.192 11.528 3.429] [15.984 17.609 12.73 27.705 16.503 17.337 16.276 26.504] [11.86 11.337 8.349 13.658 25.007 17.74 20.237 23.013]]Building our model...Start training ...99 : 641.0993619453625199 : 428.8371676892213299 : 297.4697981028265399 : 233.14519479024102499 : 192.98557066294234599 : 162.86663169198815699 : 135.89478966699724799 : 112.77961888797242899 : 92.23756791064116999 : 74.446621074744971099 : 59.2064164734652961199 : 46.576494244467821299 : 36.5031660070200351399 : 28.753778125126171499 : 22.914096973997211599 : 18.4844259118159081699 : 15.0258518201786371799 : 12.2342613631377191899 : 9.928595283175861999 : 8.00538026504608******************************Our reconstructed map:[[26.03217516 18.57601188 17.6417854 17.74106052 8.00899738 23.45962739 24.25683639 23.52688692] [20.34532322 30.86283096 14.08412555 24.79283798 16.8623232 30.0709141 19.06563574 32.19093566] [28.79064215 18.94645701 25.26600894 14.3734212 26.84098616 20.56125496 27.22489939 10.43946569] [22.32314845 23.72247534 15.95624896 20.26777129 13.74630188 24.21656068 12.33211161 18.89299591] [16.02570137 27.59958699 14.17716638 17.78058347 25.12238011 12.49517613 9.7392468 13.45919031] [18.80339915 13.04877141 16.39073949 12.79776179 11.97347116 15.91895708 11.3292454 3.33631504] [15.67305314 17.20668751 12.43561879 27.23227887 16.12905207 17.00011351 15.94369672 26.2171221 ] [11.53226582 11.37091115 8.17071381 13.7191314 24.74154248 17.43440331 19.80145666 22.61327712]] 从上面的结果可以看出，我们经过训练之后得到的重建之后的输出矩阵和原始的目标矩阵已经十分接近。 在训练过程中的loss值绘制成折线图如下： 可以看出，损失值loss在某些地方会存在一些小的波动，但在整体上，损失值loss是处在一直在减小的过程中的，这说明我们的算法是正确的。","link":"/2019/05/24/Note17-ConvBackProp-part4/"},{"title":"线性可分支持向量机","text":"一、支持向量机 支持向量机（support vector machines，简称SVM）是一种非常高效的分类器，在神经网络出现前，SVM被称为最强分类器。SVM的核心思想就是，对于两个类别的数据，每个数据点都可以抽象为多维空间中的一个坐标点，每一个点都对应于一个标签。那么我们已经知道这两个类别可以被一个超平面相隔开，即在超平面的一侧是一个类别，另一侧是另一个类别。很明显。在绝大多数情况下，这样的超平面并不唯一。其中，将两个类别分开，并且距离两个类别中离超平面最近的若干个点距离之和最大的那一个超平面则最能满足我们的要求。因为有这样的一个超平面，所有当我们引入一个新的样本数据时，我们就有最高的把握可以将这个新的样本点分类正确。这就是SVM的核心思想。 在求解SVM之前，不妨思考一下这个超平面 \\(S\\)应该满足怎样的条件。假设超平面 \\(S\\)的方程是 \\(\\omega \\cdot x + b = 0\\)，这里的超平面可以完美的将我们的样本数据分开。我们设样本数据集为 \\(D\\)，一共包含 \\(m\\)个样本数据，每个样本都有一个标签 \\(y_i\\)，标签的值是1或者-1，即 \\(D = \\{(x_1, y_1),(x_2, y_2),...,(x_m, y_m)\\}\\)，其中 \\(y_i \\in \\{1, -1\\}\\)。这里的1或者-1本质上是SVM结果的符号。根据SVM的定义，我们可以发现，超平面 \\(S\\)到每一个类别的样本数据的最小值是相同的，因为只有这样才可以保证SVM对没一个类别都能公平地进行分类。 函数间隔 首先，我么可以将上述地超平面 \\(S\\)方程看作一个函数 \\(f(x) = \\omega \\cdot x + b\\)。同时，我们引入一个新的定义，叫做函数间隔。当我们将上述的超平面方程看作是一个函数时，如果我们代入某些坐标点，这些坐标点位于超平面上，则\\(f(x) = 0\\)。如果不在超平面上，则函数 \\(f(x) \\neq 0\\)。于是我们将代入某个坐标带你到函数中得到的函数值输出叫做函数间隔。 由上面，我们发现，如果等比例地改变 \\(\\omega, b\\)的值，该超平面并不会改变，但是得到的函数间隔会相应改变，这是因为在本质上，超平面的放长依赖于 \\(\\omega, b\\)之间的比值关系，和具体的数值无关，而函数间隔则会依赖于其具体数值。我们不妨取其中超平面其中一侧的类别中，所有该类别的样本数据距离超平面的函数间隔都是正数，其中最小的那个正数我们记作 \\(a\\)。由于函数间隔依赖于 \\(\\omega, b\\)的具体数值，那么我们总是可以动态地等比例调整 \\(\\omega, b\\)的数值，让 \\(a = 1\\)。于是，我们将这样的一对 \\(\\omega, b\\)记作 \\(\\omega_0, b_0\\)，对应的超平面方程就是 \\(\\omega_0 \\cdot x + b_0 = 0\\)。 同时，我们观察到，在函数距离为正的样本数据中，标签也可以设置为1，函数距离为负的样本数据中，标签可以被设置为-1。那么我们对所有的样本数据，都会满足下面的关系： \\[ y_i (\\omega_0 \\cdot x_i + b_0) \\geq 1, \\quad(1 \\leq i \\leq m) \\tag{1} \\] 之所以会大于等于1，是因为我们在之前已经等比例调整过 \\(\\omega,b\\)的数值，使之满足上面的关系。但是，函数距离并不能准确地刻画超平面到最近的样本距离之和最大这一条件，所以我们就必须用一个准确的距离表示方法来计算最近距离之和。根据空间中点线之间的距离计算公式，我们取函数距离分别为1和-1的两个点，分别记作 \\((x_a, y_a),(x_b, y_b)\\)，我们的目标就是要求这两个点距离超平面的距离之和最大，故我们有： \\[ \\max(\\frac{|w_0 \\cdot x_a + b_0|}{||w_0||} + \\frac{|w_0 \\cdot x_b + b_0|}{||w_0||}) \\tag{2} \\] 考虑到 \\(w_0 \\cdot x_a + b_0 = 1\\)和\\(w_0 \\cdot x_b + b_0 = -1\\)，代入公式，会有： \\[ \\max(\\frac{|w_0 \\cdot x_a + b_0|}{||w_0||} + \\frac{|w_0 \\cdot x_b + b_0|}{||w_0||}) = \\max(\\frac{1}{||w_0||} + \\frac{1}{||w_0||}) = \\max(\\frac{2}{||w_0||}) \\tag{3} \\] 于是，我么可以将SVM的求解问题改写成如下的形式： \\[ \\begin{aligned} \\max \\limits_{\\omega_0, b_0} &amp;\\quad \\frac{2}{||w_0||} \\\\ s.t.&amp;\\quad y_i (\\omega_0 \\cdot x_i + b_0) \\geq 1, \\quad(1 \\leq i \\leq m) \\end{aligned} \\tag{4} \\] 考虑到求解最大的 \\(\\frac{2}{||w_0||}\\)和求解最小的 \\(\\frac{1}{2}||w_0||^2\\)本质上是等价的，因此，可以将上面的式子改写如下： \\[ \\begin{aligned} \\min \\limits_{\\omega_0, b_0} &amp;\\quad \\frac{1}{2}||w_0||^2 \\\\ s.t.&amp;\\quad y_i (\\omega_0 \\cdot x_i + b_0) \\geq 1, \\quad(1 \\leq i \\leq m) \\end{aligned} \\tag{5} \\] 于是我们就得到了线性可分支持向量机的学习的最优化问题。 二、学习的对偶问题 有时候原始问题并不是很好求解，那么我们可以使用拉格朗日对偶性来求解问题，通过求解原始问题的对偶问题来获取原问题的最优解。 我们首先对上面的公式(5)进行一个小小的改动，并用 \\(\\omega, b\\)替代式子中的 \\(\\omega_0, b_0\\)。如下： \\[ \\begin{aligned} \\min \\limits_{\\omega, b} &amp;\\quad \\frac{1}{2}||w||^2 \\\\ s.t.&amp;\\quad y_i (\\omega \\cdot x_i + b) - 1\\geq 0, \\quad(1 \\leq i \\leq m) \\end{aligned} \\tag{6} \\] 由于公式中有 \\(m\\)个约束条件，那么我们必须要使用 \\(m\\)个拉格朗日乘子进行约束，我们对每一个约束条件都使用一个 \\(\\alpha_i\\)参数进行约束，有 \\(\\alpha = (\\alpha_1, \\alpha_2,...,\\alpha_m), \\alpha_i \\geq 0, 1 \\leq i \\leq m\\)。所以我们可以将拉格朗日函数定义如下： \\[ \\begin{aligned} L(\\omega, b, \\alpha) &amp;= \\frac{1}{2}||\\omega||^2 - \\sum_{i= 1}^{m} \\alpha_i(y_i (\\omega \\cdot x_i + b) - 1) \\\\ &amp;= \\frac{1}{2}||\\omega||^2 - \\sum_{i= 1}^{m} (\\alpha_i y_i (\\omega \\cdot x_i + b) - \\alpha_i) \\\\ &amp;= \\frac{1}{2}||\\omega||^2 - \\sum_{i= 1}^{m} \\alpha_i y_i (\\omega \\cdot x_i + b) + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= \\frac{1}{2}||\\omega||^2 - \\sum_{i= 1}^{m} \\alpha_i y_i \\omega \\cdot x_i - \\sum_{i= 1}^{m} \\alpha_i y_i b + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= \\frac{1}{2}||\\omega||^2 - \\omega \\cdot \\sum_{i= 1}^{m} \\alpha_i y_i x_i - b \\sum_{i= 1}^{m} \\alpha_i y_i + \\sum_{i= 1}^{m} \\alpha_i \\end{aligned} \\tag{7} \\] 根据拉格朗日对偶性，原始问题所对应的对偶问题就是和 \\(L(\\omega, b, \\alpha)\\) 有关的极大极小值问题，即： \\[ \\max \\limits_{\\alpha} \\min \\limits_{\\omega, b} L(\\omega, b, \\alpha) \\tag{8} \\] 对于上面的公式，我们必须先求得 \\(L(\\omega, b, \\alpha)\\)对于 \\(\\omega, b\\)的极小值，再求 \\(L(\\omega, b, \\alpha)\\) 对于 \\(\\alpha\\) 的极大值。 求 \\(L(\\omega, b, \\alpha)\\) 对于 \\(\\omega, b\\) 的极小值 我们对上面的 \\(L(\\omega, b, \\alpha)\\) 分别求相对于 \\(\\omega, b\\) 的偏导数，如下： \\[ \\frac{\\partial}{\\partial \\omega} L(\\omega, b, \\alpha)= \\omega - \\sum_{i= 1}^{m} \\alpha_i y_i x_i \\tag{9} \\] \\[ \\frac{\\partial}{\\partial b} L(\\omega, b, \\alpha) = - \\sum_{i= 1}^{m} \\alpha_i y_i \\tag{10} \\] 分别令公式(8)，(9)的偏导数为0，于是我们有： \\[ \\omega = \\sum_{i= 1}^{m} \\alpha_i y_i x_i \\tag{11} \\] \\[ \\sum_{i= 1}^{m} \\alpha_i y_i = 0 \\tag{12} \\] 将公式(11)，(12)代入公式(7)，可以得到： \\[ \\begin{aligned} L(\\omega, b, \\alpha) &amp;= \\frac{1}{2}||\\omega||^2 - \\omega \\cdot \\sum_{i= 1}^{m} \\alpha_i y_i x_i - b \\sum_{i= 1}^{m} \\alpha_i y_i + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= \\frac{1}{2} \\omega \\cdot \\omega - \\omega \\cdot \\omega + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= -\\frac{1}{2} \\omega \\cdot \\omega + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= -\\frac{1}{2} (\\sum_{i= 1}^{m} \\alpha_i y_i x_i)(\\sum_{j= 1}^{m} \\alpha_j y_j x_j) + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= -\\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i= 1}^{m} \\alpha_i \\end{aligned} \\tag{13} \\] 所以问题就变成了求解下面式子的极大值： \\[ \\max \\limits_{\\alpha} \\quad -\\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i= 1}^{m} \\alpha_i \\] 反转一下正负号，就会有： \\[ \\min \\limits_{\\alpha} \\quad \\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i= 1}^{m} \\alpha_i \\tag{14} \\] 所以原问题的对偶问题就变成了如下的形式： \\[ \\begin{aligned} \\min \\limits_{\\alpha} &amp; \\quad \\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i= 1}^{m} \\alpha_i \\\\ s.t. &amp; \\quad \\sum_{i= 1}^{m} \\alpha_i y_i = 0 \\\\ &amp; \\quad \\alpha_i \\geq 0,\\quad1 \\leq i \\leq m \\end{aligned} \\tag{15} \\] 以上就是我们需要求出的SVM原问题的对偶形式。 求解 \\(\\omega\\) 和 \\(b\\) 参数 我们求出最后满足公式(15)所设定的条件的最优解的 \\(\\alpha\\)的数值之后，接下来就需要计算超平面的 \\(\\omega\\) 和 \\(b\\) 参数了。根据公式(11)，再结合我们计算出的 \\(\\alpha\\)的最优解，就可以计算出超平面的 \\(\\omega\\)参数，即： \\[ \\omega^* = \\sum_{i= 1}^{m} \\alpha_i y_i x_i \\tag{16} \\] 由于超平面方程只和某些特定的数据样本有关，和很多数据样本无关，这些无关的样本点对应的 \\(\\alpha\\) 参数为0。因此，我们可以选择某一个 \\(\\alpha\\)的大于0的分量 \\(\\alpha_i\\)，就可以求得超平面的 \\(b\\) 参数。因为对于这类样本数据，我们可以知道它的函数间隔是1（或者-1），于是有： \\[ y_i (\\omega^* \\cdot x_i + b^*) = 1 \\tag{17} \\] 由上可以计算出 \\(b^*\\) 的数值为： \\[ b^* = y_i - \\omega^* \\cdot x_i \\\\ b^* = y_i - x_i \\cdot \\sum_{j = 1}^{m} \\alpha_j y_j x_j \\tag{18} \\] 三、支持向量和决策函数 当我们根据上面的对偶问题求解出了需要的 \\(\\omega^*\\) 和 \\(b^*\\) 参数后，我们就可以知道该分类模型的决策函数如下： \\[ f(x) = sign(\\omega^* \\cdot x + b^*) \\tag{19} \\] 其中， \\(sign()\\) 函数表示的是取数值的符号，如果数值是正数，返回1，如果数值是负数，返回-1。 在求解 \\(\\alpha\\) 参数的过程中，我们发现 \\(\\alpha\\) 的很多分量是0，这表明它在SVM中不贡献求解信息，那些 \\(\\alpha\\) 的分量不是0的样本才是提供了计算超平面的数据，因此，我们将这一类数据称为支持向量。 以上就是线性可分SVM的全部求解过程。","link":"/2019/06/15/Note21-SVM-Hard-Margin/"},{"title":"线性不可分支持向量机","text":"一、硬间隔SVM和软间隔SVM 前面简单介绍了以下线性可分SVM，本质上前面介绍的SVM属于硬间隔SVM，因为我们已经知道存在一个超平面 \\(S:\\omega \\cdot x + b = 0\\) 可以将所有的样本数据完美分开。但是实际上，并不是所有的样本数据都这么完美。有时候存在这种情况，给定一个样本数据集，绝大多数的样本都是线性可分的，但是存在某些特别的样本是线性不可分的。例如，如果我们将 \\(y = 0\\) 这条直线看作一个分类的超平面， \\(y = 0\\) 以上的点标记为1， \\(y = 0\\) 以下的点标记为-1，那么绝大多数的样本点符合这个规则，但是有一些点位于 \\(y = 0\\) 以上，却被标记为-1，有些点位于 \\(y = 0\\) 以下，却被标记为了1，这样就会导致整个数据集线性不可分。 线性不可分的数据集不能使用硬间隔SVM，根本原因在于硬间隔SVM对于函数距离的使用较为严格，必须满足 \\(y_i(\\omega \\cdot x + b) \\geq 1\\) 这个条件。所以为了可以解决上面提到的问题，我们可以在硬间隔SVM的表达式上进行一些修改。既然条件比较严格，那么我们就对每一个条件进行一定的松弛操作，所以我们需要引入一个新的松弛变量 \\(\\xi\\) ，于是对每一个条件，我们有： \\[ y_i (\\omega \\cdot x_i + b) \\geq 1 - \\xi_i \\quad (1 \\leq i \\leq m) \\tag{1} \\] 其中， \\(\\xi_i \\geq 0\\)。因为引入了多余的参数，所有我们需要对原目标函数和松弛变量之间的关系进行平衡，因此，目标函数更改为： \\[ \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i \\quad (1 \\leq i \\leq m) \\tag{2} \\] 这里的 \\(C\\) 是一个人为设定的超参数，通常 \\(C &gt; 0\\)，或者可以叫做平衡系数，或者惩罚参数，主要是用来平衡两者之间的关系。当 \\(C\\) 比较大的时候对那些对那些误分类的样本数据的惩罚作用就会比较大，反之则会比较小，所以最小化目标函数就包括了既要使得超平面 \\(S\\) 到最近样本的 距离之和最小，又要使得误分类的数据样本的数目最小。 所以，我们需要求解的问题就变成了如下形式（ \\(m\\) 为样本数目）： \\[ \\begin{aligned} \\min \\limits_{\\omega, b, \\xi} &amp; \\quad \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i \\\\ s.t. &amp; \\quad y_i (\\omega \\cdot x_i + b) \\geq 1 - \\xi_i \\quad (1 \\leq i \\leq m) \\\\ &amp; \\quad \\xi_i \\geq 0 \\quad (1 \\leq i \\leq m) \\end{aligned} \\tag{3} \\] 二、学习的对偶问题 首先我们对式(3)进行一个小小的变化，如下： \\[ \\begin{aligned} \\min \\limits_{\\omega, b, \\xi} &amp; \\quad \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i \\\\ s.t. &amp; \\quad y_i (\\omega \\cdot x_i + b) - 1 + \\xi_i \\geq 0 \\quad (1 \\leq i \\leq m) \\\\ &amp; \\quad \\xi_i \\geq 0 \\quad (1 \\leq i \\leq m) \\end{aligned} \\tag{4} \\] 和前面一样，我们对上面的式子使用拉格朗日数乘法，因为上面由两个不同类型的约束条件，所以我们需要使用两个不同的变量进行控制。在这里我们使用 \\(\\alpha = (\\alpha_1, \\alpha_2, ..., \\alpha_i)\\) 来对第一个条件进行约束，使用 \\(\\mu = (\\mu_1, \\mu_2, ..., \\mu_i)\\) 来对第二个条件进行约束，于是我们的问题就转换成了求解下面的式子的极大极小值的问题（ \\(L(w, b, \\xi, \\alpha, \\mu)\\) 表示我们使用的拉格朗日函数）： \\[ \\max_{\\alpha, \\mu} \\min_{\\omega, b, \\xi} \\quad L(w, b, \\xi, \\alpha, \\mu) \\tag{5} \\] 我们的拉格朗日函数定义如下： \\[ \\begin{aligned} L(w, b, \\xi, \\alpha, \\mu) &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\sum_{i = 1}^{m} \\alpha_i (y_i (\\omega \\cdot x_i + b) - 1 + \\xi_i) - \\sum_{i = 1}^{m} \\mu_i \\xi_i \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\sum_{i = 1}^{m} \\alpha_i y_i (\\omega \\cdot x_i + b) + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} \\alpha_i \\xi_i - \\sum_{i = 1}^{m} \\mu_i \\xi_i \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\sum_{i = 1}^{m} \\alpha_i y_i x_i - b \\sum_{i = 1}^{m} \\alpha_i y_i + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} \\alpha_i \\xi_i - \\sum_{i = 1}^{m} \\mu_i \\xi_i \\end{aligned} \\tag{6} \\] 接着，我们需要先求出目标拉格朗日函数对于 \\(\\omega\\) ，\\(b\\) ，\\(\\xi\\) 的极小值，所以我们需要对着三个变量分别求偏导，（注意，由于 \\(\\xi\\) 的每一个分量都是相同地位的，所以我们只需要对其中的一个分量求偏导即可。）如下： \\[ \\frac{\\partial}{\\partial \\omega} L(w, b, \\xi, \\alpha, \\mu)= \\omega - \\sum_{i = 1}^{m} \\alpha_i y_i x_i \\tag{7} \\] \\[ \\frac{\\partial}{\\partial b} L(w, b, \\xi, \\alpha, \\mu) = \\sum_{i = 1}^{m} \\alpha_i y_i \\tag{8} \\] \\[ \\frac{\\partial}{\\partial \\xi_i} L(w, b, \\xi, \\alpha, \\mu) = C - \\alpha_i - \\mu_i \\tag{9} \\] 我们分别令上面的三个偏导数为0，于是我们可以得到： \\[ \\omega = \\sum_{i = 1}^{m} \\alpha_i y_i x_i \\tag{10} \\] \\[ \\sum_{i = 1}^{m} \\alpha_i y_i = 0 \\tag{11} \\] \\[ C = \\alpha_i + \\mu_i \\tag{12} \\] 我们将上面的结果代入到 \\(L(w, b, \\xi, \\alpha, \\mu)\\) 中： \\[ \\begin{aligned} L(w, b, \\xi, \\alpha, \\mu) &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\sum_{i = 1}^{m} \\alpha_i y_i x_i - b \\sum_{i = 1}^{m} \\alpha_i y_i + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} \\alpha_i \\xi_i - \\sum_{i = 1}^{m} \\mu_i \\xi_i \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\omega + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} \\alpha_i \\xi_i - \\sum_{i = 1}^{m} \\mu_i \\xi_i \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\omega + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} (\\alpha_i \\xi_i + \\mu_i \\xi_i) \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\omega + \\sum_{i = 1}^{m} \\alpha_i - \\sum_{i = 1}^{m} \\xi_i (\\alpha_i + \\mu_i) \\\\ &amp;= \\frac{1}{2} ||\\omega||^2 + C \\sum_{i = 1}^{m} \\xi_i - \\omega \\cdot \\omega + \\sum_{i = 1}^{m} \\alpha_i - C \\sum_{i = 1}^{m} \\xi_i \\\\ &amp;= - \\frac{1}{2} ||\\omega||^2 + \\sum_{i = 1}^{m} \\alpha_i \\\\ &amp;= -\\frac{1}{2} (\\sum_{i= 1}^{m} \\alpha_i y_i x_i)(\\sum_{j= 1}^{m} \\alpha_j y_j x_j) + \\sum_{i= 1}^{m} \\alpha_i \\\\ &amp;= -\\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i= 1}^{m} \\alpha_i \\end{aligned} \\tag{13} \\] 所以问题就变成了求解下面式子的极大值： \\[ \\max \\limits_{\\alpha, \\mu} \\quad -\\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) + \\sum_{i= 1}^{m} \\alpha_i \\] 反转一下正负号，就会有： \\[ \\min \\limits_{\\alpha, \\mu} \\quad \\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i= 1}^{m} \\alpha_i \\tag{14} \\] 所以我们所需要的原问题的对偶问题可以表示如下： \\[ \\begin{aligned} \\min \\limits_{\\alpha, \\mu} &amp;\\quad \\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i= 1}^{m} \\alpha_i \\\\ s.t. &amp;\\quad \\sum_{i = 1}^{m} \\alpha_i y_i = 0 \\\\ &amp;\\quad C - \\alpha_i - \\mu_i = 0 \\\\ &amp;\\quad \\alpha_i \\geq 0 \\\\ &amp;\\quad \\mu_i \\geq 0, \\quad 1 \\leq i \\leq m \\end{aligned} \\tag{15} \\] 观察到上式中存在一个等式关系，于是，我们可以利用这个等式关系消去 \\(\\mu_i\\) 这个变量： \\[ \\because C - \\alpha_i - \\mu_i = 0 \\\\ \\therefore \\mu_i = C - \\alpha_i \\\\ \\therefore \\mu_i = C - \\alpha_i \\geq 0 \\\\ \\therefore C \\geq \\alpha_i \\] 于是，我们的问题可以有如下的表达： \\[ \\begin{aligned} \\min \\limits_{\\alpha} &amp;\\quad \\frac{1}{2} \\sum_{i= 1}^{m}\\sum_{j= 1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) - \\sum_{i= 1}^{m} \\alpha_i \\\\ s.t. &amp;\\quad \\sum_{i = 1}^{m} \\alpha_i y_i = 0 \\\\ &amp;\\quad 0 \\leq \\alpha_i \\leq C, \\quad 1 \\leq i \\leq m \\end{aligned}\\tag{16} \\] 以上就是我们需要求出的线性不可分SVM原问题的对偶形式。 求解 \\(\\omega\\) 和 \\(b\\) 参数 我们求出最后满足公式(16)所设定的条件的最优解的 \\(\\alpha\\) 的数值之后，接下来就需要计算超平面的 \\(\\omega\\) 和 \\(b\\) 参数了。根据公式(10)，再结合我们计算出的 \\(\\alpha\\) 的最优解，就可以计算出超平面的 \\(\\omega\\) 参数，即： \\[ \\omega^* = \\sum_{i= 1}^{m} \\alpha_i y_i x_i \\tag{17} \\] 由于超平面方程只和某些特定的数据样本有关，和很多数据样本无关，这些无关的样本点对应的 \\(\\alpha\\) 参数为0。因此，我们可以选择某一个 \\(\\alpha\\) 的分量 \\(\\alpha_i\\)，该分量满足 \\(0 &lt; \\alpha_i &lt; C\\) ，就可以求得超平面的 \\(b\\) 参数。因为对于这类样本数据，我们可以知道它的函数间隔是1（或者-1），于是有： \\[ y_i (\\omega^* \\cdot x_i + b^*) = 1 \\tag{18} \\] 由上可以计算出 \\(b^*\\) 的数值为： \\[ b^* = y_i - \\omega^* \\cdot x_i \\\\ b^* = y_i - x_i \\cdot \\sum_{j = 1}^{m} \\alpha_j y_j x_j \\tag{18} \\] 三、支持向量和决策函数 当我们根据上面的对偶问题求解出了需要的 \\(\\omega^*\\) 和 \\(b^*\\) 参数后，我们就可以知道该分类模型的决策函数如下： \\[ f(x) = sign(\\omega^* \\cdot x + b^*) \\tag{19} \\] 其中， \\(sign()\\) 函数表示的是取数值的符号，如果数值是正数，返回1，如果数值是负数，返回-1。 在线性不可分SVM的求解过程中，因为引入了松弛变量，所以支持向量的判定会较为复杂。会出现以下的情况： 如果 \\(\\alpha_i &lt; C\\)，这时 \\(\\xi_i = 0\\)，我们可以知道此时该样本位于间隔的边界上，正好属于一个支持向量。 如果 \\(\\alpha_i = C\\)，这时我们知道该样本的数据已经偏离了正确的间隔边界，所以又存在以下的几种情况： 如果此时 \\(0 &lt; \\xi_i &lt; 1\\)，那我们根据函数间隔的性质可以知道，虽然样本数据已经越过了间隔的边界，但是仍然属于分类正确的样本，此时样本数据位于超平面和间隔边界之间。 如果此时 \\(\\xi_i = 1\\)，那么我们根据函数间隔的性质，可以知道的是样本正好位于分类的超平面上。 如果此时 \\(\\xi_i &gt; 1\\)，那么这时候，样本数据已经越过了分类的超平面，已经属于误分类的样本数据了，且 \\(\\xi_i\\) 的数值越大，距离超平面越远。 以上的样本数据统称为支持向量。 综上就是线性不可分SVM的全部求解过程。","link":"/2019/06/15/Note22-SVM-Soft-Margin/"},{"title":"逻辑回归算法","text":"前言 前面主要是讲反向传播和梯度下降的方法，那么其实涉及梯度的机器学习方法并不是只有深度学习一种，逻辑回归也是可以利用梯度的信息进行参数的更新，使得模型逐步满足我们的数据要求。注意，逻辑回归输出的是属于某一种类别的概率，利用阈值的控制来进行判别，因此逻辑回归本质上是一种分类方法。 一、逻辑斯蒂回归 逻辑斯蒂回归（logistic regression，下面简称逻辑回归），是一种十分经典的分类方法。我们首先介绍一下逻辑回归的定义。 假设我们有一个数据集 \\(S\\)，一共包含\\(m\\)个样本数据，即： \\(S = \\{(x_1,y_1),(x_2,y_2),...,(x_m，y_m)\\}\\)，其中，\\(y_i \\in \\{0, 1\\}\\)。为了表示的方便，我们不妨设当\\(y_i = 1\\)时为正样本，当\\(y_i = 0\\)时为负样本，当然，反过来也是可以的，这个并不重要，只不过一般习惯这样表达。 在SVM中，我们根据数据集的分布，求解出了一个二分的超平面 \\(f(x) = \\omega \\cdot x+b\\)，现在我们要对一个新的样本点\\(x_0\\)进行分类预测，需要将这个样本点带入上面的超平面公式。当\\(f(x_0) = \\omega \\cdot x_0 + b &gt; 0\\)时，我们将这个样本点标记为1，当\\(f(x_0) = \\omega \\cdot x_0 + b \\leq 0\\)时，我们将这个样本点标记为-1。观察到SVM只能对新样本输出 \\(\\pm1\\)，无法较为准确的输出样本属于每一个类别的概率究竟是多少。SVM结果的正确性在于它保证了找到的是样本间隔最大的那个超平面，这样就可以保证以最高的精确度区分新的样本。然而SVM却无法对一个新样本的概率进行求解。而这就是逻辑回归主要做的事情，它输出的是一个概率值，当这个概率值大于一定的阈值时，样本标记为1，反之则标记为0。 二、sigmoid函数 熟悉深度学习的人肯定对这个函数非常了解，它是早期深度学习网络经常使用的激活函数之一。它的定义公式如下： \\[ sigmoid(x) = \\frac{1}{1 + e^{-x}} \\quad x \\in R \\] 它的函数图像是一个典型的S型曲线，定义域是全体实数。我们根据公式可以发现，这个函数将全体实数映射到了 \\((0, 1)\\) 区间上，并在这个区间上单调递增，\\(x\\)越大，函数值越大。而这正好符合我们需要的概率分布的规律。 三、逻辑回归模型 二项逻辑回归模型本质上是一个类似下面公式的条件分布： \\[ P(Y = 1 | x) = \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{1} \\] \\[ P(Y = 0 | x) = 1 - \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{2} \\] 其中，第一个式子是我们需要重点关注的。我们对第一个式子右边的分数，上下同时乘以 \\(e^{\\omega \\cdot x + b}\\)，得到： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x + b}}{1 + e^{\\omega \\cdot x + b}} \\tag{3} \\] 以上就是我们经常可以看到的逻辑回归的公式了。 现在我们将偏置量也放进参数 \\(\\omega\\)中，所以变量\\(x\\)的尾部会增加一个多余的维度1来和偏置量进行匹配，于是，我们有如下的表示方式： \\[ \\omega = \\begin{bmatrix} \\omega_1 &amp; \\omega_2 &amp; \\cdots &amp; \\omega_n &amp; b\\end{bmatrix} \\] \\[ x = \\begin{bmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n &amp; 1\\end{bmatrix} \\] 于是，原逻辑回归公式可以有以下的表达： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} \\tag{4} \\] 四、损失函数 很容易想到，损失函数我们仍然可以使用前面介绍的差方和的方法计算。当距离目标越近时，差方和越小，损失就会越小。事实上并不能这样进行处理。 \\[ J(\\omega, b) = \\sum_i( \\frac{1}{1 + e^{-(\\omega \\cdot x_i)}} - y_i)^2 = \\sum_i (\\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} - y_i) ^2\\tag{4} \\] 原因是如果使用差方和作为最后的损失函数，那么我们得到的最后的损失函数并不是一个简单的凹函数（或者凸函数），这个函数存在许多的局部极小值，因此很难通过梯度下降的方法收敛到全局最小值附近，这样导致的结果就是训练出来的模型并不能很好的满足我们的需要，误差较大。 所以我们必须要重新定义一个满足我们条件的损失函数，或者称为目标函数。我们考虑使用极大似然估计的方法进行参数估计。 对于其中的某一个样本，如果该样本的标签为1，那么我们需要极大化\\(P(Y = 1|x)\\)，如果该样本的标签为0，那么我们需要极大化\\(1 - P(Y = 1|x)\\)，于是对于每一个样本数据，综合来看，我们只需要极大化以下的式子即可： \\[ P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\] 上面的式子看起来很吓人，其实本质很简单。于是我们的似然函数可以表示为 \\[ L(\\omega) = \\prod_i P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\tag{5} \\] 由于这里涉及指数，而且是连乘运算，计算不方便，于是我们可以用取对数的方式进行处理，这里可以这样处理的原因是上式取最大的时候，其对数也一定是取最大值，因为对数函数是一个单调函数。于是有： \\[ log L(\\omega) = \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\tag{6} \\] 现在我们可以将之前的计算结果带入到公式(6)中进行化简： \\[ \\begin{aligned} logL(\\omega) &amp;= \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log(1 - \\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) - y_i log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i (log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) - log(\\frac{1}{1 + e^{\\omega \\cdot x_i}})) + log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(e^{\\omega \\cdot x_i}) - log(1 + e^{\\omega \\cdot x_i}) \\\\ &amp;= \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\end{aligned} \\tag{7} \\] 于是我们需要的似然函数就变成了： \\[ logL(\\omega) = \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\tag{8} \\] 上面的似然函数并不能直接进行最优化求解，于是我们常常利用梯度下降的方法进行逐步迭代求解，这就需要对上面的公式进行求导的操作，我们对参数\\(\\omega\\)求导如下： \\[ \\begin{aligned} \\frac{\\partial logL(\\omega)}{\\partial \\omega} &amp;= \\sum_i y_i x_i - \\frac{1}{1+e^{\\omega \\cdot x_i}} e^{\\omega \\cdot x_i} x_i \\\\ &amp;= \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\end{aligned} \\tag{9} \\] 以上就是利用梯度下降算法进行逻辑回归问题的求解的（偏）导数计算公式，在每一轮的迭代过程中，我们对所有的样本进行梯度计算，最后累加梯度，然后按照计算出的梯度信息更新需要的参数。 由于我们这里需要将似然函数极大化，因此和反向传播的梯度下降不同，这里使用的是类似的梯度上升的方法，于是我们有如下的迭代公式：这里的\\(\\alpha\\)指的是学习率（或者说是步长信息） \\[ \\omega := \\omega + \\alpha \\frac{\\partial logL(\\omega)}{\\partial \\omega} \\tag{10} \\] 五、关于逻辑回归的似然函数 在利用不同的方式计算损失函数时，我们总是希望得到的损失函数时一个凸函数（或者凹函数），这样我们就可以保证只有一个全局的最优解，而且不存在局部极小值或者极大值，而这些条件都对我们使用梯度下降方法来求解最优化问题十分有利。如果一个函数的二阶导数总是恒大于0，我们称这个函数为凸函数，如果一个函数的二阶导数总是恒小于0，我们称这个函数为凹函数，凸函数一定存在一个全局最小值，凹函数一定存在一个全局最大值，并且不管是凹函数还是凸函数都不存在局部极小值或者局部最大值。 我们以\\(y = x^2\\)为例，经过计算我们得到它的二阶导数为\\(y\\prime\\prime = 2\\)，是一个大于0的常数，因此该函数是一个凸函数，其不存在各种局部最小值或者局部最大值，只有一个全局最小值0（当然这个全局最小值也可以看作是一个某一个区域内的局部最小值）。 现在我们重新审视一下我们的似然函数，之前我们已经求解出了似然函数的梯度信息（一阶导数），即： \\[ \\frac{\\partial logL(\\omega)}{\\partial \\omega} = \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\tag{11} \\] 我们继续对上式进行求导的操作，有： \\[ \\begin{aligned} \\frac{\\partial^2 logL(\\omega)}{\\partial \\omega^2} &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i (1 + e^{e^{\\omega \\cdot x_i}}) - e^{\\omega \\cdot x_i}(e^{\\omega \\cdot x_i} x_i)}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - \\frac{e^{\\omega \\cdot x_i}}{(1 + e^{\\omega \\cdot x_i})^2} x_i \\cdot x_i \\end{aligned} \\tag{12} \\] 可以发现的是，上式对于任何的一个数据集合来说都是恒小于0的，因此可以说当我们使用极大似然估计作为损失函数时，该函数是一个凹函数，有一个最大值，可以利用梯度下降（严格来说这个应该是梯度上升的方法）进行求解。 通常，当我们求解出最终的参数时，可以获得一个超平面。往往我们将逻辑回归的阈值设置为0.5，将输出值高于0.5的样本标记为正样本，将输出值低于0.5的样本标记为负样本，于是，我们可以得到分类的超平面为： \\[ \\frac{1}{1 + e^{\\omega \\cdot x}} = \\frac{1}{2} \\tag{13} \\] 化简之后，我们可以得到最终的超平面的表达式为： \\[ \\omega \\cdot x = 0 \\tag{14} \\] 六，代码 这里的数据使用的是《机器学习实战》的数据，一共有100组数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100-0.017612 14.053064 0-1.395634 4.662541 1-0.752157 6.538620 0-1.322371 7.152853 00.423363 11.054677 00.406704 7.067335 10.667394 12.741452 0-2.460150 6.866805 10.569411 9.548755 0-0.026632 10.427743 00.850433 6.920334 11.347183 13.175500 01.176813 3.167020 1-1.781871 9.097953 0-0.566606 5.749003 10.931635 1.589505 1-0.024205 6.151823 1-0.036453 2.690988 1-0.196949 0.444165 11.014459 5.754399 11.985298 3.230619 1-1.693453 -0.557540 1-0.576525 11.778922 0-0.346811 -1.678730 1-2.124484 2.672471 11.217916 9.597015 0-0.733928 9.098687 0-3.642001 -1.618087 10.315985 3.523953 11.416614 9.619232 0-0.386323 3.989286 10.556921 8.294984 11.224863 11.587360 0-1.347803 -2.406051 11.196604 4.951851 10.275221 9.543647 00.470575 9.332488 0-1.889567 9.542662 0-1.527893 12.150579 0-1.185247 11.309318 0-0.445678 3.297303 11.042222 6.105155 1-0.618787 10.320986 01.152083 0.548467 10.828534 2.676045 1-1.237728 10.549033 0-0.683565 -2.166125 10.229456 5.921938 1-0.959885 11.555336 00.492911 10.993324 00.184992 8.721488 0-0.355715 10.325976 0-0.397822 8.058397 00.824839 13.730343 01.507278 5.027866 10.099671 6.835839 1-0.344008 10.717485 01.785928 7.718645 1-0.918801 11.560217 0-0.364009 4.747300 1-0.841722 4.119083 10.490426 1.960539 1-0.007194 9.075792 00.356107 12.447863 00.342578 12.281162 0-0.810823 -1.466018 12.530777 6.476801 11.296683 11.607559 00.475487 12.040035 0-0.783277 11.009725 00.074798 11.023650 0-1.337472 0.468339 1-0.102781 13.763651 0-0.147324 2.874846 10.518389 9.887035 01.015399 7.571882 0-1.658086 -0.027255 11.319944 2.171228 12.056216 5.019981 1-0.851633 4.375691 1-1.510047 6.061992 0-1.076637 -3.181888 11.821096 10.283990 03.010150 8.401766 1-1.099458 1.688274 1-0.834872 -1.733869 1-0.846637 3.849075 11.400102 12.628781 01.752842 5.468166 10.078557 0.059736 10.089392 -0.715300 11.825662 12.693808 00.197445 9.744638 00.126117 0.922311 1-0.679797 1.220530 10.677983 2.556666 10.761349 10.693862 0-2.168791 0.143632 11.388610 9.341997 00.317029 14.739025 0 前两列为数据，最后一列为对应数据的标签。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import matplotlib.pyplot as pltimport numpy as npdef read_file(): positive = [] negative = [] f = open(\"testSet.txt\", \"r\") for line in f: l = line.strip() l = l.split(\"\\t\") l = [float(i) for i in l] if l[-1] == 1.0: positive.append([l[0], l[1], 1]) else: negative.append([l[0], l[1], 1]) positive = np.array(positive, dtype=np.float) negative = np.array(negative, dtype=np.float) return positive, negativedef cost(positive, negative, w): c = 0 for i in positive: c += 1.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: c += 0.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) return cdef loop(): positive, negative = read_file() w = np.array([1, 1, 1]) w_copy = w.copy() alpha = 0.001 for i in range(2000): print(cost(positive, negative, w)) gradient = np.array([0., 0., 0.]) for i in positive: gradient += 1.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: gradient += 0.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) w = w + alpha * gradient return positive, negative, w, w_copyif __name__ == '__main__': positive, negative, w, w_origin = loop() plt.scatter(positive[:, 0], positive[:, 1], c=\"red\") plt.scatter(negative[:, 0], negative[:, 1], c='blue') xs = np.linspace(-4, 3.5, 300) ys1 = (w[0] * xs + w[2]) / (- w[1]) ys2 = (w_origin[0] * xs + w_origin[2]) / (- w_origin[1]) plt.plot(xs, ys1, c=\"green\") plt.plot(xs, ys2, c=\"yellow\") plt.show() 代码运行结果如下，其中黄色直线表示的是初始的分隔超平面，绿色的直线表示为分类超平面，可以看出，样本数据可以较好地被分隔开，第二幅图表示的是梯度上升的情况，可以看到，算法可以较好地收敛于全局最优解附近：","link":"/2019/05/10/Note3-Logic-Regression/"},{"title":"主成分分析PCA","text":"数据降维 在很多时候，我们收集的数据样本的维度很多，而且有些维度之间存在某一些联系，比如，当我们想要收集用户的消费情况时，用户的收入和用户的食物支出往往存在一些联系，收入越高往往食物的支出也越高（这里只是举个例子，不一定正确。）。那么在拿到这样的数据的时候，我们首先想到的就是我们需要对其中的信息做一些处理，排除掉一些冗余的维度，保留必要的维度信息。这样一来，我们可以大大减小我们的后期处理的工作量，这就是数据降维的基本要求。 主成分分析PCA 当拿到用户的数据时，如何确定到两个维度之间是否存在联系呢？这个就是我们需要用的协方差矩阵所作的工作。所以在使用PCA之前，我们必须对协方差有个简单的了解。 方差和协方差 我们之前遇到的大多数情况是获取到的都是一维的数据，比如数学课程的考试成绩，每个人都能获得一个分数，这些分数形成了一个一维的数组（向量），如果我们用 \\(X\\) 表示考试成绩的分布，然后我们可以估计出数学课程的考试成绩的均值 \\(\\bar{X}\\) 和方差 \\(S^2\\)，如下： \\[ \\bar{X} = \\frac{1}{N} \\sum_{i = 1}^{N} X_i \\tag{1} \\] \\[ S^2 = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X}) ^2 \\tag{2} \\] 需要注意的是在求解方差的时候，我们使用的分母是 \\(N- 1\\)，而不是 \\(N\\)，这是因为我们以上的数学成绩对真实成绩分布的一次简单抽样，为了获得更为准确的关于原分布的方差估计，我们必须使用这种所谓的“无偏估计”。 以上都是关于单一变量的方差估计，通常，我们记单变量的方差为 \\(var(X)\\)，于是我们有: \\[ var(X) = S^2 = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X}) ^2 \\tag{3} \\] 但是考虑到更普遍的情况，我们不止有数学考试（X），我们还有物理考试（Y），英语考试（Z），我们就需要考虑一个问题，不同的考试的成绩之间是否存在某种联系，于是我们就利用协方差来定义两个随机变量之间的关系。现在我们有两个随机变量，\\(X\\) 和 \\(Y\\)的抽样数据分布，我们将他们之间的协方差定义如下： \\[ cov(X, Y) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\tag{4} \\] 很明显，这里的分母依然采用的是 \\(N - 1\\)，是对原分布的无偏估计。对于只有一个变量的情况，我们有： \\[ cov(X, X) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})(X_i - \\bar{X}) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})^2 = var(X) \\tag{5} \\] 所以方差算是协方差的一个特例。协方差本质上是对两个随机变量的相关性的考察，当两个随机变量之间的协方差大于0时，表示这两个随机变量正相关；当两个随机变量之间的协方差等于0时，表示这两个随机变量没有相关关系；当两个随机变量之间的协方差小于0时，表示这两个随机变量负相关。 需要注意的是，两个随机变量相互独立和两个随机变量不相关是两个不同的概念，两个随机变量独立则一定有这两个随机变量不相关，但是两个随机变量不相关并不一定有这两个随机变量相互独立。 协方差矩阵 假设我们现在有一张数据统计的表格，每一列的数据都已经去中心化，即每一列的数据都已经减去了该列的均值。如下： No. \\(X_1\\) \\(X_2\\) ... \\(X_m\\) 1 \\(a_{11}\\) \\(a_{12}\\) ... \\(a_{1m}\\) 2 \\(a_{21}\\) \\(a_{22}\\) ... \\(a_{2m}\\) ... ... ... ... ... n \\(a_{n1}\\) \\(a_{n2}\\) ... \\(a_{nm}\\) 一共有 \\(n\\) 个样本数据，每个样本数据包括 \\(m\\) 个统计信息。对于每一列，都是一个随机变量的简单抽样，不妨我们将上面的矩阵记作 \\(X\\)，于是，我们有： \\[ X = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nm} \\end{bmatrix} = \\begin{bmatrix} c_{1} &amp; c_{2} &amp; \\cdots &amp; c_{m} \\end{bmatrix} \\tag{6} \\] 其中， \\(c_i\\) 表示的是每一个随机变量的取值分布情况，那么，第 \\(i\\) 个随机变量和第 \\(j\\) 个随机变量之间的协方差就可以表示为： \\[ cov(c_i, c_j) = \\frac{1}{n-1} c_i \\cdot c_j = \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{ki} a_{kj} \\tag{7} \\] 那么每两个随机变量之间的协方差矩阵（不妨叫做 \\(CovMatrix\\) ）可以表示如下： \\[ \\begin{aligned} CovMatrix &amp;= \\begin{bmatrix} cov(c_1, c_1) &amp; cov(c_1, c_2) &amp; \\cdots &amp; cov(c_1, c_m) \\\\ cov(c_2, c_1) &amp; cov(c_2, c_2) &amp; \\cdots &amp; cov(c_2, c_m) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ cov(c_m, c_1) &amp; cov(c_m, c_2) &amp; \\cdots &amp; cov(c_m, c_m) \\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{km} \\\\ \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{km} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{km} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} \\begin{bmatrix} \\sum_{k = 1}^{n} a_{k1} a_{k1} &amp; \\sum_{k = 1}^{n} a_{k1} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{k1} a_{km} \\\\ \\sum_{k = 1}^{n} a_{k2} a_{k1} &amp; \\sum_{k = 1}^{n} a_{k2} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{k2} a_{km} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{k = 1}^{n} a_{km} a_{k1} &amp; \\sum_{k = 1}^{n} a_{km} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{km} a_{km} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} \\begin{bmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{n1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{n2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1m} &amp; a_{2m} &amp; \\cdots &amp; a_{nm} \\\\ \\end{bmatrix} \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nm} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} X^T X \\end{aligned} \\tag{8} \\] 故，对于上述的矩阵 \\(X\\) ，我们可以求得它的协方差矩阵为： \\[ CovMatrix = \\frac{1}{n - 1} X^T X \\tag{10} \\] 其中，\\(n\\) 为样本数目，\\(X^T\\) 表示矩阵 \\(X\\) 的转置。 在处理表格时，我们已经先做了去中心化的操作，目的是使得协方差公式中的 \\(\\bar{X}\\) 为0，这样的话可以直接进行矩阵的相乘，不必在求解协方差的时候一个一个去计算。 特征值和特征向量 在很多时候，我们需要对一个矩阵求解它的特征值和特征向量，特征值和特征向量的定义是，对于一个对称方阵 \\(A\\) ，如果存在一个值 \\(\\lambda\\) 和一个向量 \\(x\\) 满足如下的条件： \\[ Ax = \\lambda x \\tag{11} \\] 那么我们就将 \\(\\lambda\\) 称为该矩阵的特征值，对应的 \\(x\\) 则称之为特征向量。 根据线性代数的相关知识，我们可以知道，对于一个实对称矩阵 \\(A\\)，一定存在 \\(n\\) 个相正交的特征向量，其中 \\(n\\) 为该矩阵的行数（或者说列数）。那么如果我们将对称矩阵的特征值按照从大到小的顺序进行排列，同时，我们对特征向量也按照其对应特征值的大小进行排列，于是，我们有如下的排列： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\\\ x_1, x_2, \\cdots, x_n \\] 对于上面的每一对特征值和特征向量，我们都能满足等式(11)。于是，我们可以将所有的特征向量组合成一个矩阵，记作 \\(W\\)，将所有的特征值依次放入一个 \\(n\\) 阶的全零方阵的对角线元素中，记作 \\(\\Sigma\\)，于是，我们可以获得下面的等式： \\[ AW = W\\Sigma \\tag{12} \\] 在上面的式子右边同时乘以 \\(W^{-1}\\) ，有： \\[ A = W \\Sigma W^{-1} \\tag{13} \\] 现在我们发现对于矩阵 \\(W\\) ，我们可以得到下面的式子： \\[ W W^T = E \\tag{14} \\] 有以上式子的原因是因为我们对所有的特征向量进行了规范化，使得所有的特征向量模长为1，而且由于\\(A\\)是实对称矩阵，所以\\(A\\)的特征向量两两正交，所以有以下的两个式子： \\[ \\lambda_i \\cdot \\lambda_i = 1 \\\\ \\lambda_i \\cdot \\lambda_j = 0, \\quad i \\ne j \\tag{15} \\] 由公式(14)，我们就可以得到： \\[ W^T = W^{-1} \\tag{16} \\] 故，我们重写式(13)，有： \\[ A = W \\Sigma W^T \\tag{17} \\] 需要注意的是，上述的公式需要满足的条件是 \\(A\\)是一个实对称方矩。 主成分分析 很明显，我们需要对原始的数据进行处理，我们希望是两个不同的数据维度之间的相关性越小越好，而同一个维度内部的方差越大越好，因为只有这样，我们才有很好的排除数据维度的相关性并进行数据的降维操作。我们这里设\\(Y\\)是经过我们变换之后的数据矩阵，那么我们的要求就是\\(Y\\)的协方差矩阵是一个对角矩阵，其中所有的非对角线上的元素都是0，而在对角线上的元素是可以为非0的。同时，我们可以设矩阵\\(P\\)是一组线性变换的相互正交的单位基，按照列进行排列。那么对于原始的数据矩阵，我们可以有： \\[ Y = XP \\tag{18} \\] 上面的式子表示的是\\(X\\)矩阵映射到以矩阵\\(P\\)为基的线性变换。我们假设原始数据\\(X\\)的协方差矩阵为\\(B\\)，经过变化之后的数据\\(Y\\)的协方差矩阵为\\(D\\)，那么我们可以有： \\[ \\begin{aligned} D &amp;= \\frac{1}{n - 1}Y^T Y \\\\ &amp;= \\frac{1}{n - 1}(XP)^T (XP) \\\\ &amp;= \\frac{1}{n - 1}P^T X^T XP \\\\ &amp;=\\frac{1}{n - 1}P^T (X^T X) P \\\\ &amp;= P^T (\\frac{1}{n - 1}X^T X) P \\\\ &amp;= P^T B P \\end{aligned} \\tag{19} \\] 从上面的式子中我们不难发现，我们需要找的是这样的一种变换，\\(D\\)是一个对角矩阵，不妨这里假设对角矩阵\\(D\\)的对角线上的元素是从大到小排列的，\\(B\\)是原始数据矩阵的协方差矩阵，因此，我们需要的找到的变换\\(P\\)可以将\\(B\\)映射成对角矩阵\\(D\\)。 此时，我们考虑到对于一个实对称矩阵\\(A\\)，我们有\\(A = W \\Sigma W^T\\)，其中，\\(\\Sigma\\)是矩阵\\(A\\)的特征值组成的对角矩阵，\\(W\\)是矩阵\\(A\\)的特征向量按照对应特征值的排列组成的特征向量矩阵，所以，结合上面的推导，我们不难发现，\\(D\\)矩阵就可以对应于这里的\\(\\Sigma\\)矩阵，\\(W\\)矩阵就可以对应于我们问题中的\\(P\\)矩阵。 \\[ D = P^T B P \\\\ (P^T)^{-1}DP^{-1} = B \\\\ \\] 考虑到\\(P\\)矩阵是由一组正交单位基组成的，所以满足： \\[ P^T P = E \\] 所以有： \\[ P^T = P^{-1} \\] 代入上面的式子，我们有： \\[ (P^T)^{-1}DP^{-1} = P D P^T = B \\] 我们可以发现，我们需要的变换矩阵\\(P\\)就是由\\(B\\)的特征向量组成的。因此，问题就转换成了求解\\(B\\)的特征值和特征向量，而\\(B\\)又是原始矩阵\\(X\\)的协方差矩阵。 由于我们这里主要关心的是维度之间的相关性的大小，而且在后面的处理中，我们需要求解协方差矩阵的特征向量，所以我们可以将求解协方差矩阵的系数省略。 在后面的代码中，我们就直接省略了协方差矩阵的前系数，进一步简化了计算。 数据降维 在前面的式子中，我们并没有对数据的维度进行操作，即我们保留了所有的数据维度，但是并不是所有的数据维度都是足够必要的。由于矩阵的特征值往往会有比较大的差距，当我们求解出了原始数据的协方差矩阵的特征值和特征向量之后，我们可以舍弃掉太小的特征值以及对应的特征向量，这是因为特征值太小，转换之后的对应的数据维度的方差就会很小，也就是说该维度数据之间的差距不大，不能帮我们很好的区分数据之间的差别，舍弃之后也不会丢失太多的信息，舍弃之后反而可以减小数据的维度信息，更好的节省数据的占用空间，这对于一些大量数据的处理是十分有益的。 我们求出原始数据矩阵的协方差矩阵的特征值和特征向量之后，我们将特征向量进行从大到小进行排序，并按照特征值的顺序将特征向量进行排序。我们可以取前\\(K\\)个特征值和特征向量，并将这些向量组合成一个变换矩阵。将这个变换矩阵应用到原始数据矩阵上即可对原始数据进行降维。这里的\\(K\\)按照任务的要求可以有不同的取值。还有一种选取特征值的方法就是选取前\\(K\\)个特征值，这些特征值之和占全部特征值之和的90%或者95%以上。 PCA实例 接下来就是一个PCA的实例，在sklearn的python包中，有一个鸢尾花数据集，包括了150组鸢尾花的数据，分为三类，每一组数据包含四个数据维度，我们的目标就是将这个数据集的数据维度减少到两维以便我们可以在二维平面上进行绘制。 这里使用了numpy自带的特征值和特征向量的求解函数，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import matplotlib.pyplot as pltfrom sklearn.datasets import load_irisimport numpy as npdef load_data(): data = load_iris() y = data.target x = data.data return x, ydef pca(x, dimensions=2): x = np.array(x) assert len(x.shape) == 2, \"The data must have two dimensions.\" # 求解协方差矩阵 matrix = np.matmul(np.transpose(x), x) # 求解特征值和特征向量 values, vectors = np.linalg.eig(matrix) # 按照从大到小的顺序给特征值排序 index = np.argsort(values)[::-1] # 选取前几个特征值较大的特征向量，并组成一个变换矩阵 vectors = vectors[:, index[:dimensions]] # 应用变换矩阵，将原始数据进行映射 x_dimension_reduction = np.matmul(x, vectors) return x_dimension_reductiondef draw(x, y): red_x, red_y = [], [] blue_x, blue_y = [], [] green_x, green_y = [], [] for i in range(len(x)): if y[i] == 0: red_x.append(x[i][0]) red_y.append(x[i][1]) elif y[i] == 1: blue_x.append(x[i][0]) blue_y.append(x[i][1]) else: green_x.append(x[i][0]) green_y.append(x[i][1]) plt.scatter(red_x, red_y, c='r', marker='x') plt.scatter(blue_x, blue_y, c='b', marker='D') plt.scatter(green_x, green_y, c='g', marker='.') plt.show()if __name__ == '__main__': x, y = load_data() # 去中心化 x = x - np.mean(x, 0) x_dimension_reduction = pca(x, 2) draw(x_dimension_reduction, y) 结果如下。可以看到，数据被较好地区分开了，其中一组数据已经完全和其他的两组数据脱离，另外两组数据也有了明显的分隔界限。这证明我们的PCA方法是正确的。","link":"/2019/05/12/Note5-PCA/"},{"title":"反向传播算法（一）之反向传播入门","text":"一、反向传播算法 近年来，深度学习的快速发展带来了一系列喜人的成果，不管是在图像领域还是在NLP领域，深度学习都显示了其极其强大的能力。而深度学习之所以可以进行大规模的参数运算，反向传播算法功不可没，可以说，没有反向传播算法，深度学习就不可能得以快速发展，因此在此之前，有必要了解一下反向传播算法的具体原理和公式推导。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 二、梯度下降 首先反向传播算法的核心思路就是梯度下降，那么我们必须要明白什么是梯度，从几何上理解，一个函数（此处默认该函数处处可导）的图像会在其空间内呈现出一个曲面（曲线）。以 \\(f(x) = x^2 + y ^2\\) 为例，该函数会在三维空间（x, y, z）中形成一个曲面，其中，x, y可看作相互独立的两个变量，那么我们分别对x,y求偏导数，会有 \\(\\frac{\\partial f}{\\partial x} = 2x\\)，\\(\\frac{\\partial f}{\\partial y} = 2y\\)，因此，该函数的梯度可以表示为 \\((2x, 2y)\\)，如在坐标（1， 3）处的梯度，代入公式可以得到（2， 6）。该数值表示在x, y构成的平面上，我们首先所处的位置是（1， 3）点，该处的函数值是10。如果我需要以最快的速度增大函数值，那么我需要根据 向量（2，6） 的方向前进。 因此梯度在几何上的直观理解是一个表明方向的向量，只有朝着这个向量所指示的方向前进，函数值才会增加的最快。可以这么说（不算很严谨），梯度所在的空间是所有的自变量构成的空间，并指示着自变量需要变化的方向。 由于梯度指示的是函数值增大最快的方向，那么我们朝着相反的方向前进，函数值也必定会下降最快（所以我们在公式中是减去梯度，而不是加上梯度），这就是梯度下降算法的核心。由于梯度值只在一个很小的范围内近似保持不变，所以我们需要进行迭代，并且需要用一个步长变量来控制下降的幅度，这个步长变量就是我们经常谈到的学习率。 三、单层全连接层以及单个输出，不使用激活函数 在所有的矩阵相乘的情况中，输入输出之间只有一个全连接层并且该全连接层的输出仅仅是一个常数（或者说是一个1x1大小的矩阵），同时并不使用非线性激活函数的情况是最简单的，因此，首先可以考虑这种最简单的情况。 我们这里假设输入\\(x\\)是一个长度为3的向量，按照Tensor Flow所设定的数据格式，我们这里设定该输入向量为行向量，即\\(x = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix}\\)，输出为一个回归值 \\(\\hat{y}\\)，目标回归值为\\(y\\)，全连接层权值记为 \\(\\omega\\)，（\\(\\omega\\)是一个矩阵，大小为3x1），偏置项记为\\(b\\)，（\\(b\\)也可以视作一个矩阵，大小为1x1，也可以看作是一个数值，因为这里不关心\\(b\\)的维度信息，因此不做严格的区分。）。当我们定义了以上的相关参数之后，就可以进行如下的运算： \\[ x * \\omega + b = \\hat{y} \\tag{1} \\] 即： \\[ \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} * \\begin{bmatrix} \\omega_1 \\\\ \\omega_2 \\\\ \\omega_3 \\end{bmatrix} + b = \\hat{y} \\tag{2} \\] 这里取损失cost的计算方式为差值的平方，即 \\(C = cost(\\hat{y}, y) = (\\hat{y} - y)^2\\)，很显然，我们对 \\(\\hat{y}\\) 计算偏导数（导数）可得到： \\(\\frac{\\partial C}{\\partial \\hat{y}} = 2 (\\hat{y} - y)\\)。 将上面的(2)式展开，可以得到下面的多项式： \\[ \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3 + b = \\hat{y} \\tag{3} \\] 因为我们关心的是 \\(\\omega_1\\)，\\(\\omega_2\\)，\\(\\omega_3\\) 以及 \\(b\\) 的更新梯度，因此我们需要对这四个变量求偏导数。根据 \\(\\hat{y}\\) 的计算公式，我们不难看出这四个变量的偏导数计算公式如下： \\[ \\frac{\\partial \\hat{y}}{\\partial \\omega_1} = x_1，\\frac{\\partial \\hat{y}}{\\partial \\omega_2} = x_2，\\frac{\\partial \\hat{y}}{\\partial \\omega_3} = x_3，\\frac{\\partial \\hat{y}}{\\partial b} = 1 \\tag{4} \\] 本质上，我们需要将\\(C\\)的数值降低到全局最小值（或者局部最小值），因此我们需要根据 \\(\\frac{\\partial C}{\\partial \\omega_1}\\)，\\(\\frac{\\partial C}{\\partial \\omega_2}\\)，\\(\\frac{\\partial C}{\\partial \\omega_3}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) 这四个参数的梯度信息来更新相关参数的数值。根据求导公式的链式法则（chain rule），我们有： \\[ \\frac{\\partial C}{\\partial \\omega_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_1} = 2 (\\hat{y} - y) \\cdot x_1 \\tag{5} \\] \\[ \\frac{\\partial C}{\\partial \\omega_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_2} = 2 (\\hat{y} - y) \\cdot x_2 \\tag{6} \\] \\[ \\frac{\\partial C}{\\partial \\omega_3} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_3} = 2 (\\hat{y} - y) \\cdot x_3 \\tag{7} \\] \\[ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b} = 2 (\\hat{y} - y) \\cdot 1 \\tag{8} \\] 上式(5)~(8)就是我们计算得到的梯度信息，根据梯度信息，我们就可以更新相关的参数了，以下公式中的 \\(\\alpha\\) 表示的是学习率，为人为设置的一个超参数。 \\[ \\omega_1 := \\omega_1 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_1} = \\omega_1 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_1) \\tag{9} \\] \\[ \\omega_2 := \\omega_2 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_2} = \\omega_2 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_2) \\tag{10} \\] \\[ \\omega_3 := \\omega_3 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_3} = \\omega_3 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_3) \\tag{11} \\] \\[ b := b - \\alpha \\cdot \\frac{\\partial C}{\\partial b} = b - \\alpha \\cdot (2 (\\hat{y} - y)) \\tag{12} \\] 以上的公式就已经可以用来进行反向传播，或者说梯度下降了，但是实际上，在代码编写的时候，直接使用上面的公式会显得非常繁琐，因此，我们常常使用上面公式的向量化表达，这样可以使代码编写简洁高效，并且由于numpy等python包对向量和矩阵运算进行了很大程度的优化，因此运算速度也比直接使用上述公式要快。 我们将每个变量的梯度按照次序排好，放入一个矩阵中，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_1} \\\\ \\frac{\\partial C}{\\partial \\omega_2} \\\\ \\frac{\\partial C}{\\partial \\omega_3} \\end{bmatrix} = \\begin{bmatrix} 2(\\hat{y} - y) \\cdot x_1 \\\\ 2(\\hat{y} - y) \\cdot x_2 \\\\ 2(\\hat{y} - y) \\cdot x_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} = x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] 化简之后，\\(\\omega\\) 权值梯度更新的公式如下： \\[ \\frac{\\partial C}{\\partial \\omega} = x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] \\[ \\omega := \\omega - \\alpha * x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] 四、代码 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npparam = {}nodes = {}learning_rate = 0.001def forward(x): nodes[\"matmul\"] = np.matmul(x, param[\"w\"]) nodes['add'] = nodes['matmul'] + param[\"b\"] return nodes['add']def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def backward(x, y_pred, y): param['w'] -= learning_rate * np.matmul(np.transpose(x), cost_gradient(y_pred, y)) param['b'] -= learning_rate * cost_gradient(y_pred, y)def setup(): param[\"w\"] = np.random.random([3, 1]) param[\"b\"] = np.random.random([1, 1]) x = np.array([[1., 2., 3.]]) y = np.array([[1]]) for i in range(200): y_pred = forward(x) backward(x, y_pred, y) print(\"预测结果：\", y_pred, \" 梯度下降之后：\", forward(x), \" 真实回归值：\", y, \" Loss：\", cost(y_pred, y)) passif __name__ == '__main__': setup() 结果如下： 12345678910111213141516171819202122232425262728293031323334353637预测结果： [[3.35507743]] 梯度下降之后： [[3.28442511]] 真实回归值： [[1]] Loss： 5.546389698793334预测结果： [[3.28442511]] 梯度下降之后： [[3.21589235]] 真实回归值： [[1]] Loss： 5.218598067594647预测结果： [[3.21589235]] 梯度下降之后： [[3.14941558]] 真实回归值： [[1]] Loss： 4.910178921799803预测结果： [[3.14941558]] 梯度下降之后： [[3.08493312]] 真实回归值： [[1]] Loss： 4.619987347521437预测结果： [[3.08493312]] 梯度下降之后： [[3.02238512]] 真实回归值： [[1]] Loss： 4.346946095282921预测结果： [[3.02238512]] 梯度下降之后： [[2.96171357]] 真实回归值： [[1]] Loss： 4.090041581051698预测结果： [[2.96171357]] 梯度下降之后： [[2.90286216]] 真实回归值： [[1]] Loss： 3.8483201236115456......预测结果： [[2.16884003]] 梯度下降之后： [[2.13377483]] 真实回归值： [[1]] Loss： 1.366187026290616预测结果： [[2.13377483]] 梯度下降之后： [[2.09976159]] 真实回归值： [[1]] Loss： 1.2854453730368411预测结果： [[2.09976159]] 梯度下降之后： [[2.06676874]] 真实回归值： [[1]] Loss： 1.2094755514903637预测结果： [[2.06676874]] 梯度下降之后： [[2.03476568]] 真实回归值： [[1]] Loss： 1.137995546397283预测结果： [[2.03476568]] 梯度下降之后： [[2.00372271]] 真实回归值： [[1]] Loss： 1.0707400096052042预测结果： [[2.00372271]] 梯度下降之后： [[1.97361103]] 真实回归值： [[1]] Loss： 1.0074592750375366预测结果： [[1.97361103]] 梯度下降之后： [[1.9444027]] 真实回归值： [[1]] Loss： 0.9479184318828179预测结果： [[1.9444027]] 梯度下降之后： [[1.91607062]] 真实回归值： [[1]] Loss： 0.8918964525585433预测结果： [[1.91607062]] 梯度下降之后： [[1.8885885]] 真实回归值： [[1]] Loss： 0.8391853722123339......预测结果： [[1.6356086]] 梯度下降之后： [[1.61654034]] 真实回归值： [[1]] Loss： 0.40399829055941733预测结果： [[1.61654034]] 梯度下降之后： [[1.59804413]] 真实回归值： [[1]] Loss： 0.38012199158735577预测结果： [[1.59804413]] 梯度下降之后： [[1.58010281]] 真实回归值： [[1]] Loss： 0.3576567818845428预测结果： [[1.58010281]] 梯度下降之后： [[1.56269972]] 真实回归值： [[1]] Loss： 0.3365192660751663预测结果： [[1.56269972]] 梯度下降之后： [[1.54581873]] 真实回归值： [[1]] Loss： 0.316630977450124预测结果： [[1.54581873]] 梯度下降之后： [[1.52944417]] 真实回归值： [[1]] Loss： 0.29791808668282155预测结果： [[1.52944417]] 梯度下降之后： [[1.51356084]] 真实回归值： [[1]] Loss： 0.2803111277598668预测结果： [[1.51356084]] 梯度下降之后： [[1.49815402]] 真实回归值： [[1]] Loss： 0.2637447401092591......预测结果： [[1.00722162]] 梯度下降之后： [[1.00700497]] 真实回归值： [[1]] Loss： 5.215181191515595e-05预测结果： [[1.00700497]] 梯度下降之后： [[1.00679482]] 真实回归值： [[1]] Loss： 4.906963983096906e-05预测结果： [[1.00679482]] 梯度下降之后： [[1.00659098]] 真实回归值： [[1]] Loss： 4.616962411696138e-05预测结果： [[1.00659098]] 梯度下降之后： [[1.00639325]] 真实回归值： [[1]] Loss： 4.344099933164832e-05预测结果： [[1.00639325]] 梯度下降之后： [[1.00620145]] 真实回归值： [[1]] Loss： 4.087363627114841e-05预测结果： [[1.00620145]] 梯度下降之后： [[1.00601541]] 真实回归值： [[1]] Loss： 3.84580043675206e-05预测结果： [[1.00601541]] 梯度下降之后： [[1.00583495]] 真实回归值： [[1]] Loss： 3.618513630940355e-05预测结果： [[1.00583495]] 梯度下降之后： [[1.0056599]] 真实回归值： [[1]] Loss： 3.4046594753515e-05预测结果： [[1.0056599]] 梯度下降之后： [[1.0054901]] 真实回归值： [[1]] Loss： 3.203444100358307e-05预测结果： [[1.0054901]] 梯度下降之后： [[1.0053254]] 真实回归值： [[1]] Loss： 3.0141205540271894e-05 可以发现，经过梯度下降之后，预测的回归值逐渐接近真实的回归值，loss也一直在不断降低，证明我们的算法是正确的。","link":"/2019/05/12/Note6-BackProp-1/"},{"title":"步长stride为s的二维卷积方法的反向传播算法","text":"前言 在之前讨论了步长stride为1的卷积方式的反向传播，但是很多时候，使用的卷积步长会大于1，这个情况下的卷积方式的反向传播和步长为1的情况稍稍有些区别，不过区别并没有想象中那么大，因此下面就对步长stride大于1的情况进行简单的阐述。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，不涉及十分复杂的公式推导，争取可以以一种简单的方式来理解卷积的反向传播。希望可以很好的帮助理解反向传播算法。 需要注意的是，在本文中，所有的正向传播过程中，卷积的步长stride均固定为2。 一，参数设置 这里我们设置我们的数据矩阵（记作\\(x\\)）大小为5x5，卷积核（记作\\(k\\)）大小为3x3，由于步长是2，因此，卷积之后获得的结果是一个2x2大小的数据矩阵（不妨我们记作\\(u\\)）。偏置项我们记为\\(b\\)，将和卷积之后的矩阵进行相加。 我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为5x5 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 设置为2 padding VALID 偏置项\\(b\\) 一个浮点数 和前面一样，我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为（需要注意的是这里步长选取为2）： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} \\] 将矩阵\\(u\\)进一步展开，我们有： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 3}k_{1, 1} + x_{1, 4}k_{1, 2} +x_{1, 5}k_{1, 3} + \\\\ x_{2, 3}k_{2, 1} + x_{2, 4}k_{2, 2} +x_{2, 5}k_{2, 3} + \\\\ x_{3, 3}k_{3, 1} + x_{3, 4}k_{3, 2} +x_{3, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{3, 1}k_{1, 1} + x_{3, 2}k_{1, 2} +x_{3, 3}k_{1, 3} + \\\\ x_{4, 1}k_{2, 1} + x_{4, 2}k_{2, 2} +x_{4, 3}k_{2, 3} + \\\\ x_{5, 1}k_{3, 1} + x_{5, 2}k_{3, 2} +x_{5, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 3}k_{1, 1} + x_{3, 4}k_{1, 2} +x_{3, 5}k_{1, 3} + \\\\ x_{4, 3}k_{2, 1} + x_{4, 4}k_{2, 2} +x_{4, 5}k_{2, 3} + \\\\ x_{5, 3}k_{3, 1} + x_{5, 4}k_{3, 2} +x_{5, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差传递 步长为2的二维卷积已经在上面的式子中被完整的表示出来了，因此，下一步就是需要对误差进行传递，和前面步长为1的情况一样，我们可以将上面的结果保存在一张表格中，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。于是，表格如下： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{i, j}}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} k_{1, 1}\\) \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} k_{1, 2}\\) \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) \\(k_{1, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} k_{1, 3} + \\delta_{1, 2}k_{1, 1}\\) \\(\\partial x_{1, 4}\\) 0 \\(k_{1, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 4}} = \\delta_{1, 2}k_{1, 2}\\) \\(\\partial x_{1, 5}\\) 0 \\(k_{1, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 5}} = \\delta_{1, 2}k_{1, 3}\\) \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} k_{2, 1}\\) \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} k_{2, 2}\\) \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) \\(k_{2, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} k_{1, 3} + \\delta_{1, 2}k_{2, 1}\\) \\(\\partial x_{2, 4}\\) 0 \\(k_{2, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 4}} = \\delta_{1, 2}k_{2, 2}\\) \\(\\partial x_{2, 5}\\) 0 \\(k_{2, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 5}} = \\delta_{1, 2}k_{2, 3}\\) \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 \\(k_{1, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1}k_{3, 1} + \\delta_{2, 1}k_{1, 1}\\) \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) 0 \\(k_{1, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1}k_{3, 2} + \\delta_{2, 1}k_{1, 2}\\) \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) \\(k_{3, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 1}\\) \\(\\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1}k_{3, 3} + \\delta_{1, 2}k_{3, 1} + \\delta_{2, 1}k_{1, 3} + \\delta_{2, 2}k_{1, 1}\\) \\(\\partial x_{3, 4}\\) 0 \\(k_{3, 2}\\) 0 \\(k_{1, 2}\\) \\(\\frac{\\partial L}{\\partial x_{3, 4}} = \\delta_{1, 2}k_{3, 2} + \\delta_{2, 2}k_{1, 2}\\) \\(\\partial x_{3, 5}\\) 0 \\(k_{3, 3}\\) 0 \\(k_{1, 3}\\) \\(\\frac{\\partial L}{\\partial x_{3, 5}} = \\delta_{1, 2}k_{3, 3} + \\delta_{2, 2}k_{1, 3}\\) \\(\\partial x_{4, 1}\\) 0 0 \\(k_{2, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{4, 1}} = \\delta_{2, 1}k_{2, 1}\\) \\(\\partial x_{4, 2}\\) 0 0 \\(k_{2, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{4, 2}} = \\delta_{2, 1}k_{2, 2}\\) \\(\\partial x_{4, 3}\\) 0 0 \\(k_{2, 3}\\) \\(k_{2, 1}\\) \\(\\frac{\\partial L}{\\partial x_{4, 3}} = \\delta_{2, 1}k_{2, 3} + \\delta_{2, 2}k_{2, 1}\\) \\(\\partial x_{4, 4}\\) 0 0 0 \\(k_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{4, 4}} = \\delta_{2, 2}k_{2, 2}\\) \\(\\partial x_{4, 5}\\) 0 0 0 \\(k_{2, 3}\\) \\(\\frac{\\partial L}{\\partial x_{4, 5}} = \\delta_{2, 2}k_{2, 3}\\) \\(\\partial x_{5, 1}\\) 0 0 \\(k_{3, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{5, 1}} = \\delta_{2, 1}k_{3, 1}\\) \\(\\partial x_{5, 2}\\) 0 0 \\(k_{3, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{5, 2}} = \\delta_{2, 1}k_{3, 2}\\) \\(\\partial x_{5, 3}\\) 0 0 \\(k_{3, 3}\\) \\(k_{3, 1}\\) \\(\\frac{\\partial L}{\\partial x_{5, 3}} = \\delta_{2, 1}k_{3, 3} + \\delta_{2, 2}k_{3, 1}\\) \\(\\partial x_{5, 4}\\) 0 0 0 \\(k_{3, 2}\\) \\(\\frac{\\partial L}{\\partial x_{5, 4}} = \\delta_{2, 2}k_{3, 2}\\) \\(\\partial x_{5, 5}\\) 0 0 0 \\(k_{3, 3}\\) \\(\\frac{\\partial L}{\\partial x_{5, 5}} = \\delta_{2, 2}k_{3, 3}\\) 可以看出，数据依然都是很规律的进行着重复。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。那么，根据求导的链式法则，我们有： 根据求偏导数的链式法则，我们可以有： \\[ \\frac{\\partial L}{\\partial x_{i, j}} = \\sum_{p = 1} \\sum_{k = 1} \\frac{\\partial L}{\\partial u_{p, k}} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{i, j}} = \\sum_{p = 1} \\sum_{k = 1} \\delta_{p, k} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{i, j}} \\] 我们以\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)为例，我们有： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_{3, 3}} &amp;= \\sum_{p = 1} \\sum_{k = 1} \\frac{\\partial L}{\\partial u_{p, k}} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{3, 3}} \\\\ &amp;= \\sum_{p = 1} \\sum_{k = 1} \\delta_{p, k} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{3, 3}} \\\\ &amp;= \\delta_{1, 1}\\frac{\\partial u_{1, 1}}{\\partial x_{3, 3}} + \\delta_{1, 2}\\frac{\\partial u_{1, 2}}{\\partial x_{3, 3}} + \\delta_{2, 1}\\frac{\\partial u_{2, 1}}{\\partial x_{3, 3}} + \\delta_{2, 2}\\frac{\\partial u_{2, 2}}{\\partial x_{3, 3}} \\\\ &amp;= \\delta_{1, 1}k_{3, 3} + \\delta_{1, 2}k_{3, 1} + \\delta_{2, 1}k_{1, 3} + \\delta_{2, 2}k_{1, 1} \\end{aligned} \\] 类似地，我们可以计算出所有的输入矩阵中的元素所对应的偏导数信息，所有的偏导数计算结果均在上表中列出。 和前面步长stride为1的卷积方式的误差传递类似，我们需要对传递来的误差矩阵和卷积核进行一定的处理，然后再进行卷积，得到应该传递到下一层的网络结构中，所以我们需要的解决问题的问题有三个，即：1.误差矩阵如何处理，2.卷积核如何处理，3.如何进行卷积。 同样，我们将\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)单独拿出来进行考察，如果需要用到全部的卷积核的元素的话，并不能和传递来的误差矩阵相匹配，为了使得两者可以再维度上相匹配，我们再误差矩阵中添加若干0，和步长stride为1的卷积反向传播一样，我们也将卷积核进行180°翻转，于是，我们可以得到： \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 由于padding策略一直默认为是VALID，而且上面的两个矩阵形状相同，所以此时的步长stride参数不会影响到最终的结果。 如果按照我们之前的策略，再在添加0之后的误差矩阵外面填补上合适数目的0的话，有： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 同样，上面的卷积过程步长stride参数为1。 不妨将上面的卷积的结果记为\\(conv1\\)，然后，我们将\\(\\frac{\\partial L}{\\partial x_{i, j}}\\)按照对应的顺序进行排列，我们将结果记作\\(conv2\\)，即： \\[ conv2 = \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} \\] 经过计算，我们发现\\(conv1\\)和\\(conv2\\)正好相等。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 可以发现，在前面提出的三个问题中，有两个问题的答案是和步长stride为1的二维卷积相同的，唯一不同的是，我们需要在误差矩阵的相邻元素之间插入若干0来完成卷积误差的产生。 误差矩阵插入0的方式 很明显，我们唯一需要解决的问题就是如何在误差矩阵中插入0。在这里直接给出结论，那就是每个相邻的元素之间应该插入（步长stride - 1）个0，或者说每个元素之间的距离是卷积的步长。因为在这个模型中，唯一和前面的卷积方式不同的变量就是步长stride，那么需要满足的条件也必然和步长有关。 当我们在元素之间插入合适数目的0之后，接下来就是在误差矩阵周围填补上合适数目的0层，然后将卷积核旋转180°，最后按照步长为1的方式进行卷积，最后得到应该向前传递的误差矩阵。 这两步和步长stride为1的反向传播算法相同。 三、参数更新 当我们解决了误差的向前传递之后，下一步就是解决参数的更新的问题。和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， 即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以有下面的式子：这里以求解\\(\\frac{\\partial L}{\\partial k_{1, 1}}\\) 为例： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial k_{1, 1}} =&amp; \\frac{\\partial L}{\\partial u_{1, 1}} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 2}} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 1}} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 2}} \\frac{\\partial u_{2, 2}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\delta_{1, 2} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\delta_{2, 1} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\delta_{2, 2} \\frac{\\partial u_{2, 2}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 3} \\end{aligned} \\] 类似地，我们将所有地偏导数信息都求出来，汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = \\delta_{1, 1} x_{1, 2} + \\delta_{1, 2} x_{1, 4} + \\delta_{2, 1} x_{3, 2} + \\delta_{2, 2} x_{3, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = \\delta_{1, 1} x_{1, 3} + \\delta_{1, 2} x_{1, 5} + \\delta_{2, 1} x_{3, 3} + \\delta_{2, 2} x_{3, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = \\delta_{1, 1} x_{2, 1} + \\delta_{1, 2} x_{2, 3} + \\delta_{2, 1} x_{4, 1} + \\delta_{2, 2} x_{4, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = \\delta_{1, 1} x_{2, 2} + \\delta_{1, 2} x_{2, 4} + \\delta_{2, 1} x_{4, 2} + \\delta_{2, 2} x_{4, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = \\delta_{1, 1} x_{2, 3} + \\delta_{1, 2} x_{2, 5} + \\delta_{2, 1} x_{4, 3} + \\delta_{2, 2} x_{4, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = \\delta_{1, 1} x_{3, 1} + \\delta_{1, 2} x_{3, 3} + \\delta_{2, 1} x_{5, 1} + \\delta_{2, 2} x_{5, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = \\delta_{1, 1} x_{3, 2} + \\delta_{1, 2} x_{3, 4} + \\delta_{2, 1} x_{5, 2} + \\delta_{2, 2} x_{5, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = \\delta_{1, 1} x_{3, 3} + \\delta_{1, 2} x_{3, 5} + \\delta_{2, 1} x_{5, 3} + \\delta_{2, 2} x_{5, 5} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1} + \\delta_{1, 2} + \\delta_{2, 1} + \\delta_{2, 2} \\] 和前面地误差传递类似，我们发现可以在误差矩阵中插入若干个0来和输入矩阵\\(x\\)来保持维度上的匹配。即有： \\[ \\frac{\\partial L}{\\partial k} = [\\frac{\\partial L}{\\partial k_{i, j}}] = \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 据此，我们可以发现，在卷积核参数更新的过程中，我们也需要对误差矩阵进行插入0的操作。而且插入0的方式和误差传递过程中的方式完全相同。所以，我们可以总结出步长为s的时候卷积反向传播的卷积核参数更新的方法，即：1.首先在接收到的误差矩阵中插入合适数目的0，2.在输入矩阵\\(x\\)上应用误差矩阵进行步长为1的卷积，从而得到卷积核的更新梯度。 同样，我们由上面的推导可以发现，无论是何种方式的卷积操作，偏置项\\(b\\)的更新梯度都是接收到的误差矩阵中的元素之和。 四、总结 我们将上面的求解过程总结如下有： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵 输入卷积核\\(k\\) 一个二维矩阵 步长\\(stride\\) 一个正整数s padding VALID 偏置项\\(b\\) 一个浮点数 正向传播： 1conv(x, kernel, bias, &quot;VALID&quot;) 反向传播： 12345678910111213141516conv_backward(error, x, kernel, bias): # 计算传递给下一层的误差 1.在接收到的error矩阵的矩阵中插入合适数目的0，使得每个元素之间的0的数目为(stride - 1) 2.在error周围填补上合适数目的0 3.将kernel旋转180° 4.将填补上0的误差和旋转之后的kernel进行步长为1的卷积，从而得到传递给下一层的误差new_error。 # 更新参数 1.在接收到的error矩阵的矩阵中插入合适数目的0，使得每个元素之间的0的数目为(stride - 1) 2.将输入矩阵x和插入0之后的误差矩阵error进行步长为1的卷积，得到kernel的更新梯度 3.将上一层传递来的误差矩阵error所有元素求和，得到bias的更新梯度 4.kernel := kernel - 学习率 * kernel的更新梯度 5.bias := bias - 学习率 * bias的更新梯度 # 返回误差，用以传递到下一层 return new_error","link":"/2019/05/24/Note15-ConvBackProp-part2/"},{"title":"多通道卷积以及激活函数","text":"前言 前面讲了很多二维平面上的卷积，甚至用代码实现了一个简单的两层二维卷积网络，但是在实际的情况下，我们使用的更多的是三维矩阵，即矩阵的\\(shape\\)往往是\\([height, width, channels]\\)。在这种情况下，我们的卷积核就会多出一个参数来和通道\\(channels\\)参数进行匹配，即，这个时候，我们的卷积核的\\(shape\\)会变成\\([kernel\\_height, kernel\\_width, channels]\\)。所以接下来就是要弄清楚在这种多通道的情况下，卷积是如何进行反向传播的。 一、带通道的卷积 由于带有多个通道属性，因此，我们可以将每个通道的数据都视为一个二维的矩阵，卷积核也按照通道的数目拆分成多个二维的卷积核数据，因此，当我们分别对这些二维矩阵进行卷积之后，在将所有的结果相加即可得到最后的结果。这就是带有通道数据的卷积方式的本质。反过来，我们也利用这种本质，来进行反向传播。 在本文的模型中，我们假定平面上的卷积为\\(plane\\_conv(x, kernel, stride)\\)，再定义带有数据通道的卷积函数为\\(conv(x, kernel, stride)\\)，其中，\\(x\\)和\\(kernel\\)均为三维矩阵，格式分别为：\\(x.shape: [height, width, channels]\\)，\\(kernel.shape: [kernel\\_height, kernel\\_width, channels]\\)，则我们有： \\[ conv(x, kernel, stride) = \\sum_{i = 0}^{channels} plane\\_conv(x_{i}, kernel_{i}, stride) \\] 上面的公式的含义就是多通道卷积产生的结果是将各个通道进行分开，然后在每个通道上分别进行二维平面上的卷积，最后将这些二维的卷积结果相加得到的，而事实上，这本身就是多通道卷积的本质含义。 所以当我们对其中的每一通道的\\(kernel_{i}\\)和\\(x_i\\)求导时，我们有： \\[ \\frac{\\partial conv(x, kernel, stride)}{\\partial x_i} = \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial x_i} \\] \\[ \\frac{\\partial conv(x, kernel, stride)}{\\partial kernel_i} = \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial kernel_i} \\] 现在，我们假设由上层传来的误差为\\(\\delta = \\frac{\\partial L}{\\partial conv(x, kernel, stride)}\\)，那么我们在上式的两边同时乘以误差\\(\\delta\\)，根据求导的链式法则，我们有： \\[ \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial conv(x, kernel, stride)}{\\partial x_i} = \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial x_i} \\] \\[ \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial conv(x, kernel, stride)}{\\partial kernel_i} = \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial kernel_i} \\] 化简之后，我们有： \\[ \\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial x_i} \\] \\[ \\frac{\\partial L}{\\partial kernel_i} = \\frac{\\partial L}{\\partial conv(x, kernel, stride)} \\cdot \\frac{\\partial plane\\_conv(x_{i}, kernel_{i}, stride)}{\\partial kernel_i} \\] 上述的两个等式的左边就是我们需要计算的每一个通道的偏导数，即向前传递的误差的偏导数和需要用来更新参数的偏导数，等式右边则变成了接收到的误差矩阵和每个通道上的二维卷积求导数，因此等式右边就变成了我们已经知道的在二维平面上进行反向传播的偏导数求解了。所以我们将问题由三维空间分解到了二维空间，而二维空间上的卷积及其反向传播我们在之前已经花了较多篇幅进行讲解，所以这里不再赘述了。 需要注意的是，上面的推导过程都仅限于一个卷积核，若有多个卷积核则需要在每一个卷积核上利用这个方法进行偏导数的求解。所以，对于多样本的卷积而言，无论是正向传播还是反向传播，时间复杂度都是很高的。 因此，关于多核卷积的反向传播，我们可以总结如下： 123456789101112conv_backward(error, x, kernel, bias): 1. 取出每一对输入样本的特征图和误差 2. 对取出的每一对，我们进行如下操作： 1. 对误差的每一个通道： 1. 将通道数据进行扩展，主要是在通道维度上进行扩展，使其在通道数目上和输入矩阵一致。 2. 根据步长stride参数在误差矩阵的每一个通道数据间插入合适数目的0。 3. 根据卷积核的尺寸在插入0的误差矩阵外围填补上合适层数的0。 4. 将这一层的卷积核取出，并旋转180°。 5. 将误差矩阵和卷积核进行步长stride为1的卷积，并保存得到的矩阵，此矩阵即为需要传递的误差矩阵。 6. 将数据矩阵和插入0之后的误差矩阵进行步长stride为1的卷积即可得到卷积核的更新梯度矩阵。 2. 计算偏置项bias的更新梯度。 3. 对卷积核和偏置项进行更新。 我们使用代码对上述的算法进行一定的验证。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243import numpy as npimport matplotlib.pyplot as pltlearning_rate = 0.0003def plane_convolution(x, kernel, stride): \"\"\" 二维平面上的卷积，padding为VALID :param x: 被卷积的特征矩阵，是一个二维矩阵 :param kernel: 卷积核参数，为一个二维矩阵 :param stride: 步长信息，一个正整数 :return: 卷积之后的矩阵信息 \"\"\" assert len(x.shape) == 2 assert len(kernel.shape) == 2 assert type(stride) is int assert (x.shape[0] - kernel.shape[0]) % stride == 0 and (x.shape[1] - kernel.shape[1]) % stride == 0 result = np.zeros([(x.shape[0] - kernel.shape[0]) // stride + 1, (x.shape[1] - kernel.shape[1]) // stride + 1]) for i in range(0, x.shape[0] - kernel.shape[0] + 1, stride): for j in range(0, x.shape[1] - kernel.shape[1] + 1, stride): sum = 0 for p in range(kernel.shape[0]): for k in range(kernel.shape[1]): sum += x[i + p][j + k] * kernel[p][k] result[i // stride][j // stride] = sum return resultdef convolution_one_kernel(x, kernel, stride): \"\"\" 二维平面上的卷积，padding为VALID :param x: 被卷积的特征矩阵，是一个三维矩阵 :param kernel: 卷积核参数，为一个三维矩阵 :param stride: 步长信息，一个正整数 :return: 卷积之后的矩阵信息，是一个二维矩阵 \"\"\" assert len(x.shape) == 3 assert len(kernel.shape) == 3 assert type(stride) is int assert (x.shape[0] - kernel.shape[0]) % stride == 0 and (x.shape[1] - kernel.shape[1]) % stride == 0 assert x.shape[-1] == kernel.shape[-1] result = np.zeros(shape=[(x.shape[0] - kernel.shape[0]) // stride + 1, (x.shape[1] - kernel.shape[1]) // stride + 1, x.shape[2]]) for i in range(x.shape[-1]): result[:, :, i] = plane_convolution(x[:, :, i], kernel[:, :, i], stride) return resultdef convolution_multi_kernel(x, kernel, stride, bias): \"\"\" :param x: 三维矩阵 :param kernel: 四维矩阵，[height, width, in_channel, out_channel] :param stride: 一个正整数 :return: \"\"\" # # TODO: many assertion here matrix = np.zeros([(x.shape[0] - kernel.shape[0]) // stride + 1, (x.shape[1] - kernel.shape[1]) // stride + 1, kernel.shape[-1]]) depth = kernel.shape[-1] for i in range(depth): matrix[:, :, i] = np.sum(convolution_one_kernel(x, kernel[:, :, :, i], stride), axis=-1) + bias[i] return matrix# 对矩阵的上下左右分别进行填补0的操作。def padding_zeros(x, left_right, top_bottom): \"\"\" 对矩阵的外围进行填补0的操作。 :param x: 一个二维矩阵 :param left_right: 一个长度为2的数组，分别表示左侧和右侧需要填补的0的层数 :param top_bottom: 一个长度为2的数组，分别表示上侧和下侧需要填补的0的层数 :return: 填补之后的矩阵 \"\"\" assert len(x.shape) == 3 assert len(left_right) == 2 and len(top_bottom) == 2 new_x = np.zeros([top_bottom[0] + top_bottom[1] + x.shape[0], left_right[0] + left_right[1] + x.shape[1], x.shape[2]]) new_x[top_bottom[0]: top_bottom[0] + x.shape[0], left_right[0]: left_right[0] + x.shape[1], ::] = x return new_xdef insert_zeros(x, stride): \"\"\" 在矩阵的每两个相邻元素之间插入一定数目的0 :param x: 一个二维矩阵 :param stride: 一个非负数 :return: 插入0之后的矩阵 \"\"\" assert len(x.shape) == 3 assert type(stride) is int and stride &gt;= 0 new_x = np.zeros([(x.shape[0] - 1) * stride + x.shape[0], (x.shape[1] - 1) * stride + x.shape[1], x.shape[2]]) for i in range(x.shape[0]): for j in range(x.shape[1]): new_x[i * (stride + 1), j * (stride + 1), ::] = x[i][j] return new_xdef rotate_180_degree(x): \"\"\" 将矩阵旋转180°，这一步主要是针对卷积核而言。 :param x: 需要被旋转的矩阵 :return: 旋转之后的矩阵 \"\"\" assert len(x.shape) == 3 return np.rot90(np.rot90(x))class conv(object): def __init__(self, kernel, stride, bias): if type(kernel) is np.ndarray and len(kernel.shape) == 4: self.kernel = kernel elif type(kernel) is list and len(kernel) == 4: self.kernel = np.random.normal(loc=0.0, scale=0.01, size=kernel) else: raise ValueError(\"Kernel input is wrong!\") if type(bias) in (float, int): self.bias = np.ones(shape=[self.kernel.shape[-1]], dtype=np.float64) * bias elif type(bias) in (np.ndarray, list) and len(bias) == self.kernel.shape[-1]: self.bias = np.array(bias, dtype=np.float64) shape = self.kernel.shape assert shape[0] == shape[1], \\ \"For simplicity and in almost situations, Kernel's height should equal to its width...\" assert type(stride) is int, \"parameter 'stride' should be a integer...\" self.stride = stride self.x = None def forward(self, x): x = np.array(x, dtype=np.float64) assert len(x.shape) == 4, \"input 'X' must 4-dimension matrix...\" self.x = x result = [] for x_ in x: result.append(convolution_multi_kernel(x_, self.kernel, self.stride, self.bias)) return np.array(result, dtype=np.float64) def backward(self, error): sample_num = error.shape[0] error_new = np.zeros_like(self.x) kernel_grad = np.zeros_like(self.kernel) bias_grad = np.zeros_like(self.bias) for n in range(sample_num): error_n = error[n] sample_n = self.x[n] for c in range(error_n.shape[-1]): error_ = error_n[:, :, c: c + 1] error_ = np.tile(error_, (1, 1, sample_n.shape[-1])) # insert zeros error_inserted = insert_zeros(error_, self.stride - 1) # padding zeros error_ = padding_zeros(error_inserted, [self.kernel.shape[0] - 1, self.kernel.shape[0] - 1], [self.kernel.shape[1] - 1, self.kernel.shape[1] - 1]) # rotate kernels kernel_rotated = rotate_180_degree(self.kernel[:, :, :, c]) # add this sample's error error_new[n] += convolution_one_kernel(error_, kernel_rotated, 1) # save the gradient of this channel kernel_grad[:, :, :, c] += convolution_one_kernel(sample_n, error_inserted, 1) bias_grad += np.sum(error_n, axis=(0, 1)) self.kernel -= learning_rate * kernel_grad / sample_num self.bias -= learning_rate * bias_grad /sample_num return np.array(error_new, dtype=np.float64)if __name__ == '__main__': import tensorflow as tf tf.enable_eager_execution() map = np.random.normal(size=[3, 10, 10, 3]) kernel1 = np.random.normal(size=[3, 3, 3, 5]) kernel2 = np.random.normal(size=[4, 4, 5, 6]) conv1 = conv(kernel1, 1, 0.5) feature1 = conv1.forward(map) conv2 = conv(kernel2, 2, 1.3) target = conv2.forward(feature1) kernel1 = np.random.normal(size=[3, 3, 3, 5]) kernel2 = np.random.normal(size=[4, 4, 5, 6]) conv1 = conv(kernel1, 1, 0) f1 = conv1.forward(map) conv2 = conv(kernel2, 2, 0) f2 = conv2.forward(f1) loss_collection = [] for loop in range(300): loss = np.mean(np.square(f2 - target)) print(\"Loop \", loop, \": \", loss) loss_collection.append(loss) error_ = 1 / (f2.shape[0] * f2.shape[1] * f2.shape[2]) * (f2 - target) error_ = conv2.backward(error_) error_ = conv1.backward(error_) f1 = conv1.forward(map) f2 = conv2.forward(f1) plt.plot(np.arange(len(loss_collection)), loss_collection) plt.show() # 程序的运行结果如下： 12345678910111213141516171819202122232425262728Loop 0 : 4566.690565697412Loop 1 : 4271.494073404713Loop 2 : 4007.910539798463Loop 3 : 3771.192713523367Loop 4 : 3557.4953404882185Loop 5 : 3363.6713649500307Loop 6 : 3187.120855206984Loop 7 : 3025.6774125025804Loop 8 : 2877.521659695657Loop 9 : 2741.1145805573606Loop 10 : 2615.1456043041508Loop 11 : 2498.491776241481Loop 12 : 2390.185356338042Loop 13 : 2289.387890624407......Loop 287 : 50.79360170770062Loop 288 : 50.409882234698294Loop 289 : 50.02968824460652Loop 290 : 49.65298004060671Loop 291 : 49.27971847348985Loop 292 : 48.90986493247369Loop 293 : 48.54338133620478Loop 294 : 48.18023012394164Loop 295 : 47.820374246914355Loop 296 : 47.46377715985639Loop 297 : 47.11040281270491Loop 298 : 46.76021564246494Loop 299 : 46.41318056523445 需要注意的是，上面的代码并不是十分高效的反向传播，相反，在执行效率上，上面的代码显得很低下，但是我们的目的是了解反向传播的过程，因此，这里并没有进行任何优化，以一种尽管低效但却十分直观的方法来演示反向传播。 可以发现，在卷积的反向传播过程中，二维卷积的反向传播是基础，理解了在二维平面上的卷积，我们就可以很容易地推广到带有通道的卷积上以及多样本的卷积上。 二、激活函数 其实，本质上，激活函数\\(Activation Function\\)可以看作是一种不带有任何参数的层，它的目的就是让输入的数据产生一些非线性的变化。 我们假设经过某一个操作\\(Op\\)之后，我们需要对输入进行激活（我们记激活函数为\\(g(x)\\)，则，根据上面的描述，我们可以很容易得到： \\[ y = g(Op(x)) \\] 假设，我们根据反向传播算法获得的关于激活函数的输出的梯度为：\\(\\frac{\\partial L}{\\partial y} = \\delta\\)，则，我们对上面的式子求导数，我们就可以得到： \\[ \\frac{\\partial y}{\\partial Op(x)} = g'(Op(x)) \\] 根据求导的链式法则，我们在上式的两边同时乘以\\(\\frac{\\partial L}{\\partial y} = \\delta\\)，则有： \\[ \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial Op(x)} = \\frac{\\partial L}{\\partial Op(x)} = \\delta \\cdot g'(Op(x)) \\] 所以，我们发现，对于激活函数这一个比较特殊的层来说，它的反向传播就是将接受来的误差乘以它自身在输入矩阵上的导数，从而得到需要传递给下一层的误差，而激活函数层因为往往不带有可训练参数，因此也就不存在参数更新的问题。 三、总结 花了这么多篇幅，总算是弄清楚了最最常见的卷积的反向传播的方式，虽然时间花费比较多，但是这些都是值得的，现在已经可以进行自己的深度学习框架的编写了。在这些基础上，可以在花些时间学习GPU编程，将自己的深度学习框架迁移到GPU上运行，从而获得执行效率的大幅提升。","link":"/2019/05/27/Note18-ConvBackProp-part5/"},{"title":"奇异值分解SVD","text":"前言 奇异值分解(Singular Value Decomposition，以下简称SVD)是在一种十分经典的无监督的机器学习算法，它可以用于处理降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。 特征值和特征向量 再了解SVD之前，对于矩阵的特征值和特征向量还是有必要做一个简单的回顾。 假设矩阵\\(A\\)是一个实对称矩阵，大小为\\(n \\times n\\)，如果有一个实数\\(\\lambda\\)和一个长度为\\(n\\)的向量\\(\\alpha\\)，满足下面的等式关系： \\[ A \\alpha = \\lambda \\alpha \\tag{1} \\] 我们就将\\(\\lambda\\)称为矩阵\\(A\\)的特征值，将\\(\\alpha\\)称为矩阵\\(A\\)的特征向量，是一个列向量。 由线性代数的相关知识我们可以知道，对于一个\\(n\\)阶的实对称矩阵，一定存在\\(n\\)个相互正交的特征向量。对于每一个特征向量，我们可以将其规范化，使其模长\\(|\\alpha|\\)为1。于是，我们有： \\[ \\alpha_i^T \\alpha_i = 1 \\\\ \\alpha_i^T \\alpha_j = 0, \\quad i \\ne j \\] 我们将所有的特征值按照从大到小的顺序进行排列，并将对应的特征的特征向量也按照特征值的大小进行排列，于是，我们会有以下的个排列： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\\\ \\alpha_1, \\alpha_2, \\cdots, \\alpha_n \\] 上面的所有的特征值可以依次被放入一个$ n n\\(的对角矩阵的对角线元素中，我们不妨称这个对角矩阵为\\)$，于是我们有： \\[ \\Sigma = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ddots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ddots &amp; \\lambda_n \\\\ \\end{bmatrix} \\] 同样，我们也将上述的特征向量按照特征值的顺序进行排列，也组成了一个\\(n \\times n\\)的矩阵，不妨记作\\(W\\)，于是有： \\[ W = [\\alpha_1, \\alpha_2, \\cdots, \\alpha_n] \\] 综上，我们可以得到如下的关系式： \\[ A W = W \\Sigma \\tag{2} \\] 在等式的两边同时乘以\\(W^{-1}\\)，我们可以得到： \\[ A = W \\Sigma W^{-1} \\tag{3} \\] 考虑到特征向量彼此两两正交，模长为1，我们就有： \\[ W W^T = E \\tag{4} \\] 即： \\[ W^{-1} = W^T \\tag{5} \\] 将式(5)代入式(3)中，我们可以得到： \\[ A = W \\Sigma W^{-1} = W \\Sigma W^T \\tag{6} \\] 这是一个非常重要的关系，在前面我们就是利用这个关系来计算主成分分析PCA的。 奇异值分解SVD 在前面已经找到了一个重要的关系，在我们之前的讨论过程中，我们已经知道满足这个条件的矩阵的要求是该矩阵是一个实对称矩阵，如果只是一个普通的矩阵\\(A_{m \\times n}\\)，我们其实也可以通过类似的方法进行矩阵分解，这时候我们需要构造的矩阵等式如下： \\[ A = U \\Sigma V^T \\tag{7} \\] 其中，\\(U\\)是一个大小为$m m \\(的矩阵，\\)\\(是一个大小为\\)m n\\(的矩阵，\\)V^T\\(是一个大小为\\)n n$的矩阵。 矩阵\\(U\\) 的求解 定义了相关的矩阵等式关系之后，我们既需要求解其中的每一个矩阵的具体数值。首先来求解矩阵\\(U\\)的数值。 首先构造矩阵\\(AA^T\\)，该矩阵是一个大小为\\(m \\times m\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_m \\\\ u_1, u_2, \\cdots, u_m \\] 其中每一对特征值和特征向量都满足： \\[ (AA^T) u_i = \\lambda_i u_i \\tag{7} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(U\\)， 即： \\[ U = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\] 矩阵\\(V\\) 的求解 和求解矩阵\\(U\\)类似，首先构造矩阵\\(A^T A\\)，该矩阵是一个大小为\\(n \\times n\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\sigma_1, \\sigma_2, \\cdots, \\sigma_n \\\\ v_1, v_2, \\cdots, v_n \\] 其中每一对特征值和特征向量都满足： \\[ (A^TA) v_i = \\sigma_i v_i \\tag{8} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(V\\)， 即： \\[ V = \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} \\] 矩阵\\(\\Sigma\\) 的求解 当求出矩阵\\(U\\)和矩阵\\(V\\)时，我们就可以求出矩阵\\(\\Sigma\\)了： \\[ U \\Sigma V^T = A\\\\ \\Sigma = U^{-1} A (V^T)^{-1} \\] 考虑到矩阵\\(U\\)和矩阵\\(V\\)的特殊性质，即\\(U^{-1} = U^T\\)，\\(V^{-1} = V^T\\)，代入上面的式子中，我们有： \\[ \\Sigma = U^T A V \\tag{9} \\] 原理 其实，以上的步骤并不是随便得来的，是有严格证明的，下面以证明矩阵\\(V\\)为例进行说明。 首先，我们等式\\(A = U \\Sigma V^T\\)左右两边分别求矩阵转置，得到： \\[ A^T = V \\Sigma^T U^T \\] 接着有： \\[ \\begin{aligned} A^T A &amp;= (V \\Sigma^T U^T)(U \\Sigma V^T) \\\\ &amp;= V \\Sigma^T U^T U \\Sigma V^T \\\\ &amp;= V \\Sigma^T \\Sigma V^T \\\\ &amp;= V (U^T A V)^T (U^T A V) V^T \\\\ &amp;= V (V^T A^T U)(U^T A V) V^T \\\\ &amp;= V V^T A^T U U^T A V V^T \\\\ &amp;= A^T A \\end{aligned} \\] 注意到上面证明过程的第三行，我们有$A^T A = V ^T V^T \\(，而矩阵\\)V\\(是矩阵\\)A^T A\\(的特征向量组成的矩阵，因此，矩阵\\)^T \\(就是矩阵\\)A^T A\\(的特征值组成的对角矩阵，这又给了我们一个新的求解\\)$矩阵的方法。 当我们求出所有的 \\(A^T A\\) 矩阵的特征值之后，我们在这些特征值的基础上进行开方操作，并将结果组成一个对角矩阵，这个对角矩阵就是我们需要的 \\(\\Sigma\\) 矩阵。 所以我们发现，实际上\\(\\Sigma\\)是一个对角矩阵，只在对角线上存在着有效元素，其他部位的元素都为0。我们将矩阵\\(\\Sigma\\)的对角线上的元素称为奇异值。 同样的道理，如果我们计算\\(A A^T\\)，我们也能得到相同的结果，而计算过程就是证明矩阵\\(U\\)的正确性的过程。 接下来，我们考虑这样一件事，我们已经求解出了所有的矩阵信息，有\\(A = U \\Sigma V^T\\)，我们同时在等式的左右两边乘以\\(V\\)，有： \\[ AV = U \\Sigma V^T V = U \\Sigma \\] 即： \\[ A \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\begin{bmatrix} a_1 &amp; \\; &amp; \\; \\\\ \\; &amp; a_2 \\; &amp; \\\\ \\; &amp; \\; &amp; \\ddots \\end{bmatrix}_{m \\times n} \\] 其中，\\(a_i\\) 表示的是 \\(\\Sigma\\) 矩阵的对角线元素，所以有： \\[ A v_i = u_i a_i \\] 故： \\[ a_i = \\frac{A v_i}{u_i} \\tag{9} \\] 上述也是一种求解\\(\\Sigma\\)矩阵的方法。 SVD的使用 接下来，我们通过一个简单的例子来实际使用以下SVD。假设我们的数据矩阵\\(A\\)如下： \\[ A = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] 接下来我们计算\\(A^T A\\)和\\(A A^T\\)，有： \\[ AA^T = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\\\ A^TA = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 2 \\\\2 &amp; 5 \\end{bmatrix} \\] 对于矩阵\\(AA^T\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\quad \\lambda_1 = 6, u_1 = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{30}} \\\\ \\frac{2}{\\sqrt{30}} \\end{bmatrix} \\quad \\lambda_2 = 1, u_2 = \\begin{bmatrix} 0 \\\\ \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\quad \\lambda_3 = 0, u_3 = \\begin{bmatrix} -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{bmatrix} \\] 对于矩阵\\(A^TA\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\lambda_1 = 6, v_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}}\\end{bmatrix} \\quad \\lambda_2 = 1, v_2 = \\begin{bmatrix} \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}}\\end{bmatrix} \\] 由上面的特征值信息，我们可以求出矩阵\\(\\Sigma\\)的对角线元素，即直接在特征值的基础上取根号即可，依次为： \\[ a_1 = \\sqrt{6}, a_2 = 1 \\] 由上面的所有计算结果，我们可以得到矩阵\\(U\\)，\\(V\\)和\\(\\Sigma\\)，如下： \\[ U = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\] \\[ V = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] \\[ \\Sigma = \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\] 于是，我们的原始矩阵\\(A\\)可以分解成如下： \\[ A = U \\Sigma V^T \\] 即： \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] 在前面的这个求解的例子中，我们很难发现SVD究竟有什么用处，毕竟求解矩阵乘积，矩阵的特征值和特征向量都是十分复杂是过程。实际上我们在求解奇异值的时候，可以利用矩阵的特征值进行求解，这样奇异值就和特征值具有相同的特点，如果按照从大到小的顺序排列奇异值，我们发现奇异值的数值下降很快，在有些情况下，奇异值前10%~15%的数值之和已经占据了全部奇异值数值之和的90%以上，而过于小的奇异值我们可以将其省略，因为奇异值太小，对最后的生成矩阵影响就会非常小，可以直接忽略不计。当我们采用这种策略的时候，我们可以适当的选取数值最大的几个奇异值，并选取对应的特征向量，这个时候我们就可以只是用这些数据来重构我们的原始矩阵。 下面是一个具体的使用SVD重构数据矩阵的python代码实例，数据来源为《机器学习实战》一书。在代码中，我们试图去利用SVD重构一个01矩阵组成的数字0。 数据如下： 12345678910111213141516171819202122232425262728293031320000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)# 调整特征向量，使得所有向量的分量之和为正数。def adjust(vectors): values = np.sum(vectors, axis=0) for i, v in enumerate(values): if v &lt; 0: vectors[:, i] *= -1 return vectors# 利用numpy的求解特征值和特征向量的函数进行求解。def eig(matrix): values, vectors = np.linalg.eig(matrix) # 有时候函数会返回复数值，这个时候我们只需要取其实数部分。 values = np.real(values) vectors = np.real(vectors) # 按照特征值的大小进行排序。 index = np.argsort(values)[::-1] values = values[index] vectors = adjust(vectors[:, index]) # 由于求解过程存在一定的误差，因此特征值会出现极小的负数，我们可以直接将其置为0。 values = np.maximum(values, 0) return values, vectorsdef svd(matrix): # 返回左侧矩阵的特征值和特征向量 left_values, left_vectors = eig(np.matmul(matrix, np.transpose(matrix))) # 返回右侧矩阵的特征值和特征向量 right_values, right_vectors = eig(np.matmul(np.transpose(matrix), matrix)) # Sigma矩阵 sigma = np.zeros_like(matrix, dtype=np.float64) for i in range(min(len(left_values), len(right_values))): sigma[i][i] = math.sqrt(left_values[i]) printMat(np.matmul(np.matmul(left_vectors, sigma), np.transpose(right_vectors))) pass# 输出，这里借鉴了《机器学习实战》一书的输出模板def printMat(inMat, thresh=0.6): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(\"1\", end=\"\") else: print(\"0\", end=\"\") print()if __name__ == '__main__': matrix = load_data() svd(matrix) 结果如下： 12345678910111213141516171819202122232425262728293031320000000000000000001000000000000000000000000011011110000000000000000000000001111111100000000000000000000000111111111100000000000000000000000111111110111010000000000000000011011111111111100000000000000001111111111111101000000000000000001111000001111110000000000000000011110000011111100000000000001111110000000011110000000000000011111100000000111100000000000000111111000000000111100000000000001111110000000001111000000000000000111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000011110000000000000011111000000011110100000000000000111111000001111110000000000000001111111111111111000000000000000011111111111110000000000000000000111111111111100000000000000000000000111111111000000000000000000000001111111111000000000000000000000000111111000000000000 可以看到，我们自己实现的SVD可以较好地重构出原始数据矩阵。由于求解过程中的误差，我们的重构结果和实际的数据矩阵还是有些不同的，所以，numpy已经将SVD进行了封装，我们可以直接使用其封装好的函数进行求解SVD，这样求出的结果会比自己手动求解的结果好。 使用numpy中的SVD的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)def printMat(inMat, thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(1, end=\"\") else: print(0, end=\"\") print()def imgCompress(numSV=5, thresh=0.8): matrix = load_data() print(\"****original matrix******\") printMat(matrix, thresh) U, Sigma, VT = np.linalg.svd(matrix) SigRecon = np.zeros((numSV, numSV)) for k in range(numSV): # construct diagonal matrix from vector SigRecon[k, k] = Sigma[k] reconMat = np.matmul(np.matmul(U[:, :numSV], SigRecon), VT[:numSV, :]) print(\"****reconstructed matrix using %d singular values******\" % numSV) printMat(reconMat, thresh)if __name__ == '__main__': imgCompress() 结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566****original matrix******0000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000****reconstructed matrix using 5 singular values******0000000000000000000000000000000000000000000000111100000000000000000000000000001111100000000000000000000000001111111000000000000000000000111111111111000000000000000000001111111111111000000000000000000011111111111111000000000000000000111111000001111000000000000000001111100000001110000000000000000111110000000001110000000000000011111100000000011110000000000000111111000000000111100000000000001111110000000001111000000000000000111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011110000000000000011111000000000111100000000000000111110000000001111000000000000001111100000000011110000000000000011111000000000111000000000000000111110000000111100000000000000000011111111111111100000000000000000011111111111111000000000000000000111111111111110000000000000000000111111111100000000000000000000001111111110000000000000000000000000111110000000000000 可以发现，当我们只使用5个奇异值以及其对应的特征向量的时候，我们就可以获得非常好的重构效果，这个时候，我们的数据量为： \\[ s = 32 * 5 + 5 + 5 * 32 = 325 \\\\ compress\\_rate = \\frac{325}{32 * 32} = 37.1\\% \\] 我们的数据量仅仅是原来的三分之一，因此可以SVD很好的进行数据压缩。 SVD和PCA 在求解SVD的过程中，我们求解了两个特殊的矩阵\\(U\\) 和\\(V\\) ，实际上，\\(U\\) 被称为是左奇异矩阵，\\(V\\) 被称为是右奇异矩阵。 回想起在求解PCA的过程中，我们也需要求解右奇异矩阵，并利用右奇异矩阵进行了数据的降维，这就是右奇异矩阵的作用之一。当我们考察左奇异矩阵的时候，发现合理的使用右奇异矩阵本质上是对数据量进行了一定的减小，当我们拥有一个矩阵\\(A\\)，大小为\\(m \\times n\\)，其中，\\(m\\) 表示的是数据量，即行数，\\(n\\) 表示的是数据的维度数目，即列数，当我们对数据矩阵右乘右奇异矩阵的时候，列数可以根据选择的特征向量的数目进行减小，当我们当数据矩阵左乘左奇异矩阵的时候，行数可以根据选择的特征向量的数目进行减小。因此，左奇异矩阵可以压缩行数，右奇异矩阵可以压缩列数，综合在一起就可以对数据进行较为全面的压缩。","link":"/2019/05/10/Note4-SVD/"},{"title":"反向传播算法（二）之稍复杂的反向传播","text":"前言 前面介绍了单层全连接层，并且没有使用激活函数，这种情况比较简单，这一篇文章打算简单介绍一下多个输出，以及使用激活函数进行非线性激活的情况。还是请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 一、参数设置 和前面一样，这里使用的是长度为3的行向量，即 \\(x = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix}\\)，输出这里设置为长度为2的行向量，即 \\(\\hat{y} = \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix}\\)。权值参数我们记为 \\(\\omega\\)，偏置量我们记为 \\(b\\)，由于这里我们模拟的是进行分类操作，因此这里引入了一个非线性激活函数 \\(g\\)，为了方便我们进行求导，我们这里设置激活函数为sigmoid，即: \\[g(x) = \\frac{1}{1 + e^{-x}}, g\\prime(x) = g(x)(1 - g(x)) \\tag{1}\\] 有了上述的参数设置，我们可以有下面的式子： \\[ g(x \\omega + b) = \\hat{y} \\tag{2} \\] 继续将式子展开，我们有： \\[ g(\\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{21} &amp; \\omega_{22} \\\\ \\omega_{31} &amp; \\omega_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2\\end{bmatrix}) = \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix} \\tag{3} \\] 三、首先不考虑激活函数 我们首先不考虑激活函数，因此，我们可以暂时将结果记为 \\(a = \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}\\)。于是，我们可以得到下面的式子： \\[ \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix} = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{21} &amp; \\omega_{22} \\\\ \\omega_{31} &amp; \\omega_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2\\end{bmatrix} \\tag{4} \\] 将上面的公式(4)完全展开，可以得到下面的两个式子： \\[ a_1 = \\omega_{11} x_1 + \\omega_{21} x_2 + \\omega_{31} x_3 + b_1 \\tag{5} \\] \\[ a_2 = \\omega_{12} x_1 + \\omega_{22} x_2 + \\omega_{32} x_3 + b_2 \\tag{6} \\] 和前面的情况类似，我们可以对上面的两个式子中的参数求偏导数，于是，我们得到对于各个参数的偏导数计算公式如下： \\[ \\frac{\\partial a_1}{\\partial \\omega_{11}} = x_1, \\frac{\\partial a_1}{\\partial \\omega_{21}} = x_2, \\frac{\\partial a_1}{\\partial \\omega_{31}} = x_3, \\frac{\\partial a_1}{\\partial b_1} = 1 \\tag{7} \\] \\[ \\frac{\\partial a_2}{\\partial \\omega_{12}} = x_1, \\frac{\\partial a_2}{\\partial \\omega_{22}} = x_2, \\frac{\\partial a_2}{\\partial \\omega_{32}} = x_3, \\frac{\\partial a_2}{\\partial b_2} = 1 \\tag{8} \\] 以上就是现阶段的偏导数的计算公式。下一阶段我们将激活函数也考虑进来。 四、将激活函数也考虑进来 这一阶段我们考虑对 \\(a = \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}\\) 使用非线性激活函数激活，即我们有： \\[ \\hat{y} = g(a) \\tag{9} \\] 展开之后就变成： \\[ \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix} = g(\\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}) \\tag{10} \\] 对应每一个元素，我们有： \\[ \\hat{y}_1 = g(a_1), \\hat{y}_2 = g(a_2) \\tag{11} \\] 所以我们求得每一个 \\(\\hat{y}_i\\) 对 \\(a_i\\) 的偏导数如下： \\[ \\frac{\\partial \\hat{y}_1}{\\partial a_1} = g\\prime(a_1), \\frac{\\partial \\hat{y}_2}{\\partial a_2} = g\\prime(a_2) \\tag{12} \\] 五、损失值定义 和前面的情况类似，我们使用输出与目标值之间的差值的平方和作为最后的cost，即： \\[ C = cost = \\sum(\\hat{y}_i - y_i)^2 = (\\hat{y}_1 - y_1)^2 + (\\hat{y}_2 - y_2)^2 \\tag{13} \\] 根据上式，我们可以得到 \\(C\\) 关于两个预测输出 \\(\\hat{y}_1\\)，\\(\\hat{y}_2\\)的偏导数： \\[ \\frac{\\partial C}{\\partial \\hat{y}_1} = 2 * (\\hat{y}_1 - y_1), \\frac{\\partial C}{\\partial \\hat{y}_2} = 2 * (\\hat{y}_2 - y_2) \\tag{14} \\] 六、综合 前面所做的工作实际上是在一步一步求解每一个环节的偏导数公式，根据求导公式的链式法则（chain rule），我们可以得到以下的每一个参数（\\(\\omega\\)，\\(b\\)）对于最后的cost的偏导数公式： \\[ \\frac{\\partial C}{\\partial \\omega_{11}} = \\frac{\\partial a_1}{\\partial \\omega_{11}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_1 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{15} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{21}} = \\frac{\\partial a_1}{\\partial \\omega_{21}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_2 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{16} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{31}} = \\frac{\\partial a_1}{\\partial \\omega_{31}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_3 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{17} \\] \\[ \\frac{\\partial C}{\\partial b_1} = \\frac{\\partial a_1}{\\partial b_1} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{18} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{12}} = \\frac{\\partial a_2}{\\partial \\omega_{12}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_1 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{19} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{22}} = \\frac{\\partial a_2}{\\partial \\omega_{22}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_2 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{20} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{32}} = \\frac{\\partial a_2}{\\partial \\omega_{32}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_3 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{21} \\] \\[ \\frac{\\partial C}{\\partial b_2} = \\frac{\\partial a_2}{\\partial b_2} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{22} \\] 和前面一样，上面的公式已经可以用于进行梯度计算和反向传播了，但是上面的公式看上去不仅繁琐而且容易出错，因此，很有必要对上面的公式进行整理，以便我们用向量和矩阵进行表示和计算。 我们将每个变量的梯度按照次序排好，首先是 \\(\\omega\\) 参数的第一列，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{11}} \\\\ \\frac{\\partial C}{\\partial \\omega_{21}} \\\\ \\frac{\\partial C}{\\partial \\omega_{31}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\\\ x_2 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\\\ x_3 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\end{bmatrix} \\tag{23} \\] 接着是 \\(\\omega\\) 参数的第二列，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{12}} \\\\ \\frac{\\partial C}{\\partial \\omega_{22}} \\\\ \\frac{\\partial C}{\\partial \\omega_{32}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\\\ x_2 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\\\ x_3 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\tag{24} \\] 将两个矩阵结合在一起： \\[ \\begin {aligned} \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{11}} &amp; \\frac{\\partial C}{\\partial \\omega_{12}} \\\\ \\frac{\\partial C}{\\partial \\omega_{121}} &amp; \\frac{\\partial C}{\\partial \\omega_{22}} \\\\ \\frac{\\partial C}{\\partial \\omega_{31}} &amp; \\frac{\\partial C}{\\partial \\omega_{32}} \\end{bmatrix} &amp;= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= x^T \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\end{aligned} \\tag{25} \\] 注：上面公式中的 \\(\\cdot*\\) 为向量（矩阵）点乘，即表示向量（矩阵）对应位置的数值分别相乘，和矩阵的相乘不同。 最后化简如下: \\[ \\frac{\\partial C}{\\partial \\omega} = x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\tag{26} \\] 对于偏置量 \\(b\\)，我们计算梯度，然后进行整理，如下：（这里实际上进行了一定的简化，当我们设定偏置量为一个一维数组时，我们需要对下面的结果在列方向上取均值，以保证最后的结果可以和偏置量进行维度上的匹配。） \\[ \\begin{aligned} \\frac{\\partial C}{\\partial b} &amp;= \\begin{bmatrix} \\frac{\\partial C}{b_1} &amp; \\frac{\\partial C}{b_2} \\end{bmatrix} \\\\\\ &amp;= \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= \\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix} \\end{aligned} \\tag{27} \\] 以上就是单层全连接层，使用激活函数激活的梯度下降公式。 七、代码 这里我使用了两个训练样本，偏置量设定为一个1-D的数组，因此在更新参数时，需要对返回的结果取均值。详细请见backward函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npparam = {}nodes = {}learning_rate = 0.1def sigmoid(x): return 1.0 / (1. + np.exp(- x))def sigmoid_gradient(x): sig = sigmoid(x) return sig * (1. - sig)def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def forward(x): nodes['matmul'] = np.matmul(x, param['w']) nodes['bias'] = nodes['matmul'] + param['b'] nodes['sigmoid'] = sigmoid(nodes['bias']) return nodes['sigmoid']def backward(x, y_pred, y): matrix = np.multiply(sigmoid_gradient(nodes['bias']), cost_gradient(y_pred, y)) matrix2 = np.mean(matrix, 0, keepdims=False) param['w'] -= learning_rate * np.matmul(np.transpose(x), matrix) param['b'] -= learning_rate * matrix2def setup(): x = np.array([[1., 2., 3.], [3., 2., 1.]]) y = np.array([[1., 0.], [0., 1.]]) param['w'] = np.array([[.1, .2], [.3, .4], [.5, .6]]) param['b'] = np.array([0., 0.]) for i in range(1000): y_pred = forward(x) backward(x, y_pred, y) print(\"梯度下降前：\", y_pred, \"\\n梯度下降后：\", forward(x), \"\\ncost：\", cost(forward(x), y))if __name__ == '__main__': setup() 结果如下：可以看见，结果确实是在逐步想着目标结果靠近，cost值不断在减小。证明我们的算法是正确的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161梯度下降前： [[0.90024951 0.94267582] [0.80218389 0.88079708]]梯度下降后： [[0.87638772 0.93574933] [0.74070904 0.87317416]]cost： 1.4556414717601052梯度下降前： [[0.87638772 0.93574933] [0.74070904 0.87317416]]梯度下降后： [[0.84537106 0.92722992] [0.66043307 0.86435062]]cost： 1.3382380273209387梯度下降前： [[0.84537106 0.92722992] [0.66043307 0.86435062]]梯度下降后： [[0.80943371 0.91658752] [0.56909364 0.85403973]]cost： 1.2216201634346886梯度下降前： [[0.80943371 0.91658752] [0.56909364 0.85403973]]梯度下降后： [[0.77530479 0.90307287] [0.48379806 0.84187495]]cost： 1.1250926413642042梯度下降前： [[0.77530479 0.90307287] [0.48379806 0.84187495]]梯度下降后： [[0.74994151 0.88562481] [0.41750738 0.8273968 ]]cost： 1.050964830757349梯度下降前： [[0.74994151 0.88562481] [0.41750738 0.8273968 ]]梯度下降后： [[0.73518018 0.86276177] [0.37075203 0.81005788]]cost： 0.9880224900744513梯度下降前： [[0.73518018 0.86276177] [0.37075203 0.81005788]]梯度下降后： [[0.7288795 0.83251337] [0.33814879 0.78928408]]cost： 0.9253306342190869梯度下降前： [[0.7288795 0.83251337] [0.33814879 0.78928408]]梯度下降后： [[0.72817698 0.7925729 ] [0.31464068 0.76467358]]cost： 0.8564368364354394梯度下降前： [[0.72817698 0.7925729 ] [0.31464068 0.76467358]]梯度下降后： [[0.73084978 0.74107485] [0.29686131 0.73646017]]cost： 0.7792136510576879梯度下降前： [[0.73084978 0.74107485] [0.29686131 0.73646017]]梯度下降后： [[0.73542993 0.67843692] [0.28276129 0.70627592]]cost： 0.6965017597370333梯度下降前： [[0.73542993 0.67843692] [0.28276129 0.70627592]]梯度下降后： [[0.74100699 0.60952933] [0.27110861 0.67770711]]cost： 0.6159759746541653梯度下降前： [[0.74100699 0.60952933] [0.27110861 0.67770711]]梯度下降后： [[0.7470327 0.54300877] [0.2611523 0.65537568]]cost： 0.5458174231304158梯度下降前： [[0.7470327 0.54300877] [0.2611523 0.65537568]]梯度下降后： [[0.75318337 0.48629069] [0.25242337 0.64233397]]cost： 0.48903961977358096梯度下降前： [[0.75318337 0.48629069] [0.25242337 0.64233397]]梯度下降后： [[0.75927196 0.44162035] [0.24462032 0.63846146]]cost： 0.4435277424325401梯度下降前： [[0.75927196 0.44162035] [0.24462032 0.63846146]]梯度下降后： [[0.76519387 0.40729807] [0.23754304 0.64153151]]cost： 0.4059519984224861梯度下降前： [[0.76519387 0.40729807] [0.23754304 0.64153151]]梯度下降后： [[0.77089406 0.38056044] [0.23105412 0.64898998]]cost： 0.3739098246421919梯度下降前： [[0.77089406 0.38056044] [0.23105412 0.64898998]]梯度下降后： [[0.77634715 0.35906729] [0.22505587 0.65883242]]cost： 0.3459953751213052梯度下降前： [[0.77634715 0.35906729] [0.22505587 0.65883242]]梯度下降后： [[0.78154526 0.34118504] [0.21947641 0.66972652]]cost： 0.3213801718742878梯度下降前： [[0.78154526 0.34118504] [0.21947641 0.66972652]]梯度下降后： [[0.78649079 0.32585178] [0.21426107 0.68086606]]cost： 0.29951983756020983......梯度下降前： [[0.97352909 0.02666433] [0.02647091 0.97333567]]梯度下降后： [[0.97354315 0.02664997] [0.02645685 0.97335003]]cost： 0.002820371366327034梯度下降前： [[0.97354315 0.02664997] [0.02645685 0.97335003]]梯度下降后： [[0.97355719 0.02663563] [0.02644281 0.97336437]]cost： 0.002817357738697952梯度下降前： [[0.97355719 0.02663563] [0.02644281 0.97336437]]梯度下降后： [[0.97357121 0.02662131] [0.02642879 0.97337869]]cost： 0.002814350458880144梯度下降前： [[0.97357121 0.02662131] [0.02642879 0.97337869]]梯度下降后： [[0.9735852 0.02660701] [0.0264148 0.97339299]]cost： 0.0028113495069739336梯度下降前： [[0.9735852 0.02660701] [0.0264148 0.97339299]]梯度下降后： [[0.97359917 0.02659274] [0.02640083 0.97340726]]cost： 0.002808354863162452梯度下降前： [[0.97359917 0.02659274] [0.02640083 0.97340726]]梯度下降后： [[0.97361312 0.02657849] [0.02638688 0.97342151]]cost： 0.0028053665077110495梯度下降前： [[0.97361312 0.02657849] [0.02638688 0.97342151]]梯度下降后： [[0.97362705 0.02656426] [0.02637295 0.97343574]]cost： 0.002802384420967047梯度下降前： [[0.97362705 0.02656426] [0.02637295 0.97343574]]梯度下降后： [[0.97364096 0.02655005] [0.02635904 0.97344995]]cost： 0.002799408583359122梯度下降前： [[0.97364096 0.02655005] [0.02635904 0.97344995]]梯度下降后： [[0.97365484 0.02653587] [0.02634516 0.97346413]]cost： 0.002796438975397068梯度下降前： [[0.97365484 0.02653587] [0.02634516 0.97346413]]梯度下降后： [[0.97366871 0.02652171] [0.02633129 0.97347829]]cost： 0.002793475577671274梯度下降前： [[0.97366871 0.02652171] [0.02633129 0.97347829]]梯度下降后： [[0.97368255 0.02650757] [0.02631745 0.97349243]]cost： 0.0027905183708523346梯度下降前： [[0.97368255 0.02650757] [0.02631745 0.97349243]]梯度下降后： [[0.97369637 0.02649345] [0.02630363 0.97350655]]cost： 0.002787567335690624梯度下降前： [[0.97369637 0.02649345] [0.02630363 0.97350655]]梯度下降后： [[0.97371017 0.02647935] [0.02628983 0.97352065]]cost： 0.0027846224530159356","link":"/2019/05/12/Note7-BackProp-2/"},{"title":"反向传播算法（三）之完整的反向传播算法","text":"前言 前面介绍了单层全连接层并使用激活函数激活的情况，尝试去进行了多样本的梯度下降计算，这一篇文章打算简单介绍一下多层全连接层的梯度下降的情况，重点在于如何进行梯度的向后传播。还是请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 一、模型定义 和前面的模型类似，我们使用的输入是一个长度为3的行向量，输出为长度为2的行向量，激活函数设置为 \\(g\\)，我们这里使用的是sigmoid激活函数，即： \\[ g(x) = \\frac{1}{1 + e^{-x}} \\tag{1} \\] 模型定义如图，首先定义一下字母记号（这里的字母表示是根据我自己的习惯来的，和其他的表示方法或许有点不同，不过没有关系。），\\(L\\)表示网络的层数，在我们上图中，可以靠打拼一共有三层(包括输入层)，所以\\(L = 3\\)。我们记网络的第\\(i\\)层为\\(a^i\\)，将输入层记作\\(a^0 = x\\)，很明显，输出层我们可以记作\\(a^{L-1} = \\hat{y}\\)，这里的 \\(\\hat{y}\\) 表示整个网络的输出。一般地，我们使用上标标记参数是属于网络的哪一层，下标表示该参数在参数矩阵（向量）中的位置。 我们在这里使用\\(z^i\\)表示还没有使用激活函数的网络第\\(i\\)层，很明显这里的\\(i\\)的范围是：\\(1 \\leq i \\leq L-1\\)，因为输入层不需要使用激活函数激活。 于是,我们可以得到下面的式子： \\[ \\begin{aligned} z^0 &amp;= a^0 \\omega^0 + b^0 \\\\ a^1 &amp;= g(z^0) \\\\ z^1 &amp;= a^1 \\omega^1 + b^1 \\\\ \\hat{y} &amp;= a^2 = g(z^1) \\end{aligned} \\] 其中的 \\(\\omega\\)表示的是每一层全连接的权重矩阵，\\(b\\)表示的是每一层的偏置量。不难看出，\\(\\omega^0\\)是一个3x3大小的矩阵，\\(b^0\\)是一个长度为3的行向量，\\(\\omega^1\\)是一个3x2大小的矩阵，\\(b^1\\)是一个长度为2的行向量。 和前面定义的模型类似，这里我们仍然使用差的平方之和作为最后的损失函数，即： \\[ C = cost(\\hat{y}, y) = \\sum(\\hat{y}_i - y_i)^2 = (\\hat{y}_1 - y_1)^2 + (\\hat{y}_2 - y_2)^2 = (a^2_1 - y_1)^2 + (a^2_2 - y_2)^2 \\tag{2} \\] 二、基本原理 首先要了解的是，所谓的反向传播，到底向后传播的是什么。简单来说，算法向后传播的是误差，即我们希望的目标值和真实值之间的差距，这里的目标值是网络每一层的输出，这里的真实值是理想中的那个完美的模型产生的数值。但是很显然，我们并不了解那个完美模型的每一层的输出是什么，我们只知道最后的标签（即 \\(y\\) ），所以我们需要根据最后一层的输出和 \\(y\\) 之间的误差，去调整每一层的输出，在这个调整的过程中，我们就是在调整每一层的权值和偏置量。 理解偏导数 对于偏导数 \\(\\frac{\\partial C}{\\partial a^{i}} ,(1 \\leq i \\leq L - 1)\\)，我们可以将这个偏导数理解成对于 \\(a^i\\)的一个小小的变化，\\(C\\)能有多敏感，我们这里说到的敏感度和前面说的误差本质上是一回事，因为每一层的 \\(a\\)都受到前面一层的输出的影响，所以当我们在向后传播误差到前面的全连接层的时候，我们必然会求出每一层的偏导数，即 \\(\\frac{\\partial C}{\\partial a^{i}} ,(1 \\leq i \\leq L - 1)\\)。此处 \\(i\\)不会取到0，这是因为在我们设定的模型结构中，\\(a^0\\) 表示的是输出层，而输入层本质上是不包含误差的，因此在我们这样的设置下，\\(i\\)的范围是\\(1，2，...,L- 1\\)。需要注意的是，有些网络会将输入层表示为 \\(a^1\\)，此时，\\(i\\)的最小取值就是2。不管如何设置，这些都是基于相同的原理。 求解偏导数 假设我们现在只关注最后的一层全连接层，会有： \\[ g(\\begin{bmatrix} a^1_1 &amp; a^1_2 &amp; a^1_3 \\end{bmatrix} \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{12} \\\\ \\omega^1_{21} &amp; \\omega^1_{22} \\\\ \\omega^1_{31} &amp; \\omega^1_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b^1_1 &amp; b^1_2\\end{bmatrix}) = \\begin{bmatrix} a^2_1 &amp; a^2_2 \\end{bmatrix} \\tag{3} \\] 我们沿用之前定义好的字母表示方法，用 \\(z^i\\)表示每一层未被激活时的矩阵，于是，我们可以有下面的式子： \\[ z^1 = a^1 \\omega^1 + b^1 \\\\ a^2 = g(z^1) \\\\ C = \\sum (a^2_i - y_i) ^2 \\] 将上面的第一个式子展开，我们有： \\[ \\begin{bmatrix} z^1_1 &amp; z^1_2\\end{bmatrix} = \\begin{bmatrix} a^1_1 &amp; a^1_2 &amp; a^1_3 \\end{bmatrix} \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{12} \\\\ \\omega^1_{21} &amp; \\omega^1_{22} \\\\ \\omega^1_{31} &amp; \\omega^1_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b^1_1 &amp; b^1_2\\end{bmatrix} \\tag{4} \\] 继续将式子完全展开： \\[ z^1_1 = a^1_1 \\omega^1_{11} + a^1_2 \\omega^1_{21} + a^1_3 \\omega^1_{31} + b^1_1 \\tag{5} \\] \\[ z^1_2 = a^1_1 \\omega^1_{12} + a^1_2 \\omega^1_{22} + a^1_3 \\omega^1_{32} + b^1_2 \\tag{6} \\] 接着我们对前一层的输出求解偏导数，即，我们需要对 \\(a^1_1\\)，\\(a^1_2\\)，\\(a^1_3\\)求解偏导数。所以我们会有： \\[ \\frac{\\partial z^1_1}{\\partial a^1_1} = \\omega^1_{11},\\frac{\\partial z^1_1}{\\partial a^1_2} = \\omega^1_{21},\\frac{\\partial z^1_1}{\\partial a^1_3} = \\omega^1_{31} \\tag{7} \\] \\[ \\frac{\\partial z^1_2}{\\partial a^1_1} = \\omega^1_{12},\\frac{\\partial z^1_2}{\\partial a^1_2} = \\omega^1_{22},\\frac{\\partial z^1_2}{\\partial a^1_3} = \\omega^1_{32} \\tag{8} \\] 确实，这一步有些难以理解，实际上我们只是将后面一层的误差（敏感程度）通过求导的方式传递到前面一层而已。 对 \\(z^i\\)求偏导数 我们考虑对 \\(z^1 = \\begin{bmatrix} z^1_1 &amp; z^1_2 \\end{bmatrix}\\) 使用非线性激活函数激活，即我们有： \\[ a^2 = g(z^1) \\tag{9} \\] 展开之后就变成： \\[ \\begin{bmatrix} a^2_1 &amp; a^2_2 \\end{bmatrix} = g(\\begin{bmatrix} z^1_1 &amp; z^1_2 \\end{bmatrix}) \\tag{10} \\] 对应每一个元素，我们有： \\[ a^2_1 = g(z^1_1), a^2_2 = g(z^1_2) \\tag{11} \\] 所以我们求得每一个 \\(\\hat{y}_i\\) 对 \\(a_i\\) 的偏导数如下： \\[ \\frac{\\partial a^2_1}{\\partial z^1_1} = g\\prime(z^1_1), \\frac{\\partial a^2_2}{\\partial z^1_2} = g\\prime(z^1_2) \\tag{12} \\] cost值的相关偏导数 因为 \\(C = cost = (a^2_1 - y_1)^2 + (a^2_2 - y_2)^2\\)，所以我们可以求得： \\[ \\frac{\\partial C}{\\partial a^2_1} = 2 (a^2_1 - y_1),\\frac{\\partial C}{\\partial a^2_2} = 2 (a^2_2 - y_2) \\tag{13} \\] 整理总结 根据我们之前求出来的结果，我们可以将误差传递至 \\(a^1\\)层，于是，我们可以得到下面的几个式子： \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_1}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.1} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_1}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.2} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_1}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.3} \\] \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_2}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.4} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_2}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.5} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_2}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.6} \\] 我们发现，上面的公式中，(14.1)和(14.4)，(14.2)和(14.5)，(14.3)和(14.6)计算的时同一个偏导数，那么究竟哪个偏导数的计算时正确的呢？实际上，每一个都不是正确的，但是每一个有都不是错误的，或者说每一个都只做了一半。这是因为每一个值都可以通过多条路径去影响最后的cost值。例如，以 \\(a^1_1\\)为例，它既可以和 \\(\\omega^1_{11}\\) 相乘来影响 \\(a^2_1\\) ，也可以通过和 \\(\\omega^1_{12}\\) 相乘来影响 \\(a^2_2\\) ，而这两条路最后都会影响cost值，因此，我们需要将所有的偏导数公式进行相加，得到我们最后真正的偏导数计算公式。 注意：实际上，如果对高等数学的链式法则求导有更深入的观察，可以一步就写出最后的偏导数公式，而我们上面这样做其实是不正确的，但是可以得出正确的结果。 \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_1}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} + \\frac{\\partial z^1_2}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.1} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_1}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} + \\frac{\\partial z^1_2}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.2} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_1}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} +\\frac{\\partial z^1_2}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.3} \\] 同样，和前面一样，我们需要对上面的公式进行向量化的表示，这样代码编写会方便很多且不易出错。我们将(15.1)，(15.2)和(15.3)的结果整理成一个行向量（因为我们设定的模型的输入是一个行向量，所以，模型每一层的输出也都是一个行向量。行向量如下： \\[ \\begin{aligned} \\begin{bmatrix} \\frac{\\partial C}{\\partial a^1_1} &amp; \\frac{\\partial C}{\\partial a^1_2} &amp; \\frac{\\partial C}{\\partial a^1_3}\\end{bmatrix} &amp;= \\begin{bmatrix} \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial z^1_1}{\\partial a^1_1} &amp; \\frac{\\partial z^1_1}{\\partial a^1_2} &amp; \\frac{\\partial z^1_1}{\\partial a^1_3} \\\\ \\frac{\\partial z^1_2}{\\partial a^1_1} &amp; \\frac{\\partial z^1_2}{\\partial a^1_2} &amp; \\frac{\\partial z^1_2}{\\partial a^1_3} \\end{bmatrix} \\\\ &amp;= (\\begin{bmatrix} \\frac{\\partial a^2_1}{\\partial z^1_1} &amp; \\frac{\\partial a^2_2}{\\partial z^1_2}\\end{bmatrix} \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) \\begin{bmatrix} \\frac{\\partial z^1_1}{\\partial a^1_1} &amp; \\frac{\\partial z^1_1}{\\partial a^1_2} &amp; \\frac{\\partial z^1_1}{\\partial a^1_3} \\\\ \\frac{\\partial z^1_2}{\\partial a^1_1} &amp; \\frac{\\partial z^1_2}{\\partial a^1_2} &amp; \\frac{\\partial z^1_2}{\\partial a^1_3} \\end{bmatrix} \\\\ &amp;= (g\\prime(z^1) \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{21} &amp; \\omega^1_{31} \\\\ \\omega^1_{12} &amp; \\omega^1_{22} &amp; \\omega^1_{32} \\end{bmatrix} \\\\ &amp;= (g\\prime(z^1) \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) (\\omega^1)^T \\end{aligned} \\tag{16} \\] 在上面的公式中，\\((\\omega^1)^T​\\) 表示的是 \\(\\omega^1​\\) 参数矩阵的转置，\\(\\cdot *​\\) 符号表示的是矩阵（向量）之间的点乘，即对应元素之间的相乘。 如果我们将 \\(\\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}​\\) 记作 \\(\\delta^2​\\) ，将 \\(\\begin{bmatrix} \\frac{\\partial C}{\\partial a^1_1} &amp; \\frac{\\partial C}{\\partial a^1_2} &amp; \\frac{\\partial C}{\\partial a^1_3}\\end{bmatrix}​\\) 记作 \\(\\delta^1​\\)，那么我们就会得到更加简化的公式： \\[ \\delta^1 = (g\\prime(z^1) \\cdot * \\delta^2)(\\omega^1)^T \\tag{17} \\] 公式(17)就是我们需要找到的反向传播的核心公式。更一般的，如果我们有 \\(L\\) 层网络（包括输入层，那么，误差向后传递的核心公式就是如下： \\[ \\delta^i = (g\\prime(z^i) \\cdot * \\delta^{i + 1})(\\omega^i)^T \\quad (1 \\leq i \\leq L - 1) \\tag{18} \\] 其中，最后一层的 \\(\\delta^{L-1}\\) 就是根据输出值和真实值的计算公式，对每一个输出值进行求导操作。 根据传递来的误差进行参数的更新 现在，让我们重新审视一下我们之前在第二篇中求解出的参数更新公式。 \\[ \\frac{\\partial C}{\\partial \\omega} = x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\tag{19} \\] \\[ \\frac{\\partial C}{\\partial b} = \\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix} \\tag{20} \\] 按照我们在这一篇中使用的符号记法，重新写一下会有（因为这里有两层全连接层，之前只有一层，因此，我们这里的 \\(\\omega\\) 和 \\(b\\) 都使用的是最后一层的权值和偏置量，因此 \\(x\\) 就变成了倒数第二层的输出 \\(a^1\\)。）： \\[ \\frac{\\partial C}{\\partial \\omega^1} = (a^1)^T(g\\prime(z^1) \\cdot * \\delta^2) \\tag{21} \\] \\[ \\frac{\\partial C}{\\partial b^1} = g\\prime(z^1) \\cdot * \\delta^2 \\tag{22} \\] 于是根据上面的式子，我们就可以归纳出一般情况下的权重和偏置量的偏导数公式了。如果按照我们这篇文章中的字母记号的方法，那么，我们可以有：（\\(0 \\leq i \\leq L- 2\\)，\\(L\\)表示的是网络的层数，包括输入层。） \\[ \\frac{\\partial C}{\\partial \\omega^i} = (a^i)^T(g\\prime(z^i) \\cdot * \\delta^{i+1}) \\tag{23} \\] \\[ \\frac{\\partial C}{\\partial b^i} = g\\prime(z^i) \\cdot * \\delta^{i+1} \\tag{24} \\] 公式(18),(23),(24)就是反向传播算法的核心公式了，一般而言，我们首先会求出所有的 \\(\\delta\\) 参数，再根据 \\(\\delta\\)参数去求解所有的参数梯度，最后统一进行梯度的更新。 三、代码 和文中所使用的模型是一样的，由两个全连接层构成，使用sigmoid函数激活。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as npparam = {}nodes = {}learning_rate = 0.1def sigmoid(x): return 1.0 / (1. + np.exp(- x))def sigmoid_gradient(x): sig = sigmoid(x) return sig * (1. - sig)def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def forward(x): nodes[\"a0\"] = x nodes['matmul0'] = np.matmul(x, param['w0']) nodes['z0'] = nodes['matmul0'] + param['b0'] nodes[\"a1\"] = sigmoid(nodes['z0']) nodes['matmul1'] = np.matmul(nodes['a1'], param['w1']) nodes['z1'] = nodes['matmul1'] + param['b1'] nodes['a2'] = sigmoid(nodes['z1']) return nodes['a2'] passdef backward(x, y_pred, y): \"\"\"compute delta\"\"\" delta2 = cost_gradient(y_pred, y) delta1 = np.matmul(np.multiply(sigmoid_gradient(nodes['z1']), delta2), np.transpose(param['w1'])) \"\"\"update\"\"\" gradient = {} gradient['w1'] = np.matmul(np.transpose(nodes['a1']), np.multiply(sigmoid_gradient(nodes[\"z1\"]), delta2)) gradient['b1'] = np.mean(np.multiply(sigmoid_gradient(nodes[\"z1\"]), delta2), axis=0) gradient[\"w0\"] = np.matmul(np.transpose(nodes['a0']), np.multiply(sigmoid_gradient(nodes[\"z0\"]), delta1)) gradient['b0'] = np.mean(np.multiply(sigmoid_gradient(nodes[\"z0\"]), delta1), axis=0) param['w1'] -= learning_rate * gradient['w1'] param['b1'] -= learning_rate * gradient['b1'] param[\"w0\"] -= learning_rate * gradient['w0'] param['b0'] -= learning_rate * gradient['b0'] passdef setup(): x = np.array([[1., 2., 3.], [3., 2., 1.]]) y = np.array([[1., 0.], [0., 1.]]) param['w0'] = np.random.random([3, 3]) param['b0'] = np.array([0., 0., 0.]) param['w1'] = np.random.random([3, 2]) param['b1'] = np.array([0., 0.]) for i in range(1000): y_pred = forward(x) backward(x, y_pred, y) print(\"梯度下降前：\", y_pred, \"\\n梯度下降后：\", forward(x), \"\\ncost：\", cost(forward(x), y))if __name__ == '__main__': setup() 结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176梯度下降前： [[0.79830536 0.83580604] [0.80449064 0.83875726]]梯度下降后： [[0.78872254 0.82729775] [0.79552187 0.83086468]]cost： 1.3905215341662558梯度下降前： [[0.78872254 0.82729775] [0.79552187 0.83086468]]梯度下降后： [[0.77882103 0.81832367] [0.78626614 0.82257321]]cost： 1.3682684724974281梯度下降前： [[0.77882103 0.81832367] [0.78626614 0.82257321]]梯度下降后： [[0.76863531 0.80888786] [0.77675443 0.81388922]]cost： 1.3458138579376486梯度下降前： [[0.76863531 0.80888786] [0.77675443 0.81388922]]梯度下降后： [[0.75820643 0.79900214] [0.76702339 0.8048258 ]]cost： 1.3232863979727467梯度下降前： [[0.75820643 0.79900214] [0.76702339 0.8048258 ]]梯度下降后： [[0.74758142 0.78868705] [0.75711474 0.79540344]]cost： 1.3008248813647023梯度下降前： [[0.74758142 0.78868705] [0.75711474 0.79540344]]梯度下降后： [[0.73681235 0.77797253] [0.74707448 0.7856506 ]]cost： 1.27857494201472梯度下降前： [[0.73681235 0.77797253] [0.74707448 0.7856506 ]]梯度下降后： [[0.72595508 0.76689824] [0.73695182 0.77560392]]cost： 1.2566851295390669梯度下降前： [[0.72595508 0.76689824] [0.73695182 0.77560392]]梯度下降后： [[0.71506782 0.75551347] [0.72679789 0.76530802]]cost： 1.2353024551577543梯度下降前： [[0.71506782 0.75551347] [0.72679789 0.76530802]]梯度下降后： [[0.70420955 0.74387642] [0.71666439 0.75481498]]cost： 1.214567655578547梯度下降前： [[0.70420955 0.74387642] [0.71666439 0.75481498]]梯度下降后： [[0.69343832 0.73205294] [0.70660222 0.74418325]]cost： 1.1946104774003092梯度下降前： [[0.69343832 0.73205294] [0.70660222 0.74418325]]梯度下降后： [[0.68280973 0.72011482] [0.69666013 0.73347616]]cost： 1.1755453177778676梯度下降前： [[0.68280973 0.72011482] [0.69666013 0.73347616]]梯度下降后： [[0.67237554 0.70813752] [0.68688359 0.72276011]]cost： 1.1574675529546317梯度下降前： [[0.67237554 0.70813752] [0.68688359 0.72276011]]梯度下降后： [[0.66218247 0.69619771] [0.67731377 0.71210253]]cost： 1.1404508391169166梯度下降前： [[0.66218247 0.69619771] [0.67731377 0.71210253]]梯度下降后： [[0.65227136 0.6843707 ] [0.66798687 0.70156968]]cost： 1.124545582249731梯度下降前： [[0.65227136 0.6843707 ] [0.66798687 0.70156968]]梯度下降后： [[0.64267666 0.67272797] [0.65893364 0.69122464]]cost： 1.1097786568052626梯度下降前： [[0.64267666 0.67272797] [0.65893364 0.69122464]]梯度下降后： [[0.63342615 0.66133502] [0.6501792 0.68112551]]cost： 1.0961543258622262梯度下降前： [[0.63342615 0.66133502] [0.6501792 0.68112551]]梯度下降后： [[0.62454101 0.65024966] [0.64174305 0.6713239 ]]cost： 1.0836561996688798梯度下降前： [[0.62454101 0.65024966] [0.64174305 0.6713239 ]]梯度下降后： [[0.61603613 0.63952088] [0.63363933 0.66186397]]cost： 1.0722499837437804梯度下降前： [[0.61603613 0.63952088] [0.63363933 0.66186397]]梯度下降后： [[0.60792055 0.62918816] [0.62587715 0.6527818 ]]cost： 1.0618867231940436梯度下降前： [[0.60792055 0.62918816] [0.62587715 0.6527818 ]]梯度下降后： [[0.60019807 0.61928143] [0.61846111 0.64410532]]cost： 1.0525062481304481梯度下降前： [[0.60019807 0.61928143] [0.61846111 0.64410532]]梯度下降后： [[0.59286793 0.60982139] [0.6113918 0.63585445]]cost： 1.0440405589832986梯度下降前： [[0.59286793 0.60982139] [0.6113918 0.63585445]]梯度下降后： [[0.5859255 0.60082016] [0.60466638 0.62804172]]cost： 1.036416947829888梯度下降前： [[0.5859255 0.60082016] [0.60466638 0.62804172]]梯度下降后： [[0.57936295 0.59228224] [0.59827914 0.62067296]]cost： 1.029560718871301梯度下降前： [[0.57936295 0.59228224] [0.59827914 0.62067296]]梯度下降后： [[0.57316992 0.58420554] [0.59222202 0.61374817]]cost： 1.0233974361978069梯度下降前： [[0.57316992 0.58420554] [0.59222202 0.61374817]]梯度下降后： [[0.5673341 0.57658245] [0.58648511 0.60726243]]cost： 1.0178546819469387梯度下降前： [[0.5673341 0.57658245] [0.58648511 0.60726243]]梯度下降后： [[0.56184178 0.56940091] [0.58105707 0.6012068 ]]cost： 1.0128633489602261......梯度下降前： [[0.94927668 0.05029516] [0.05573697 0.94479925]]梯度下降后： [[0.94931374 0.05025926] [0.05570013 0.94483464]]cost： 0.011240812046284514梯度下降前： [[0.94931374 0.05025926] [0.05570013 0.94483464]]梯度下降后： [[0.94935073 0.05022344] [0.05566335 0.94486995]]cost： 0.011225473896214073梯度下降前： [[0.94935073 0.05022344] [0.05566335 0.94486995]]梯度下降后： [[0.94938765 0.05018769] [0.05562665 0.9449052 ]]cost： 0.011210176001756078梯度下降前： [[0.94938765 0.05018769] [0.05562665 0.9449052 ]]梯度下降后： [[0.94942449 0.05015202] [0.05559002 0.94494039]]cost： 0.011194918207872375梯度下降前： [[0.94942449 0.05015202] [0.05559002 0.94494039]]梯度下降后： [[0.94946125 0.05011641] [0.05555345 0.94497551]]cost： 0.011179700360308792梯度下降前： [[0.94946125 0.05011641] [0.05555345 0.94497551]]梯度下降后： [[0.94949794 0.05008087] [0.05551696 0.94501057]]cost： 0.011164522305590443梯度下降前： [[0.94949794 0.05008087] [0.05551696 0.94501057]]梯度下降后： [[0.94953456 0.05004541] [0.05548053 0.94504556]]cost： 0.011149383891016414梯度下降前： [[0.94953456 0.05004541] [0.05548053 0.94504556]]梯度下降后： [[0.9495711 0.05001001] [0.05544418 0.94508049]]cost： 0.011134284964655492梯度下降前： [[0.9495711 0.05001001] [0.05544418 0.94508049]]梯度下降后： [[0.94960757 0.04997468] [0.05540789 0.94511535]]cost： 0.011119225375340889 可以看到，我们的算法是可以很好的进行反向传播，并且可以很好地减小cost值。","link":"/2019/05/12/Note8-BackProp-3/"},{"title":"YOLOv3源码阅读：model.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族，针对于Two-Stage目标检测算法普遍存在的运算速度慢的缺点，YOLO创造性的提出了One-Stage。也就是将物体分类和物体定位在一个步骤中完成。YOLO直接在输出层回归bounding box的位置和bounding box所属类别，从而实现one-stage。 经过两次迭代，YOLO目前的最新版本为YOLOv3，在前两版的基础上，YOLOv3进行了一些比较细节的改动，效果有所提升。 本文正是希望可以将源码加以注释，方便自己学习，同时也愿意分享出来和大家一起学习。由于本人还是一学生，如果有错还请大家不吝指出。 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：YOUR_PATH\\YOLOv3_TensorFlow-master.py 这里函数和类的主要作用是对YOLO模型进行封装，类中的函数主要包括： 模型的简历 特征图信息和anchors的联合使用 loss的计算 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536# coding=utf-8# for better understanding about yolov3 architecture, refer to this website (in Chinese):# https://blog.csdn.net/leviopku/article/details/82660381from __future__ import division, print_functionimport tensorflow as tfslim = tf.contrib.slimfrom utils.layer_utils import conv2d, darknet53_body, yolo_block, upsample_layerclass yolov3(object): def __init__(self, class_num, anchors, use_label_smooth=False, use_focal_loss=False, batch_norm_decay=0.999, weight_decay=5e-4): \"\"\" yolov3 class :param class_num: 类别数目 :param anchors: anchors，一般来说是9个anchors :param use_label_smooth: 是否使用label smooth，默认为False :param use_focal_loss: 是否使用focal loss，默认为False :param batch_norm_decay: BN的衰减系数 :param weight_decay: 权重衰减系数 \"\"\" # self.anchors = [[10, 13], [16, 30], [33, 23], # [30, 61], [62, 45], [59, 119], # [116, 90], [156, 198], [373,326]] self.class_num = class_num self.anchors = anchors self.batch_norm_decay = batch_norm_decay self.use_label_smooth = use_label_smooth self.use_focal_loss = use_focal_loss self.weight_decay = weight_decay def forward(self, inputs, is_training=False, reuse=False): \"\"\" 进行正向传播，返回的是若干特征图 :param inputs: shape: [N, height, width, channel] :param is_training: :param reuse: :return: \"\"\" # 获取输入图片的高度height和宽度width # the input img_size, form: [height, width] # it will be used later self.img_size = tf.shape(inputs)[1:3] # batch normalization的相关参数 # set batch norm params batch_norm_params = { 'decay': self.batch_norm_decay, 'epsilon': 1e-05, 'scale': True, 'is_training': is_training, 'fused': None, # Use fused batch norm if possible. } # slim的arg scope，可以简化代码的编写，共用一套参数设置 with slim.arg_scope([slim.conv2d, slim.batch_norm], reuse=reuse): with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, biases_initializer=None, activation_fn=lambda x: tf.nn.leaky_relu(x, alpha=0.1), weights_regularizer=slim.l2_regularizer(self.weight_decay)): # DarkNet 的主体部分，主要作用是提取图片中的各种特征信息。 # 这里可以获取三张特征图，分别取自DarkNet的三个不同的阶段。 # 每一个阶段对应于不同的特征粒度，结合更多的特征可以增强模型的表达能力。 # 理论上来说特征提取网络也可以采用其他的网络结构，但是效果可能会有所差异。 # 如果输入图片的尺寸为[416, 416]，则三张特征图的尺寸分别为 # route_1 : [1, 52, 52, 256] # route_2 : [1, 26, 26, 512] # route_3 : [1, 13, 13, 1024] with tf.variable_scope('darknet53_body'): route_1, route_2, route_3 = darknet53_body(inputs) # 根据前面的特征图，进行特征融合操作，这样可以提供更多的信息。 with tf.variable_scope('yolov3_head'): # 使用YOLO_block函数来处理得到的特征图，并返回两张特征图。 # 本质上，YOLO_block函数仅仅包含若干层卷积层。 # 其中，inter1的作用是用来后续进行特征融合，net的主要作用是用以计算后续的坐标和概率等信息。 inter1, net = yolo_block(route_3, 512) # 进行依次卷积，主要是为了进行通道数目调整 feature_map_1 = slim.conv2d(net, 3 * (5 + self.class_num), 1, stride=1, normalizer_fn=None, activation_fn=None, biases_initializer=tf.zeros_initializer()) feature_map_1 = tf.identity(feature_map_1, name='feature_map_1') # 进行一次卷积，调整通道数目为256。并进行上采样，这里的上采样主要是用最近邻插值法。 inter1 = conv2d(inter1, 256, 1) inter1 = upsample_layer(inter1, tf.shape(route_2)) # 进行特征的融合，这里是通道的融合 concat1 = tf.concat([inter1, route_2], axis=3) # 下面的和前面的过程是一致的，不再赘述。 inter2, net = yolo_block(concat1, 256) feature_map_2 = slim.conv2d(net, 3 * (5 + self.class_num), 1, stride=1, normalizer_fn=None, activation_fn=None, biases_initializer=tf.zeros_initializer()) feature_map_2 = tf.identity(feature_map_2, name='feature_map_2') inter2 = conv2d(inter2, 128, 1) inter2 = upsample_layer(inter2, tf.shape(route_1)) concat2 = tf.concat([inter2, route_1], axis=3) _, feature_map_3 = yolo_block(concat2, 128) feature_map_3 = slim.conv2d(feature_map_3, 3 * (5 + self.class_num), 1, stride=1, normalizer_fn=None, activation_fn=None, biases_initializer=tf.zeros_initializer()) feature_map_3 = tf.identity(feature_map_3, name='feature_map_3') # 将三张特征图返回，shape分别如下：（输入图片尺寸默认为[416, 416]) # feature_map_1: [1, 13, 13, 255] # feature_map_2: [1, 26, 25, 255] # feature_map_3: [1, 52, 52, 255] return feature_map_1, feature_map_2, feature_map_3 def reorg_layer(self, feature_map, anchors): ''' feature_map: a feature_map from [feature_map_1, feature_map_2, feature_map_3] returned from `forward` function anchors: shape: [3, 2] ''' \"\"\"需要注意的是，我们在下面的代码中会经常涉及到height， width这两个概念，在YOLOv3中，height表示的是竖直方向， width表示的是水平方向，同样，x的方向也表示的是水平方向，y的方向是竖直方向\"\"\" # NOTE: size in [h, w] format! don't get messed up! # 获取特征图的尺寸信息，顺序为： [height, width] grid_size = tf.shape(feature_map)[1:3] # [13, 13] # the downscale ratio in height and weight # 计算此特征图和原图片的缩放尺寸，顺序为： [height, width] ratio = tf.cast(self.img_size / grid_size, tf.float32) # rescale the anchors to the feature_map # NOTE: the anchor is in [w, h] format! # 将anchors映射到特征图上,主要是大小上的映射,将anchors的尺寸分别处以下采样倍数即可 # 需要注意的是，anchors的顺序是[width, height]！所因此下面代码中ratio的下标是反的. # 所以计算出的rescaled_anchors的顺序也是[width, height]。 rescaled_anchors = [(anchor[0] / ratio[1], anchor[1] / ratio[0]) for anchor in anchors] # 将特征图reshape一下,主要是将最后一个通道进行分离 feature_map = tf.reshape(feature_map, [-1, grid_size[0], grid_size[1], 3, 5 + self.class_num]) # split the feature_map along the last dimension # shape info: take 416x416 input image and the 13*13 feature_map for example: # box_centers: [N, 13, 13, 3, 2] last_dimension: [center_x, center_y]. # 需要注意的是这里的center_x, 和center_y的方向表示,center_x表示的是 # box_sizes: [N, 13, 13, 3, 2] last_dimension: [width, height] # conf_logits: [N, 13, 13, 3, 1] # prob_logits: [N, 13, 13, 3, class_num] # 沿着最后一个数据通道进行分离,分别分离成2, 2, 1, class_num的矩阵. box_centers, box_sizes, conf_logits, prob_logits = tf.split(feature_map, [2, 2, 1, self.class_num], axis=-1) # 将box的中心数据限制在（0， 1）的范围之内， # 因为YOLO将图片分成了一个一个的格子，每一个格子的长宽被设置为1，这里的中心数据本质上是相对于格子左上角的偏移。 box_centers = tf.nn.sigmoid(box_centers) # use some broadcast tricks to get the mesh coordinates # grid_x: [0, 1, 2, ..., width - 1] grid_x = tf.range(grid_size[1], dtype=tf.int32) # grid_y: [0, 1, 2, ..., height - 1] grid_y = tf.range(grid_size[0], dtype=tf.int32) # grid_x: [[0, 1, 2, ..., width - 1], # [0, 1, 2, ..., width - 1], # ... # [0, 1, 2, ..., width - 1]] # grid_y: [[0, 0, 0, ..., 0], # [1, 1, 1, ..., 1], # ... # [height - 1, height - 1, height - 1, ..., height - 1]] grid_x, grid_y = tf.meshgrid(grid_x, grid_y) x_offset = tf.reshape(grid_x, (-1, 1)) # [0, 1, 2, .., width - 1, 0, 1, 2, ..width - 1, ......, 0, 1, 2, .. width - 1] y_offset = tf.reshape(grid_y, (-1, 1)) # [0, 0, 0, .., 0, 1, 1, 1, ...1, ......, height -1, height -1, .., height - 1] # x_y_offset: [[0, 0], # [1, 0], # ... # [width - 1, 0], # [0, 1], # [1, 1], # ... # [width - 1, 1], # ...... # [0, height - 1], # [1, height - 1], # ... # [width - 1, height - 1]] x_y_offset = tf.concat([x_offset, y_offset], axis=-1) # shape: [13, 13, 1, 2] 、[height, width, 1, 2] x_y_offset = tf.cast(tf.reshape(x_y_offset, [grid_size[0], grid_size[1], 1, 2]), tf.float32) # get the absolute box coordinates on the feature_map # broadcast机制： [N, height, width, 3, 2] = [N, height, width, 3, 2] + [height, width, 1, 2] box_centers = box_centers + x_y_offset # rescale to the original image scale # 将box的中心重新映射到原始尺寸的图片上。 # 在前面的代码中，最后一个维度的顺序一直是[width, height]的格式，二ratio的顺序是[height, width]， # 因此这是需要对ratio取反遍历，结果的顺序依然是[width, height]。 box_centers = box_centers * ratio[::-1] # avoid getting possible nan value with tf.clip_by_value # 和前面的过程一样，这里对box的尺寸进行变换，最后一维度的顺序依然是[width, height] box_sizes = tf.exp(box_sizes) * rescaled_anchors # box_sizes = tf.clip_by_value(tf.exp(box_sizes), 1e-9, 100) * rescaled_anchors # rescale to the original image scale # 一样是将box的尺寸重新映射到原始图片上 box_sizes = box_sizes * ratio[::-1] # shape: [N, 13, 13, 3, 4]、[N, height, width, 3, 4] # last dimension: (center_x, center_y, w, h) boxes = tf.concat([box_centers, box_sizes], axis=-1) # shape: # x_y_offset: [13, 13, 1, 2], [height, width, 1, 2] # boxes: [N, 13, 13, 3, 4], rescaled to the original image scale # conf_logits: [N, 13, 13, 3, 1]、 [N, height, width, 3, 1] # prob_logits: [N, 13, 13, 3, class_num]、 [N, height, width, 3, class_num] return x_y_offset, boxes, conf_logits, prob_logits def predict(self, feature_maps): ''' Receive the returned feature_maps from `forward` function, the produce the output predictions at the test stage. ''' # feature_map_1, feature_map_2, feature_map_3 = feature_maps # 将特征图和不同尺寸的anchors相结合，缩放程度大的特征图和大尺寸的anchors相结合， # 反之，缩放程度小的特征图和小尺寸的anchors相结合 feature_map_anchors = [(feature_map_1, self.anchors[6:9]), (feature_map_2, self.anchors[3:6]), (feature_map_3, self.anchors[0:3])] # 利用特征图和其对应的anchors计算每一张特征图的预测回归框，置信程度，分类概率等 reorg_results = [self.reorg_layer(feature_map, anchors) for (feature_map, anchors) in feature_map_anchors] def _reshape(result): # 取出每一个特征图对应的所有信息，包括预测回归框，置信程度，分类概率等 x_y_offset, boxes, conf_logits, prob_logits = result # 获得特征图的尺寸，[height, width] grid_size = tf.shape(x_y_offset)[:2] # 将boxes， 前景置信度，分类概率展开 boxes = tf.reshape(boxes, [-1, grid_size[0] * grid_size[1] * 3, 4]) conf_logits = tf.reshape(conf_logits, [-1, grid_size[0] * grid_size[1] * 3, 1]) prob_logits = tf.reshape(prob_logits, [-1, grid_size[0] * grid_size[1] * 3, self.class_num]) # shape: (take 416*416 input image and feature_map_1 for example), # boxes: [N, 13*13*3, 4] , [N, height * width * anchor_num, 4] # conf_logits: [N, 13*13*3, 1], [N, height * width * anchor_num, 1] # prob_logits: [N, 13*13*3, class_num], [N, height * width * anchor_num, class_num] return boxes, conf_logits, prob_logits boxes_list, confs_list, probs_list = [], [], [] for result in reorg_results: # 对每个特征图的偏移量，boxes，前景置信度，分类概率等进行处理（主要是reshape），得到boxes，前景置信度，分类概率。 boxes, conf_logits, prob_logits = _reshape(result) # 对置信度和概率进行sigmoid处理，保证数值位于0~1之间 confs = tf.sigmoid(conf_logits) probs = tf.sigmoid(prob_logits) # 将所有的boxes， 前景置信度，分类概率保存起来 boxes_list.append(boxes) confs_list.append(confs) probs_list.append(probs) # collect results on three scales # take 416*416 input image for example: # shape: [N, (13*13+26*26+52*52)*3, 4]、[N, box_num, 4] boxes = tf.concat(boxes_list, axis=1) # shape: [N, (13*13+26*26+52*52)*3, 1]、[N, box_num, 1] confs = tf.concat(confs_list, axis=1) # shape: [N, (13*13+26*26+52*52)*3, class_num]、[N, box_num, class_num] probs = tf.concat(probs_list, axis=1) # 接下来处理boxes，我们需要将存储格式为中心加尺寸的box数据变换成左上角和右下角的坐标。 center_x, center_y, width, height = tf.split(boxes, [1, 1, 1, 1], axis=-1) x_min = center_x - width / 2 y_min = center_y - height / 2 x_max = center_x + width / 2 y_max = center_y + height / 2 boxes = tf.concat([x_min, y_min, x_max, y_max], axis=-1) # 返回boxes，前景置信度，以及分类概率 return boxes, confs, probs def loss_layer(self, feature_map_i, y_true, anchors): ''' calc loss function from a certain scale input: feature_map_i: feature maps of a certain scale. shape: [N, 13, 13, 3*(5 + num_class)] etc. y_true: y_ture from a certain scale. shape: [N, 13, 13, 3, 5 + num_class + 1] etc. anchors: shape [9, 2] ''' # size in [h, w] format! don't get messed up! # 获取特征图的尺寸，这里的顺序是[height, width] grid_size = tf.shape(feature_map_i)[1:3] # the downscale ratio in height and weight # 计算下采样的倍数，使用的是原始图片的尺寸除以特征图的尺寸，所以顺序依然是[height, width] ratio = tf.cast(self.img_size / grid_size, tf.float32) # N: batch_size # 样本数目，或者说batch size，这里转换成了浮点数 N = tf.cast(tf.shape(feature_map_i)[0], tf.float32) # 根据特征图和每一个特征图对应的anchors计算预测的Bboxes，每一个框的概率以及每一个框属于前景的概率。 # 这里返回的第一个参数是每一张特征图上的偏移量。 # x_y_offset: [height, width, 1, 2] # pred_boxes: [N, height, width, 3, 4] # pred_conf_logits: [N, height, width, 3, 1] # pred_prob_logits: [N, height, width, 3, 80(num_class)] x_y_offset, pred_boxes, pred_conf_logits, pred_prob_logits = self.reorg_layer(feature_map_i, anchors) ########### # get mask ########### # shape: take 416x416 input image and 13*13 feature_map for example: # [N, 13, 13, 3, 1] # y true的最后一维的格式是[4, 1, 80, 1],分别表示4位坐标， 1位前景标志位，80个分类标记，1位mix up标记位 # y_true的最后一个维度的4号位（由0开始计数）上存储的是当前位置是否是一个有效的前景. # 如果某一个目标的中心落入框中，则是一个有效的前景，当前位是1，否则当前位置是0. # 以13 * 13的特征图为例，object mask的shape是[N, 13, 13, 3, 1] ([N, height, width, 3, 1]). object_mask = y_true[..., 4:5] # shape: [N, 13, 13, 3, 4] &amp; [N, 13, 13, 3] ==&gt; [V, 4] # V: num of true gt box # 根据上面计算出来的有效前景框，提取有效的ground truth前景框的坐标， # valid true boxes的shape：[V, 4]， 这里的V表示的是有效的ground truth前景框的数目。 valid_true_boxes = tf.boolean_mask(y_true[..., 0:4], tf.cast(object_mask[..., 0], 'bool')) # shape: [V, 2] # 将gt目标框的中心和高度宽度分离成两个矩阵，每个矩阵的shape都是[V, 2] valid_true_box_xy = valid_true_boxes[:, 0:2] valid_true_box_wh = valid_true_boxes[:, 2:4] # shape: [N, 13, 13, 3, 2] # 同样，我们将特征图预测的每个位置的目标框的中心坐标和高度宽度提取出来。 # pred boxes的最后一个维度是[2, 2, 1, 80, 1], # 分别表示预测的边界框的中心位置（2），预测的边界框的高度宽度（2），预测的边界框的前景置信度（1），分类置信度（80），mixup权重（1） pred_box_xy = pred_boxes[..., 0:2] pred_box_wh = pred_boxes[..., 2:4] # calc iou # shape: [N, 13, 13, 3, V] # 计算在每个位置上，每个预测的目标框和V个gt目标框之间的iou，返回相对应的矩阵。 iou = self.broadcast_iou(valid_true_box_xy, valid_true_box_wh, pred_box_xy, pred_box_wh) # shape: [N, 13, 13, 3] # 这一步相当于是为每一个预测的目标框匹配一个最佳的iou。 # 当然有些预测的目标框是不和任何的gt目标框相交的，此时它的最佳匹配的iou就是0. best_iou = tf.reduce_max(iou, axis=-1) # get_ignore_mask # 计算出那些和任何一个gt目标边界框的iou都小于0.5的预测目标框的标记。 # 虽然某些框和目标有一定的重叠，但是重叠部分不是很大，我们忽略掉这些框 # shape：[N, 13, 13, 3] ignore_mask = tf.cast(best_iou &lt; 0.5, tf.float32) # shape: [N, 13, 13, 3, 1] # 扩展出最后一个维度，这个ignore mask后面计算损失会用到 ignore_mask = tf.expand_dims(ignore_mask, -1) # get xy coordinates in one cell from the feature_map # numerical range: 0 ~ 1 # shape: [N, 13, 13, 3, 2] # 计算gt目标框和预测的目标框相对于网格坐标的偏移量。 true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset pred_xy = pred_box_xy / ratio[::-1] - x_y_offset # get_tw_th # numerical range: 0 ~ 1 # shape: [N, 13, 13, 3, 2] # 计算gt目标框和预测的目标框相对于anchors的大小缩放量 true_tw_th = y_true[..., 2:4] / anchors pred_tw_th = pred_box_wh / anchors # for numerical stability # 为了保证数据的稳定性，因为log(0)会趋向于负无穷大，因此将0设置为1，log之后就会变成0，可以看作不影响。 true_tw_th = tf.where(condition=tf.equal(true_tw_th, 0), x=tf.ones_like(true_tw_th), y=true_tw_th) pred_tw_th = tf.where(condition=tf.equal(pred_tw_th, 0), x=tf.ones_like(pred_tw_th), y=pred_tw_th) # 取对数，这里使用了范围的限制，小于1e-9的会强制变成1e-9，大于1e9的数据会变成1e9。 # shape: [N, 13, 13, 3, 2] true_tw_th = tf.log(tf.clip_by_value(true_tw_th, 1e-9, 1e9)) pred_tw_th = tf.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e9)) # box size punishment: # box with smaller area has bigger weight. This is taken from the yolo darknet C source code. # shape: [N, 13, 13, 3, 1] # 对于目标框尺寸的惩罚，尺寸较小的框具有较大的权重。 box_loss_scale = 2. - (y_true[..., 2:3] / tf.cast(self.img_size[1], tf.float32)) * ( y_true[..., 3:4] / tf.cast(self.img_size[0], tf.float32)) ############ # loss_part ############ # mix_up weight # [N, 13, 13, 3, 1] # mix up 权重 mix_w = y_true[..., -1:] # shape: [N, 13, 13, 3, 1] # 这里计算目标框的中心偏移的损失和高度宽度的损失，这里使用了均方和的方式计算。 # 从式子中可以看出，我们关注的只有object mask为1的目标，即有效的目标框，其他的目标框就被忽略了。 xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale * mix_w) / N wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale * mix_w) / N # shape: [N, 13, 13, 3, 1] # 前景的正样本mask，这里直接使用了object mask，因为这一部分肯定是正确的前景 conf_pos_mask = object_mask # 前景的负样本mask # 这里的采样法是没有任何一个gt目标框的中心落入框中，并且和任何一个gt目标框的iou都小于0.5的框作为前景采样的负样本。 # 这里的iou控制就是使用的ignore mask conf_neg_mask = (1 - object_mask) * ignore_mask # 使用交叉熵公式计算最后的损失，唯一的区别就是采样的方式，一个是正样本采样，一个是负样本采样 conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf_logits) conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf_logits) # TODO: may need to balance the pos-neg by multiplying some weights # 二者相加就是最后的前景分类的损失 conf_loss = conf_loss_pos + conf_loss_neg # 是否使用focal loss，默认为False if self.use_focal_loss: alpha = 1.0 gamma = 2.0 # TODO: alpha should be a mask array if needed # Focal loss的计算，这不是YOLO的中点，在此省略 focal_mask = alpha * tf.pow(tf.abs(object_mask - tf.sigmoid(pred_conf_logits)), gamma) conf_loss *= focal_mask # 将结果和mis up权重相乘，并取均值作为最后的损失标量 conf_loss = tf.reduce_sum(conf_loss * mix_w) / N # shape: [N, 13, 13, 3, 1] # whether to use label smooth # 是否使用label smooth，默认为False if self.use_label_smooth: delta = 0.01 label_target = (1 - delta) * y_true[..., 5:-1] + delta * 1. / self.class_num else: label_target = y_true[..., 5:-1] # 分类损失，这里仍然使用的是交叉熵损失。这里还是只对有效的前景框计算损失。最后仍然要和mix up权重相乘 class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_target, logits=pred_prob_logits) * mix_w # 取均值作为最后的分类损失的标量 class_loss = tf.reduce_sum(class_loss) / N # 返回最后的所有损失 return xy_loss, wh_loss, conf_loss, class_loss def compute_loss(self, y_pred, y_true): ''' param: y_pred: returned feature_map list by `forward` function: [feature_map_1, feature_map_2, feature_map_3] y_true: input y_true by the tf.data pipeline ''' # 以下的四个变量分别用来保存四个方面的loss。 loss_xy, loss_wh, loss_conf, loss_class = 0., 0., 0., 0. # 对anchors进行分组，因为每一层特征图都对应三个不同尺度的anchors。 anchor_group = [self.anchors[6:9], self.anchors[3:6], self.anchors[0:3]] # 对每一张特征图和其对应的真实值以及其对应的anchors计算损失。 # 一共有三张特征图，故一共存在三个不同尺度的损失。 # calc loss in 3 scales for i in range(len(y_pred)): # 分别计算损失 result = self.loss_layer(y_pred[i], y_true[i], anchor_group[i]) loss_xy += result[0] loss_wh += result[1] loss_conf += result[2] loss_class += result[3] total_loss = loss_xy + loss_wh + loss_conf + loss_class return [total_loss, loss_xy, loss_wh, loss_conf, loss_class] def broadcast_iou(self, true_box_xy, true_box_wh, pred_box_xy, pred_box_wh): ''' maintain an efficient way to calculate the ios matrix between ground truth true boxes and the predicted boxes note: here we only care about the size match ''' # shape: # true_box_??: [V, 2] # pred_box_??: [N, 13, 13, 3, 2] # shape: [N, 13, 13, 3, 1, 2] pred_box_xy = tf.expand_dims(pred_box_xy, -2) pred_box_wh = tf.expand_dims(pred_box_wh, -2) # shape: [1, V, 2] true_box_xy = tf.expand_dims(true_box_xy, 0) true_box_wh = tf.expand_dims(true_box_wh, 0) # [N, 13, 13, 3, 1, 2] &amp; [1, V, 2] ==&gt; [N, 13, 13, 3, V, 2] intersect_mins = tf.maximum(pred_box_xy - pred_box_wh / 2., true_box_xy - true_box_wh / 2.) intersect_maxs = tf.minimum(pred_box_xy + pred_box_wh / 2., true_box_xy + true_box_wh / 2.) intersect_wh = tf.maximum(intersect_maxs - intersect_mins, 0.) # shape: [N, 13, 13, 3, V] intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1] # shape: [N, 13, 13, 3, 1] pred_box_area = pred_box_wh[..., 0] * pred_box_wh[..., 1] # shape: [1, V] true_box_area = true_box_wh[..., 0] * true_box_wh[..., 1] # [N, 13, 13, 3, V] iou = intersect_area / (pred_box_area + true_box_area - intersect_area + 1e-10) return iou","link":"/2019/05/22/Note11-YOLOv3-part03/"},{"title":"步长stride为1的二维卷积方法的反向传播算法","text":"前言 近年来，深度学习的快速发展带来了一系列喜人的成果，不管是在图像领域还是在NLP领域，深度学习都显示了其极其强大的能力。而深度学习之所以可以进行大规模的参数运算，反向传播算法功不可没，可以说，没有反向传播算法，深度学习就不可能得以快速发展，因此在此之前，有必要了解一下反向传播算法的具体原理和公式推导。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，不涉及十分复杂的公式推导，争取可以以一种简单的方式来理解卷积的反向传播。希望可以很好的帮助理解反向传播算法。 在之前曾经见到的说明过全连接层的反向传播，因此，这一次主要是专注于卷积层的反向传播。 需要注意的是，在本文中，所有的正向传播过程中，卷积的步长stride均固定为1。 一、参数设置 在前面的全连接层的反向传播算法的推导中，其实可以发现，反向传播算法的主要核心功能就是两个，一个是进行误差的向前传播，一个是进行参数的更新，当解决了这两个问题之后，某一个特定操作的反向传播算法就得到了解决。因此，我们先从一个简单具体的实例入手。 由于卷积往往可以看作一个二维平面上的操作，那么我们就先设定我们对一个二维数据矩阵进行卷积，卷积核则也是一个二维矩阵，步长参数我们首先设置为1，在以后的说明中，步长可以设定为其他的数值。按照TensorFlow定义的卷积格式，这里的padding我们默认均为VALID，事实上，如果设置为SAME，也可以通过填补0的方式改变成VALID。 这里我们设置我们的数据矩阵（记作\\(x\\)）大小为5x5，卷积核（记作\\(k\\)）大小为3x3，由于步长是1，因此，卷积之后获得的结果是一个3x3大小的数据矩阵（不妨我们记作\\(u\\)）。偏置项我们记为\\(b\\)，将和卷积之后的矩阵进行相加。 我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为5x5 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 始终为1 padding VALID 偏置项\\(b\\) 一个浮点数 我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} &amp; u_{1, 3}\\\\ u_{2, 1} &amp; u_{2, 2} &amp; u_{2, 3}\\\\ u_{3, 1} &amp; u_{3, 2} &amp; u_{3, 3}\\\\ \\end{bmatrix} \\] 我们将结果\\(u\\)继续展开，可以得到下面的庞大的矩阵： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} &amp; u_{1, 3}\\\\ u_{2, 1} &amp; u_{2, 2} &amp; u_{2, 3}\\\\ u_{3, 1} &amp; u_{3, 2} &amp; u_{3, 3}\\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 2}k_{1, 1} + x_{1, 3}k_{1, 2} +x_{1, 4}k_{1, 3} + \\\\ x_{2, 2}k_{2, 1} + x_{2, 3}k_{2, 2} +x_{2, 4}k_{2, 3} + \\\\ x_{3, 2}k_{3, 1} + x_{3, 3}k_{3, 2} +x_{3, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 3}k_{1, 1} + x_{1, 4}k_{1, 2} +x_{1, 5}k_{1, 3} + \\\\ x_{2, 3}k_{2, 1} + x_{2, 4}k_{2, 2} +x_{2, 5}k_{2, 3} + \\\\ x_{3, 3}k_{3, 1} + x_{3, 4}k_{3, 2} +x_{3, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{2, 1}k_{1, 1} + x_{2, 2}k_{1, 2} +x_{2, 3}k_{1, 3} + \\\\ x_{3, 1}k_{2, 1} + x_{3, 2}k_{2, 2} +x_{3, 3}k_{2, 3} + \\\\ x_{4, 1}k_{3, 1} + x_{4, 2}k_{3, 2} +x_{4, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{2, 2}k_{1, 1} + x_{2, 3}k_{1, 2} +x_{2, 4}k_{1, 3} + \\\\ x_{3, 2}k_{2, 1} + x_{3, 3}k_{2, 2} +x_{3, 4}k_{2, 3} + \\\\ x_{4, 2}k_{3, 1} + x_{4, 3}k_{3, 2} +x_{4, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{2, 3}k_{1, 1} + x_{2, 4}k_{1, 2} +x_{2, 5}k_{1, 3} + \\\\ x_{3, 3}k_{2, 1} + x_{3, 4}k_{2, 2} +x_{3, 5}k_{2, 3} + \\\\ x_{4, 3}k_{3, 1} + x_{4, 4}k_{3, 2} +x_{4, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{3, 1}k_{1, 1} + x_{3, 2}k_{1, 2} +x_{3, 3}k_{1, 3} + \\\\ x_{4, 1}k_{2, 1} + x_{4, 2}k_{2, 2} +x_{4, 3}k_{2, 3} + \\\\ x_{5, 1}k_{3, 1} + x_{5, 2}k_{3, 2} +x_{5, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 2}k_{1, 1} + x_{3, 3}k_{1, 2} +x_{3, 4}k_{1, 3} + \\\\ x_{4, 2}k_{2, 1} + x_{4, 3}k_{2, 2} +x_{4, 4}k_{2, 3} + \\\\ x_{5, 2}k_{3, 1} + x_{5, 3}k_{3, 2} +x_{5, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 3}k_{1, 1} + x_{3, 4}k_{1, 2} +x_{3, 5}k_{1, 3} + \\\\ x_{4, 3}k_{2, 1} + x_{4, 4}k_{2, 2} +x_{4, 5}k_{2, 3} + \\\\ x_{5, 3}k_{3, 1} + x_{5, 4}k_{3, 2} +x_{5, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差前向传递 在前面已经完整的表示出了卷积的所有操作，下面我们来进行误差传递。 我们对上面的所有的输入进行求解偏导数的操作，我们可以得到下面的一张表格，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。于是，表格如下： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{1, 3}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\partial u_{2, 3}\\) \\(\\partial u_{3, 1}\\) \\(\\partial u_{3, 2}\\) \\(\\partial u_{3, 3}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 0 0 \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 0 \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 \\(\\partial x_{1, 4}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) 0 0 0 0 0 0 \\(\\partial x_{1, 5}\\) 0 0 \\(k_{1, 3}\\) 0 0 0 0 0 0 \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 \\(k_{1, 1}\\) 0 0 0 0 0 \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\partial x_{2, 4}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\partial x_{2, 5}\\) 0 0 \\(k_{2, 3}\\) 0 0 \\(k_{1, 3}\\) 0 0 0 \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 0 \\(k_{2, 1}\\) 0 0 \\(k_{1, 1}\\) 0 0 \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) \\(\\partial x_{3, 4}\\) 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(\\partial x_{3, 5}\\) 0 0 \\(k_{3, 3}\\) 0 0 \\(k_{2, 3}\\) 0 0 \\(k_{1, 3}\\) \\(\\partial x_{4, 1}\\) 0 0 0 \\(k_{3, 1}\\) 0 0 \\(k_{2, 1}\\) 0 0 \\(\\partial x_{4, 2}\\) 0 0 0 \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(\\partial x_{4, 3}\\) 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(\\partial x_{4, 4}\\) 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(\\partial x_{4, 5}\\) 0 0 0 0 0 \\(k_{3, 3}\\) 0 0 \\(k_{2, 3}\\) \\(\\partial x_{5, 1}\\) 0 0 0 0 0 0 \\(k_{3, 1}\\) 0 0 \\(\\partial x_{5, 2}\\) 0 0 0 0 0 0 \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(\\partial x_{5, 3}\\) 0 0 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(\\partial x_{5, 4}\\) 0 0 0 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(\\partial x_{5, 5}\\) 0 0 0 0 0 0 0 0 \\(k_{3, 3}\\) 可以看出，数据都是很规律的进行着重复。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。那么，根据求导的链式法则，我们有： \\[ \\frac{\\partial L}{\\partial x_{p, k}} = \\sum^{3}_{i = 1} \\sum^{3}_{j = 1} \\frac{\\partial L}{\\partial u_{i, j}} \\cdot \\frac{\\partial u_{i, j}}{\\partial x_{p, k}} = \\sum^{3}_{i = 1} \\sum^{3}_{j = 1} \\delta_{i, j} \\cdot \\frac{\\partial u_{i, j}}{\\partial x_{p, k}} \\] 根据这个公式，我们可以有： \\[ \\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} \\cdot k_{1, 2} + \\delta_{1, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} \\cdot k_{1, 3} + \\delta_{1, 2} \\cdot k_{1, 2} + \\delta_{1, 3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 4}} = \\delta_{1, 2} \\cdot k_{1, 3} + \\delta_{1, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 5}} = \\delta_{1, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} \\cdot k_{2, 2} + \\delta_{1, 2} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1,2}+ \\delta_{2, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} \\cdot k_{2, 3} + \\delta_{1, 2} \\cdot k_{2, 2} + \\delta_{1, 3} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1,3}+ \\delta_{2,2} \\cdot k_{1, 2} +\\delta_{2,3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 4}} = \\delta_{1, 2} \\cdot k_{2, 3} + \\delta_{1, 3} \\cdot k_{2, 2} + \\delta_{2, 2} \\cdot k_{1,3}+ \\delta_{2, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 5}} = \\delta_{1, 3} \\cdot k_{2, 3} + \\delta_{2, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1} \\cdot k_{3, 2} + \\delta_{1, 2} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 2} + \\delta_{2, 2} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 2} + \\delta_{3, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1} \\cdot k_{3, 3} + \\delta_{1, 2} \\cdot k_{3, 2} + \\delta_{1, 3} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 3} + \\delta_{2, 2} \\cdot k_{2, 2} + \\delta_{2, 3} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 3} + \\delta_{3, 2} \\cdot k_{1, 2} + \\delta_{3, 3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 4}} = \\delta_{1, 2} \\cdot k_{3, 3} + \\delta_{1, 3} \\cdot k_{3, 2} + \\delta_{2, 2} \\cdot k_{2, 3} + \\delta_{2, 3} \\cdot k_{2, 2} + \\delta_{3, 2} \\cdot k_{1, 3} + \\delta_{3, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 5}} = \\delta_{1, 3} \\cdot k_{3, 3} + \\delta_{2, 3} \\cdot k_{2, 3} + \\delta_{3, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 1}} = \\delta_{2, 1} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 2}} = \\delta_{2, 1} \\cdot k_{3, 2} + \\delta_{2, 2} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 2} + \\delta_{3, 2} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 3}} = \\delta_{2, 1} \\cdot k_{3, 3} + \\delta_{2, 2} \\cdot k_{3, 2} + \\delta_{2, 3} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 3} + \\delta_{3, 2} \\cdot k_{2, 2} + \\delta_{3, 3} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 4}} = \\delta_{2, 2} \\cdot k_{3, 3} + \\delta_{2, 3} \\cdot k_{3, 2} + \\delta_{3, 2} \\cdot k_{2, 3} + \\delta_{3, 3} \\cdot k_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 5}} = \\delta_{2, 3} \\cdot k_{3, 3} + \\delta_{3, 3} \\cdot k_{2, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 1}} = \\delta_{3, 1} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 2}} = \\delta_{3, 1} \\cdot k_{3, 2} + \\delta_{3, 2} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 3}} = \\delta_{3, 1} \\cdot k_{3, 3} +\\delta_{3, 2} \\cdot k_{3, 2} + \\delta_{3, 3} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 4}} = \\delta_{3, 2} \\cdot k_{3, 3} + \\delta_{3, 3} \\cdot k_{3, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 5}} = \\delta_{3, 3} \\cdot k_{3, 3} \\] 以上的式子虽然多，烦，一不小心就容易出错，但是每一个式子都是很简单的相乘相加，因此，我们考虑使用向量化或者矩阵化的表达方式。 为了更好的进行矩阵化表达，我们将\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)单独拿出来看，我们发现，这个式子可以变成两个相同的矩阵进行卷积（由于使用的是padding为VALID的模式，因此，在这种情况下，步长stride信息可有可无。），即： \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 进一步，我们发现，以上所有的式子的构成元素都包含在上面的两个矩阵中。 我们记右侧的全部由卷积核元素构成的矩阵为\\(k'\\) 下面的一个步骤需要一点观察技巧了，如果在\\(\\delta\\)矩阵的上下左右同时填上两层0，变成如下的形式： \\[ \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;\\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ \\end{bmatrix} \\] 在此基础上，我们利用矩阵\\(k'\\)对上式进行卷积，该卷积的步长stride为1，可以得到一个和原始的输入矩阵相同大小的矩阵，不妨记该矩阵作\\(x’\\)。 接着，我们对之前求得的25个式子按照对应的顺序进行排列，记作\\(x''\\)，于是有： \\[ x'' = \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} \\] 经过计算，我们可以发现，\\(x'\\)和\\(x''\\)正好相等。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;\\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 在这个卷积操作中，步长stride为1。 所以我们发现，在卷积操作中误差的传递主要是利用该卷积的卷积核（经过一定的变换）对传递而来的误差进行卷积来完成的。所以，我们要解决的问题又两个，一个是卷积核的变换是什么样子的，另一个就是需要在传递来的误差上下左右填补多少0。 卷积核的变换 在前面，我们发现误差传递的时候使用的卷积核和正向传播时使用的略有不同，事实上，在误差传递的时候，我们使用的卷积核是正向传播时使用的卷积核的中心对称矩阵，抑或是将正向传播使用的矩阵旋转180°之后就得到了误差传递时使用的矩阵。在这里，并不需要严格证明这一结果，只需要知道需要这么做即可。 填补0 另一个问题是我们需要在传递过来的误差矩阵周围填补多少0。我们在这里假设输入矩阵是一个正方形矩阵，卷积核也是一个正方形矩阵，输入矩阵的长宽均为\\(n\\)，卷积核的长宽均为\\(k\\)，步长为1，则输出的矩阵长宽为\\(m = n - ( k - 1)\\)。假设经过填补0之后的误差矩阵长宽均为\\(x\\)，因为我们需要对卷积核进行旋转180°，所以卷积核长宽保持不变，所以有： \\[ x - (k - 1) = n \\] 又有： \\[ \\because m = n - (k - 1) \\\\ \\therefore n = m + (k - 1) \\\\ \\therefore x - (k - 1) = m + (k - 1) \\\\ \\therefore x = m + 2 * (k - 1) \\] 因此，上下左右需要填补\\(k - 1\\)层0。 在这里只讨论了正方形的输入矩阵和正方形的卷积核，但是这一结论很容易推广到任意尺寸的输入矩阵和卷积核，这里就不再赘述。 至此，我们就解决了在步长stride为1的卷积过程中的误差传递的问题。下面就是解决参数更新的问题了。 三、参数更新 和误差传递类似，我们需要对每一个可以更新的参数求解偏导数，和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， 即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以有下面的式子：这里以求解\\(\\frac{\\partial L}{\\partial k_{1, 1}}\\) 为例： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial k_{1, 1}} =&amp; \\frac{\\partial L}{\\partial u_{1, 1}} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 2}} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 3}} \\frac{\\partial u_{1, 3}}{k_{1, 1}} + \\\\ &amp;\\frac{\\partial L}{\\partial u_{2, 1}} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 2}} \\frac{\\partial u_{2, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 3}} \\frac{\\partial u_{2, 3}}{k_{1, 1}} + \\\\ &amp;\\frac{\\partial L}{\\partial u_{3, 1}} \\frac{\\partial u_{3, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{3, 2}} \\frac{\\partial u_{3, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{3, 3}} \\frac{\\partial u_{3, 3}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\delta_{1, 2} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\delta_{1, 3} \\frac{\\partial u_{1, 3}}{k_{1, 1}} + \\\\ &amp;\\delta_{2, 1} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\delta_{2, 2} \\frac{\\partial u_{2, 2}}{k_{1, 1}} + \\delta_{2, 3} \\frac{\\partial u_{2, 3}}{k_{1, 1}} + \\\\ &amp;\\delta_{3, 1} \\frac{\\partial u_{3, 1}}{k_{1, 1}} + \\delta_{3, 2} \\frac{\\partial u_{3, 2}}{k_{1, 1}} + \\delta_{3, 3} \\frac{\\partial u_{3, 3}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 2} + \\delta_{1, 3} x_{1, 3} + \\delta_{2, 1} x_{2, 1} + \\delta_{2, 2} x_{2, 2} + \\delta_{2, 3} x_{2, 3} + \\delta_{3, 1} x_{3, 1} + \\delta_{3, 2} x_{3, 2} + \\delta_{3, 3} x_{3, 3} \\end{aligned} \\] 类似地，我们可以求出剩下的所有的偏导数，这里我们汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 2} + \\delta_{1, 3} x_{1, 3} + \\delta_{2, 1} x_{2, 1} + \\delta_{2, 2} x_{2, 2} + \\delta_{2, 3} x_{2, 3} + \\delta_{3, 1} x_{3, 1} + \\delta_{3, 2} x_{3, 2} + \\delta_{3, 3} x_{3, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = \\delta_{1, 1} x_{1, 2} + \\delta_{1, 2} x_{1, 3} + \\delta_{1, 3} x_{1, 4} + \\delta_{2, 1} x_{2, 2} + \\delta_{2, 2} x_{2, 3} + \\delta_{2, 3} x_{2, 4} + \\delta_{3, 1} x_{3, 2} + \\delta_{3, 2} x_{3, 3} + \\delta_{3, 3} x_{3, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = \\delta_{1, 1} x_{1, 3} + \\delta_{1, 2} x_{1, 4} + \\delta_{1, 3} x_{1, 5} + \\delta_{2, 1} x_{2, 3} + \\delta_{2, 2} x_{2, 4} + \\delta_{2, 3} x_{2, 5} + \\delta_{3, 1} x_{3, 3} + \\delta_{3, 2} x_{3, 4} + \\delta_{3, 3} x_{3, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = \\delta_{1, 1} x_{2, 1} + \\delta_{1, 2} x_{2, 2} + \\delta_{1, 3} x_{2, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 2} + \\delta_{2, 3} x_{3, 3} + \\delta_{3, 1} x_{4, 1} + \\delta_{3, 2} x_{4, 2} + \\delta_{3, 3} x_{4, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = \\delta_{1, 1} x_{2, 2} + \\delta_{1, 2} x_{2, 3} + \\delta_{1, 3} x_{2, 4} + \\delta_{2, 1} x_{3, 2} + \\delta_{2, 2} x_{3, 3} + \\delta_{2, 3} x_{3, 4} + \\delta_{3, 1} x_{4, 2} + \\delta_{3, 2} x_{4, 3} + \\delta_{3, 3} x_{4, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = \\delta_{1, 1} x_{2, 3} + \\delta_{1, 2} x_{2, 4} + \\delta_{1, 3} x_{2, 5} + \\delta_{2, 1} x_{3, 3} + \\delta_{2, 2} x_{3, 4} + \\delta_{2, 3} x_{3, 5} + \\delta_{3, 1} x_{4, 3} + \\delta_{3, 2} x_{4, 4} + \\delta_{3, 3} x_{4, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = \\delta_{1, 1} x_{3, 1} + \\delta_{1, 2} x_{3, 2} + \\delta_{1, 3} x_{3, 3} + \\delta_{2, 1} x_{4, 1} + \\delta_{2, 2} x_{4, 2} + \\delta_{2, 3} x_{4, 3} + \\delta_{3, 1} x_{5, 1} + \\delta_{3, 2} x_{5, 2} + \\delta_{3, 3} x_{5, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = \\delta_{1, 1} x_{3, 2} + \\delta_{1, 2} x_{3, 3} + \\delta_{1, 3} x_{3, 4} + \\delta_{2, 1} x_{4, 2} + \\delta_{2, 2} x_{4, 3} + \\delta_{2, 3} x_{4, 4} + \\delta_{3, 1} x_{5, 2} + \\delta_{3, 2} x_{5, 3} + \\delta_{3, 3} x_{5, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = \\delta_{1, 1} x_{3, 3} + \\delta_{1, 2} x_{3, 4} + \\delta_{1, 3} x_{3, 5} + \\delta_{2, 1} x_{4, 3} + \\delta_{2, 2} x_{4, 4} + \\delta_{2, 3} x_{4, 5} + \\delta_{3, 1} x_{5, 3} + \\delta_{3, 2} x_{5, 4} + \\delta_{3, 3} x_{5, 5} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1}+ \\delta_{1, 2}+ \\delta_{1, 3}+ \\delta_{2, 1}+ \\delta_{2, 2}+ \\delta_{2, 3}+ \\delta_{3, 1}+ \\delta_{3, 2}+ \\delta_{3, 3} \\] 同样，我们将上面的偏导数信息整理一下，按照每个元素对应的位置进行排列，于是，我们有： \\[ \\frac{\\partial L}{\\partial k} = [\\frac{\\partial L}{\\partial k_{i, j}}] = \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} \\] 当我们这么整理之后，可以发现，这个矩阵可以拆解成两个矩阵的步长为1的卷积，即有： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 因此，我们可以总结出，权重的梯度就是输入矩阵和误差矩阵进行步长为1卷积产生的结果矩阵。 对于偏置项的梯度\\(\\frac{\\partial L}{\\partial b}\\)则是全部的误差矩阵的元素的和。 四、总结 我们将上面的求解过程总结如下有： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵 输入卷积核\\(k\\) 一个二维矩阵 步长\\(stride\\) 始终为1 padding VALID 偏置项\\(b\\) 一个浮点数 正向传播： 1conv(x, kernel, bias, &quot;VALID&quot;) 反向传播： 1234567891011121314conv_backward(error, x, kernel, bias): # 计算传递给下一层的误差 1.在error周围填补上合适数目的0 2.将kernel旋转180° 3.将填补上0的误差和旋转之后的kernel进行步长为1的卷积，从而得到传递给下一层的误差new_error。 # 更新参数 1.将输入矩阵x和上一层传递来的误差矩阵error进行步长为1的卷积，得到kernel的更新梯度 2.将上一层传递来的误差矩阵error所有元素求和，得到bias的更新梯度 3.kernel := kernel - 学习率 * kernel的更新梯度 4.bias := bias - 学习率 * bias的更新梯度 # 返回误差，用以传递到下一层 return new_error","link":"/2019/05/24/Note14-ConvBackProp-part1/"},{"title":"步长stride为s的二维卷积方法的反向传播算法：一个十分极端的例子","text":"前言 在前面的文章中，介绍了二维平面上的卷积及其反向传播的算法，但是，步长为1和2毕竟都是两个比较小的数字，如果换成更大的数字，反向传播的方式是不是还适合呢？所以，我们考虑下面这个十分极端的例子，来验证反向传播算法的有效性。 一、参数设置 在之前的参数设置中，我们使用的输入矩阵都是5x5，在这篇文章中，我们使用10x10大小的矩阵，在卷积核方面，我们依然使用3x3大小的卷积核，步长stride方面，我们使用一个很大的数字7，padding方式依然设置为VALID。 因此，我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为10x10 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 设置为7 padding VALID 偏置项\\(b\\) 一个浮点数 和前面一样，我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为（需要注意的是这里步长选取为7）： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} &amp; x_{1, 6} &amp; x_{1, 7} &amp; x_{1, 8} &amp;x_{1, 9} &amp;x_{1, 10} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} &amp; x_{2, 6} &amp; x_{2, 7} &amp; x_{2, 8} &amp;x_{2, 9} &amp;x_{2, 10} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} &amp; x_{3, 6} &amp; x_{3, 7} &amp; x_{3, 8} &amp;x_{3, 9} &amp;x_{3, 10} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} &amp; x_{4, 6} &amp; x_{4, 7} &amp; x_{4, 8} &amp;x_{4, 9} &amp;x_{4, 10} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} &amp; x_{5, 6} &amp; x_{5, 7} &amp; x_{5, 8} &amp;x_{5, 9} &amp;x_{5, 10} \\\\ x_{6, 1} &amp; x_{6, 2} &amp; x_{6, 3} &amp;x_{6, 4} &amp;x_{6, 5} &amp; x_{6, 6} &amp; x_{6, 7} &amp; x_{6, 8} &amp;x_{6, 9} &amp;x_{6, 10} \\\\ x_{7, 1} &amp; x_{7, 2} &amp; x_{7, 3} &amp;x_{7, 4} &amp;x_{7, 5} &amp; x_{7, 6} &amp; x_{7, 7} &amp; x_{7, 8} &amp;x_{7, 9} &amp;x_{7, 10} \\\\ x_{8, 1} &amp; x_{8, 2} &amp; x_{8, 3} &amp;x_{8, 4} &amp;x_{8, 5} &amp; x_{8, 6} &amp; x_{8, 7} &amp; x_{8, 8} &amp;x_{8, 9} &amp;x_{8, 10} \\\\ x_{9, 1} &amp; x_{9, 2} &amp; x_{9, 3} &amp;x_{9, 4} &amp;x_{9, 5} &amp; x_{9, 6} &amp; x_{9, 7} &amp; x_{9, 8} &amp;x_{9, 9} &amp;x_{9, 10} \\\\ x_{10, 1} &amp; x_{10, 2} &amp; x_{10, 3} &amp;x_{10, 4} &amp;x_{10, 5} &amp; x_{10, 6} &amp; x_{10, 7} &amp; x_{10, 8} &amp;x_{10, 9} &amp;x_{10, 10} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} \\] 将矩阵\\(u\\)进一步展开，我们有： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 8}k_{1, 1} + x_{1, 9}k_{1, 2} +x_{1, 10}k_{1, 3} + \\\\ x_{2, 8}k_{2, 1} + x_{2, 9}k_{2, 2} +x_{2, 10}k_{2, 3} + \\\\ x_{3, 8}k_{3, 1} + x_{3, 9}k_{3, 2} +x_{3, 10}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{8, 1}k_{1, 1} + x_{8, 2}k_{1, 2} +x_{8, 3}k_{1, 3} + \\\\ x_{9, 1}k_{2, 1} + x_{9, 2}k_{2, 2} +x_{9, 3}k_{2, 3} + \\\\ x_{10, 1}k_{3, 1} + x_{10, 2}k_{3, 2} +x_{10, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{8, 8}k_{1, 1} + x_{8, 9}k_{1, 2} +x_{8, 10}k_{1, 3} + \\\\ x_{9, 8}k_{2, 1} + x_{9, 9}k_{2, 2} +x_{9, 10}k_{2, 3} + \\\\ x_{10, 8}k_{3, 1} + x_{10, 9}k_{3, 2} +x_{10, 10}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差传递 和之前一样，为了方便计算，也为了方便观察，我们计算如下的表格，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。最后一列显示的是计算出的需要传递的误差的偏导数，具体计算方法和前面一样，在这里不再赘述： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{i, j}}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} k_{1, 1}\\) \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} k_{1, 2}\\) \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} k_{1, 3}\\) \\(\\partial x_{1, 8}\\) 0 \\(k_{1, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 8}} = \\delta_{1, 2}k_{1, 1}\\) \\(\\partial x_{1, 9}\\) 0 \\(k_{1, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 9}} = \\delta_{1, 2}k_{1, 2}\\) \\(\\partial x_{1, 10}\\) 0 \\(k_{1, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 10}} = \\delta_{1, 2}k_{1, 3}\\) \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} k_{2, 1}\\) \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} k_{2, 2}\\) \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} k_{2, 3}\\) \\(\\partial x_{2, 8}\\) 0 \\(k_{2, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 8}} = \\delta_{1, 2}k_{2, 1}\\) \\(\\partial x_{2, 9}\\) 0 \\(k_{2, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 9}} = \\delta_{1, 2}k_{2, 2}\\) \\(\\partial x_{2, 10}\\) 0 \\(k_{2, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 10}} = \\delta_{1, 2}k_{2, 3}\\) \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1} k_{3, 1}\\) \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1} k_{3, 2}\\) \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1} k_{3, 3}\\) \\(\\partial x_{3, 8}\\) 0 \\(k_{3, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 8}} = \\delta_{1, 2}k_{3, 1}\\) \\(\\partial x_{3, 9}\\) 0 \\(k_{3, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 9}} = \\delta_{1, 2}k_{3, 2}\\) \\(\\partial x_{3, 10}\\) 0 \\(k_{3, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 10}} = \\delta_{1, 2}k_{3, 3}\\) \\(\\partial x_{8, 1}\\) 0 0 \\(k_{1, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 1}} = \\delta_{2, 1} k_{1, 1}\\) \\(\\partial x_{8, 2}\\) 0 0 \\(k_{1, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 2}} = \\delta_{2, 1} k_{1, 2}\\) \\(\\partial x_{8, 3}\\) 0 0 \\(k_{1, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 3}} = \\delta_{2, 1} k_{1, 3}\\) \\(\\partial x_{8, 8}\\) 0 0 0 \\(k_{1, 1}\\) \\(\\frac{\\partial L}{\\partial x_{8, 8}} = \\delta_{2, 2}k_{1, 1}\\) \\(\\partial x_{8, 9}\\) 0 0 0 \\(k_{1, 2}\\) \\(\\frac{\\partial L}{\\partial x_{8, 9}} = \\delta_{2, 2}k_{1, 2}\\) \\(\\partial x_{8, 10}\\) 0 0 0 \\(k_{1, 3}\\) \\(\\frac{\\partial L}{\\partial x_{8, 10}} = \\delta_{2, 2}k_{1, 3}\\) \\(\\partial x_{9, 1}\\) 0 0 \\(k_{2, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 1}} = \\delta_{2, 1} k_{2, 1}\\) \\(\\partial x_{9, 2}\\) 0 0 \\(k_{2, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 2}} = \\delta_{2, 1} k_{2, 2}\\) \\(\\partial x_{9, 3}\\) 0 0 \\(k_{2, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 3}} = \\delta_{2, 1} k_{2, 3}\\) \\(\\partial x_{9, 8}\\) 0 0 0 \\(k_{2, 1}\\) \\(\\frac{\\partial L}{\\partial x_{9, 8}} = \\delta_{2, 2}k_{2, 1}\\) \\(\\partial x_{9, 9}\\) 0 0 0 \\(k_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{9, 9}} = \\delta_{2, 2}k_{2, 2}\\) \\(\\partial x_{9, 10}\\) 0 0 0 \\(k_{2, 3}\\) \\(\\frac{\\partial L}{\\partial x_{9, 10}} = \\delta_{2, 2}k_{2, 3}\\) \\(\\partial x_{10, 1}\\) 0 0 \\(k_{3, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 1}} = \\delta_{2, 1} k_{3, 1}\\) \\(\\partial x_{10, 2}\\) 0 0 \\(k_{3, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 2}} = \\delta_{2, 1} k_{3, 2}\\) \\(\\partial x_{10, 3}\\) 0 0 \\(k_{3, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 3}} = \\delta_{2, 1} k_{3, 3}\\) \\(\\partial x_{10, 8}\\) 0 0 0 \\(k_{3, 1}\\) \\(\\frac{\\partial L}{\\partial x_{10, 8}} = \\delta_{2, 2}k_{3, 1}\\) \\(\\partial x_{10, 9}\\) 0 0 0 \\(k_{3, 2}\\) \\(\\frac{\\partial L}{\\partial x_{10, 9}} = \\delta_{2, 2}k_{3, 2}\\) \\(\\partial x_{10, 10}\\) 0 0 0 \\(k_{3, 3}\\) \\(\\frac{\\partial L}{\\partial x_{10, 10}} = \\delta_{2, 2}k_{3, 3}\\) \\(else\\) 0 0 0 0 0 可以看出，无论是何种卷积方式，数据都是十分有规律地进行分布。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。 根据前面的方法，我们先要求应该传递给下一层的误差。所以第一步，我们先在接受来的误差矩阵中插入合适数目的0，由于这里前向卷积采用的步长stride是7，所以接收到误差矩阵中的每个元素之间应该插入（7 - 1 = 6）个0，即： \\[ \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 接着，由于我们采用的卷积核的大小是3x3，所有，我们依然需要在上面矩阵的外围补上（3 - 1 = 2）层0，即： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] 下一步就是将正向卷积的卷积核旋转180°，即： \\[ \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 最后一步就是将上面的误差矩阵和旋转后的卷积核进行步长为1的卷积，即： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv(stride = 1)\\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\delta_{1, 1} k_{1, 1} &amp; \\delta_{1, 1} k_{1, 2} &amp; \\delta_{1, 1} k_{1, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{1, 1} &amp; \\delta_{1, 2}k_{1, 2} &amp; \\delta_{1, 2}k_{1, 3} \\\\ \\delta_{1, 1} k_{2, 1} &amp; \\delta_{1, 1} k_{2, 2} &amp; \\delta_{1, 1} k_{2, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{2, 1} &amp; \\delta_{1, 2}k_{2, 2} &amp; \\delta_{1, 2}k_{2, 3} \\\\ \\delta_{1, 1} k_{3, 1} &amp; \\delta_{1, 1} k_{3, 2} &amp; \\delta_{1, 1} k_{3, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{3, 1} &amp; \\delta_{1, 2}k_{3, 2} &amp; \\delta_{1, 2}k_{3, 3} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} k_{1, 1} &amp; \\delta_{2, 1} k_{1, 2} &amp; \\delta_{2, 1} k_{1, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{1, 1} &amp; \\delta_{2, 2}k_{1, 2} &amp; \\delta_{2, 2}k_{1, 3} \\\\ \\delta_{2, 1} k_{2, 1} &amp; \\delta_{2, 1} k_{2, 2} &amp; \\delta_{2, 1} k_{2, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{2, 1} &amp; \\delta_{2, 2}k_{2, 2} &amp; \\delta_{2, 2}k_{2, 3} \\\\ \\delta_{2, 1} k_{3, 1} &amp; \\delta_{2, 1} k_{3, 2} &amp; \\delta_{2, 1} k_{3, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{3, 1} &amp; \\delta_{2, 2}k_{3, 2} &amp; \\delta_{2, 2}k_{3, 3} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}} &amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} &amp; \\frac{\\partial L}{\\partial x_{1, 6}} &amp; \\frac{\\partial L}{\\partial x_{1, 7}} &amp; \\frac{\\partial L}{\\partial x_{1, 8}} &amp; \\frac{\\partial L}{\\partial x_{1, 9}} &amp; \\frac{\\partial L}{\\partial x_{1, 10}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}} &amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} &amp; \\frac{\\partial L}{\\partial x_{2, 6}} &amp; \\frac{\\partial L}{\\partial x_{2, 7}} &amp; \\frac{\\partial L}{\\partial x_{2, 8}} &amp; \\frac{\\partial L}{\\partial x_{2, 9}} &amp; \\frac{\\partial L}{\\partial x_{2, 10}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}} &amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} &amp; \\frac{\\partial L}{\\partial x_{3, 6}} &amp; \\frac{\\partial L}{\\partial x_{3, 7}} &amp; \\frac{\\partial L}{\\partial x_{3, 8}} &amp; \\frac{\\partial L}{\\partial x_{3, 9}} &amp; \\frac{\\partial L}{\\partial x_{3, 10}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}} &amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} &amp; \\frac{\\partial L}{\\partial x_{4, 6}} &amp; \\frac{\\partial L}{\\partial x_{4, 7}} &amp; \\frac{\\partial L}{\\partial x_{4, 8}} &amp; \\frac{\\partial L}{\\partial x_{4, 9}} &amp; \\frac{\\partial L}{\\partial x_{4, 10}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}} &amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} &amp; \\frac{\\partial L}{\\partial x_{5, 6}} &amp; \\frac{\\partial L}{\\partial x_{5, 7}} &amp; \\frac{\\partial L}{\\partial x_{5, 8}} &amp; \\frac{\\partial L}{\\partial x_{5, 9}} &amp; \\frac{\\partial L}{\\partial x_{5, 10}} \\\\ \\frac{\\partial L}{\\partial x_{6, 1}} &amp; \\frac{\\partial L}{\\partial x_{6, 2}} &amp; \\frac{\\partial L}{\\partial x_{6, 3}} &amp; \\frac{\\partial L}{\\partial x_{6, 4}} &amp; \\frac{\\partial L}{\\partial x_{6, 5}} &amp; \\frac{\\partial L}{\\partial x_{6, 6}} &amp; \\frac{\\partial L}{\\partial x_{6, 7}} &amp; \\frac{\\partial L}{\\partial x_{6, 8}} &amp; \\frac{\\partial L}{\\partial x_{6, 9}} &amp; \\frac{\\partial L}{\\partial x_{6, 10}} \\\\ \\frac{\\partial L}{\\partial x_{7, 1}} &amp; \\frac{\\partial L}{\\partial x_{7, 2}} &amp; \\frac{\\partial L}{\\partial x_{7, 3}} &amp; \\frac{\\partial L}{\\partial x_{7, 4}} &amp; \\frac{\\partial L}{\\partial x_{7, 5}} &amp; \\frac{\\partial L}{\\partial x_{7, 6}} &amp; \\frac{\\partial L}{\\partial x_{7, 7}} &amp; \\frac{\\partial L}{\\partial x_{7, 8}} &amp; \\frac{\\partial L}{\\partial x_{7, 9}} &amp; \\frac{\\partial L}{\\partial x_{7, 10}} \\\\ \\frac{\\partial L}{\\partial x_{8, 1}} &amp; \\frac{\\partial L}{\\partial x_{8, 2}} &amp; \\frac{\\partial L}{\\partial x_{8, 3}} &amp; \\frac{\\partial L}{\\partial x_{8, 4}} &amp; \\frac{\\partial L}{\\partial x_{8, 5}} &amp; \\frac{\\partial L}{\\partial x_{8, 6}} &amp; \\frac{\\partial L}{\\partial x_{8, 7}} &amp; \\frac{\\partial L}{\\partial x_{8, 8}} &amp; \\frac{\\partial L}{\\partial x_{8, 9}} &amp; \\frac{\\partial L}{\\partial x_{8, 10}} \\\\ \\frac{\\partial L}{\\partial x_{9, 1}} &amp; \\frac{\\partial L}{\\partial x_{9, 2}} &amp; \\frac{\\partial L}{\\partial x_{9, 3}} &amp; \\frac{\\partial L}{\\partial x_{9, 4}} &amp; \\frac{\\partial L}{\\partial x_{9, 5}} &amp; \\frac{\\partial L}{\\partial x_{9, 6}} &amp; \\frac{\\partial L}{\\partial x_{9, 7}} &amp; \\frac{\\partial L}{\\partial x_{9, 8}} &amp; \\frac{\\partial L}{\\partial x_{9, 9}} &amp; \\frac{\\partial L}{\\partial x_{9, 10}} \\\\ \\frac{\\partial L}{\\partial x_{10, 1}} &amp; \\frac{\\partial L}{\\partial x_{10, 2}} &amp; \\frac{\\partial L}{\\partial x_{10, 3}} &amp; \\frac{\\partial L}{\\partial x_{10, 4}} &amp; \\frac{\\partial L}{\\partial x_{10, 5}} &amp; \\frac{\\partial L}{\\partial x_{10, 6}} &amp; \\frac{\\partial L}{\\partial x_{10, 7}} &amp; \\frac{\\partial L}{\\partial x_{10, 8}} &amp; \\frac{\\partial L}{\\partial x_{10, 9}} &amp; \\frac{\\partial L}{\\partial x_{10, 10}} \\\\ \\end{bmatrix} \\] 经过上面的计算，在误差传递上，我们的算法可以正确运行，即使步长stride是一个任意的数字。接下来我们来验证更新梯度的计算。 三、更新梯度 和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以计算出所有的需要的偏导数，这里的计算过程和前面的计算过程是一样的，这里不再赘述。汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = x_{1, 1}\\delta_{1, 1} + x_{1, 8}\\delta_{1, 2} + x_{8, 1}\\delta_{2, 1} + x_{8, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = x_{1, 2}\\delta_{1, 1} + x_{1, 9}\\delta_{1, 2} + x_{8, 2}\\delta_{2, 1} + x_{8, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = x_{1, 3}\\delta_{1, 1} + x_{1, 10}\\delta_{1, 2} + x_{8, 3}\\delta_{2, 1} + x_{8, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = x_{2, 1}\\delta_{1, 1} + x_{2, 8}\\delta_{1, 2} + x_{9, 1}\\delta_{2, 1} + x_{9, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = x_{2, 2}\\delta_{1, 1} + x_{2, 9}\\delta_{1, 2} + x_{9, 2}\\delta_{2, 1} + x_{9, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = x_{2, 3}\\delta_{1, 1} + x_{2, 10}\\delta_{1, 2} + x_{9, 3}\\delta_{2, 1} + x_{9, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = x_{3, 1}\\delta_{1, 1} + x_{3, 8}\\delta_{1, 2} + x_{10, 1}\\delta_{2, 1} + x_{10, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = x_{3, 2}\\delta_{1, 1} + x_{3, 9}\\delta_{1, 2} + x_{10, 2}\\delta_{2, 1} + x_{10, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = x_{3, 3}\\delta_{1, 1} + x_{3, 10}\\delta_{1, 2} + x_{10, 3}\\delta_{2, 1} + x_{10, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1} + \\delta_{1, 2} + \\delta_{2, 1} + \\delta_{2, 2} \\] 按照之前的算法，由于正向卷积中的步长stride为7，因此，在计算更新梯度的过程中，我们依然需要在接收到的误差矩阵的每两个相邻的元素之间插入（7 - 1 = 6）个0，即： \\[ \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 接着我们拿输入矩阵\\(x\\)和上面的矩阵进行步长为1的卷积，则可以得到卷积核参数的更新梯度。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} &amp; x_{1, 6} &amp; x_{1, 7} &amp; x_{1, 8} &amp;x_{1, 9} &amp;x_{1, 10} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} &amp; x_{2, 6} &amp; x_{2, 7} &amp; x_{2, 8} &amp;x_{2, 9} &amp;x_{2, 10} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} &amp; x_{3, 6} &amp; x_{3, 7} &amp; x_{3, 8} &amp;x_{3, 9} &amp;x_{3, 10} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} &amp; x_{4, 6} &amp; x_{4, 7} &amp; x_{4, 8} &amp;x_{4, 9} &amp;x_{4, 10} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} &amp; x_{5, 6} &amp; x_{5, 7} &amp; x_{5, 8} &amp;x_{5, 9} &amp;x_{5, 10} \\\\ x_{6, 1} &amp; x_{6, 2} &amp; x_{6, 3} &amp;x_{6, 4} &amp;x_{6, 5} &amp; x_{6, 6} &amp; x_{6, 7} &amp; x_{6, 8} &amp;x_{6, 9} &amp;x_{6, 10} \\\\ x_{7, 1} &amp; x_{7, 2} &amp; x_{7, 3} &amp;x_{7, 4} &amp;x_{7, 5} &amp; x_{7, 6} &amp; x_{7, 7} &amp; x_{7, 8} &amp;x_{7, 9} &amp;x_{7, 10} \\\\ x_{8, 1} &amp; x_{8, 2} &amp; x_{8, 3} &amp;x_{8, 4} &amp;x_{8, 5} &amp; x_{8, 6} &amp; x_{8, 7} &amp; x_{8, 8} &amp;x_{8, 9} &amp;x_{8, 10} \\\\ x_{9, 1} &amp; x_{9, 2} &amp; x_{9, 3} &amp;x_{9, 4} &amp;x_{9, 5} &amp; x_{9, 6} &amp; x_{9, 7} &amp; x_{9, 8} &amp;x_{9, 9} &amp;x_{9, 10} \\\\ x_{10, 1} &amp; x_{10, 2} &amp; x_{10, 3} &amp;x_{10, 4} &amp;x_{10, 5} &amp; x_{10, 6} &amp; x_{10, 7} &amp; x_{10, 8} &amp;x_{10, 9} &amp;x_{10, 10} \\\\ \\end{bmatrix} \\; conv(stride = 1)\\; \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 经过计算，两者的结果是相同的，这也就验证了我们的算法在一些比较极端的情况下也是正确的。 四、总结 经过一个比较极端的卷积实例的讲解，我们验证了我们算法的正确性，而下一步就是用代码实现二维平面上的卷积及其反向传播算法。","link":"/2019/05/24/Note16-ConvBackProp-part3/"},{"title":"EM算法","text":"前言 EM算法是一种可以求解含有隐变量的迭代算法，当我们在实际过程中收集数据的时候，并不一定会收集全部的有效信息。比如，当我们想统计全校学生的身高分布的时候，可以将全校学生的身高看作是一个正态分布，但是毕竟男生和女生之间身高的分布还是有些不同的，但是万一我们没有对性别信息进行统计，而只是统计了身高信息的话，求得的概率分布的参数肯定会有较大的误差，这个时候，我们就需要将每一个样本的性别分布也考虑进去，从而希望获得更准确的概率分布估计。 一、准备工作 1、极大似然估计 极大似然估计我们并不陌生，在逻辑回归的求解过程中，我们就是用了极大似然估计，现在还是简单说明一下。 假设我们现在有一个概率分布，不妨记作\\(P(x;\\theta)\\)，其中，\\(\\theta\\)是未知参数，有可能是一个数值，也有可能是多个数值组成的参数向量，\\(x\\)表示输入样本。现在我们想通过抽样的方式对参数\\(\\theta\\)进行估计。假设我们一共采集了\\(N\\)组数据，为\\(\\{x_1, x_2, \\cdots, x_N\\}\\)。那么样本的联合概率函数可以表示为关于\\(\\theta\\)的函数，即： \\[ L(\\theta) = L(x_1, x_2, \\cdots, x_N;\\theta) = \\prod_i^N P(x_i;\\theta) \\] \\(L(\\theta)\\)是参数 \\(θ\\) 的函数，随着 \\(θ\\) 在参数变化，\\(L\\)函数也在变化。而极大似然估计目的就是在样本\\(\\{x_1,...,x_N\\}\\)固定的情况下，寻找最优的 \\(θ\\) 来极大化似然函数： \\[ \\theta^{*} = \\mathop{\\arg\\max}_{\\theta}{L(\\theta)} \\] 上式在数学领域，可以看作是对 \\(θ^{*}\\)求解，求\\(L(θ)\\) 函数的极值点，导数为0处，即为 \\(θ*\\) 的点。 又因为\\(L(θ)\\) 和 \\(ln(θ)\\) 在同一个 \\(θ\\) 处取得极值，我们可以对 \\(L(θ)\\) 取对数，将连乘转化为连加(方便求导)，得到对数化似然函数： \\[ \\theta^*= \\mathop{\\arg\\max}_{\\theta}{ln\\;L(\\theta)} = \\mathop{\\arg\\max}_{\\theta} \\sum_i ln\\; P(x_i;\\theta) \\] 2、Jensen不等式 下图是一张描述Jensen不等式十分经典的图。 如果一个函数\\(f(x)\\)在其定义域内是一个连续函数，且其二阶导数恒小于等于0，我们称该函数在其定义域上是凹函数，反之，如果二阶导数恒大于等于0，我们称该函数在其定义域上是凸函数。 如果\\(f(x)\\)是一个凸函数，那么在其定义域上我们有: \\[ E(f(X)) \\geq f(E(X)) \\] 反之，如果\\(f(x)\\)是一个凹函数，在其定义域上我们有： \\[ E(f(X)) \\leq f(E(X)) \\] 其中，\\(E\\)表示对变量取期望。上面两个不等式当且仅当\\(X\\)是一个常量时可以取等号。 3、边缘分布 假设我们有两个随机变量，那么我们通过抽样，就会获得一个二维的联合概率分布\\(P(X=x_i,Y=y_j)\\)。 对每一个\\(X=x_i\\)，对其所有的\\(Y\\)进行求和操作，我们有： \\[ \\sum_{y_j}P(X=x_i, Y=y_j) = P(X=x_i) \\] 将上面的式子称之为\\(X=x_i\\)的边际分布（边缘分布）。 ​ 有了以上的一些基础准备，我们就可以来推导EM算法了。 二、EM算法 假设我们的数据集为： \\[ D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\] 其中 \\(x^{(i)}\\) 是每一个具体的输出实例，表示每一次独立实验的结果，\\(N\\)表示独立实验的次数。 我们设样本的概率分布函数为\\(P(x^{(i)};\\theta)\\)，其中\\(\\theta\\)是模型中的待估参数，可以是一个变量，也可以是多个变量所组成的参数向量。 根据极大似然估计，我们有： \\[ L(\\theta) = \\prod_{i}P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{1} \\] 两边同时取对数： \\[ ln\\;L(\\theta) = \\sum_{i} ln \\; P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{2} \\] 此时，我们可以将 \\(P(x^{(i)}; \\theta)\\)看作是关于隐变量的一个边缘分布，即（我们假设隐变量为\\(Z\\)）： \\[ ln \\; L(\\theta) = \\sum_i ln \\; \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{3} \\] 这里我们利用了边缘分布的相关等式，即： \\[ P(x^{(i)}; \\theta) = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] 在上面的式子中，\\(z\\)是一个隐藏变量，必然也会满足一个特定的概率分布，我们不妨把这个分布记作\\(Q_{i}(z^{(i)})\\)，显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1\\)。这里的下标和上标\\(i\\)表示的是第\\(i\\)个样本。故我们将上式改写成： \\[ \\begin{aligned} ln \\; L(\\theta) &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\\\ &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\end{aligned} \\tag{4} \\] 现在，我们把如下的部分单独拿出来： \\[ Q_i(z^{(i)}) = p(z^{(i)}) \\\\ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = f(z^{(i)}) \\] 很显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = \\sum_i p(z^{(i)}) = 1\\)，所以，我们可以将上式写成： \\[ ln \\; L(\\theta) = \\sum_i ln \\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)}) \\tag{5} \\] 可以看出，我们的\\(\\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)})\\)实际上实在对\\(f(z^{(i)})\\)计算期望，其中\\(p(z^{(i)})\\)是函数\\(f(z^{(i)})\\)的概率分布函数，于是，我们可以把上面的式子记作： \\[ E[f(z^{(i)})] = \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\tag{6} \\] 于是，我们的似然函数就变成了： \\[ ln \\; L(\\theta) = \\sum_i ln \\;(E[f(z^{(i)})]) = \\sum_i ln \\; (E[\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}]) \\tag{7} \\] 这个时候就是Jensen不等式出场的时候了。 我们观察到函数\\(g(x)=ln(x)\\)，它的一阶导数是\\(g'(x) = \\frac{1}{x}\\)，二阶导数是\\(g''(x) = - \\frac{1}{x^2}\\)恒小于0，因此\\(g(x) = ln(x)\\)是一个凹函数，此时，我们利用Jensen不等式处理\\(ln \\;L(\\theta)\\)，有： \\[ \\begin{aligned} H(Y|X)&amp; =\\sum_{x\\in X} p(x)H(Y|X) \\\\ &amp; =-\\sum_{x\\in X} p(x)\\sum_{y\\in Y}p(y|x)\\log p(y|x)\\\\ &amp; =-\\sum_{x\\in X} \\sum_{y\\in Y}p(y,x)\\log p(y|x) \\end{aligned} \\] 故：我们根据Jensen不等式，有了以下的一个重要的不等式关系： \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{9} \\] 需要注意的是，我们使用Jensen不等式的时候，是对\\(z^{(i)}\\)的分布使用的，而\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}))\\)是函数\\(ln \\; L(\\theta)\\)的一个下界，所以实际上，\\(ln \\; L(\\theta)\\)包含了两个参数变量，一个是\\(\\theta\\)， 一个是隐藏变量\\(z^{(i)}\\)，所以我们需要弄清楚调整\\(\\theta\\)和\\(z^{(i)}\\)的区别。 由于Jensen不等式处理的是\\(z^{(i)}\\)，因此当我们调整\\(z^{(i)}\\)的时候，我们实际上实在调整似然函数\\(ln \\; L(\\theta)\\)的下界，使得似然函数\\(ln \\; L(\\theta)\\)的下界一点一点上升，最终于此时的似然函数\\(ln \\; L(\\theta)\\)的值相等。 然后，固定\\(z^{(i)}\\)的数值，调整\\(\\theta\\)的时候，就可以将\\(z^{(i)}\\)看作是一个已知变量，这个时候就可以利用极大似然估计的方法对\\(\\theta\\)参数的值进行计算，此时会得到一个新的\\(\\theta\\)的值，不妨记作\\(\\theta'\\)。我们这个时候再根据这个新的\\(\\theta'\\)的值，重新调整\\(z^{(i)}\\)，使得函数的下界一点一点上升，达到和\\(ln \\; L(\\theta)\\)相同之后，再固定\\(z^{(i)}\\)，更新\\(\\theta\\)的值。一直重复以上过程，直到似然函数收敛到某一个极大值\\(\\theta^*\\)处。 下图是一个很经典的关于EM算法的迭代过程示意图。（图片来自网络） 在上面的求解过程中，只有当此时的函数下界等于当前\\(\\theta\\)的对数似然函数时，才能保证当我们优化了这个下界的时候，才真正优化了目标似然函数。 \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{10} \\] 在优化迭代的过程中，我们通过固定\\(\\theta\\)并调整\\(z^{(i)}\\)的可能分布，使得等式成立，即达到\\(ln \\; L(\\theta)\\)的下界。根据Jensen不等式的条件，当\\(f(x)\\)是一个凹函数的时候，有\\(f(E[X]) \\geq E[f(X)]\\)，欲使等号成立，\\(X\\)需要是一个常量。那么，在上面的式子中，我们有\\(X = \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}\\)，故此时我们需要将\\(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})\\)看作一个常数，不妨我们设这个常数为\\(C\\)，于是我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}) = C \\tag{11} \\] \\[ P(x^{(i)}, z^{(i)}; \\theta) = C Q_i(z^{(i)}) \\] \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\sum_{z^{(i)}} Q_i(z^{(i)}) \\tag{12} \\] 考虑到\\(Q_i(z^{(i)})\\)实际上是隐变量\\(z^{(i)}\\)的一个概率分布，满足： \\[ \\sum_{z^{(i)}} Q_i(z^{(i)}) = 1 ,\\quad Q_i(z^{(i)}) \\geq 0 \\] 于是，我们将\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1\\)代入到上面的式子(12)中，有： \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\tag{11} \\] 再将\\(C\\)带入到公式(11)中，我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = C = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] \\[ \\begin{aligned} Q_i(z^{(i)}) &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{\\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta)} \\\\ &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(x^{(i)};\\theta)} \\\\ &amp;= P(z^{(i)}|x^{(i)};\\theta) \\end{aligned} \\tag{12} \\] 即我们可以得到\\(Q_i(z^{(i)})\\)的值，也即我们得到\\(P(z^{(i)}|x^{(i)};\\theta)\\)的值，表示在当前的模型参数\\(\\theta\\)为定值时，在给定的\\(x^{(i)}\\)的条件下，得到\\(z^{(i)}\\)的概率大小。 至此，我们的EM算法的大部分情况进行了说明。首先，我们会对模型的参数\\(\\theta\\) 进行随机初始化，不妨记作\\(\\theta^0\\)。然后会在每一次的迭代循环中计算\\(z^{(i)}\\)的条件概率期望，这就是EM算法中的”E步“。最后再根据计算得到的概率分布，根据极大似然的方法计算在当前隐藏变量分布下的使得似然函数取得极大的\\(\\theta\\)的值，并进行更新，这就是EM算法中的”M步“。 观察到在”M步“中，我们有： \\[ ln \\; L(\\theta) = \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\] \\[ \\theta^{(j + 1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) - \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)})) \\] 观察到在上面的式子中，\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)}))\\)对于整个优化的过程来说相当于是一个常数项，因此可以省略，于是，上式可以简写成： \\[ \\begin{aligned} \\theta^{(j + 1)} &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\end{aligned} \\tag{13} \\] 公式(13)内部的\\(\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta))\\)就是EM算法的核心，我们一般将其称之为Q函数，通常记为：\\(Q(\\theta, \\theta^{(j)})\\)。 所以，我们的EM算法可以总结如下： 数据集为\\(D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\}\\) ，随机初始化模型参数\\(\\theta\\)，记作\\(\\theta^{(0)}\\)。 对每一次迭代循环，\\(j = 0, 1, 2, 3, \\cdots, M\\)，我们有： 2.1 E步（E-Step）：在当前的模型参数\\(\\theta^{(j)}\\)的条件下，计算联合分布的条件概率期望： \\[ Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\\theta^{(j)}) \\] 2.2 M步（M-Step）：在计算出条件概率分布的期望的基础上，极大化似然函数，得到新的模型参数\\(\\theta^{(j+1)}\\)的值: \\[ \\theta^{(j+1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 如果\\(\\theta^{(j+1)}\\)已经收敛，则跳出循环结束： 输出最后模型参数\\(\\theta\\)的值。 三、EM算法解决三硬币模型 三硬币模型是EM算法的一个简单使用，问题请参考《统计学习方法》一书。 假设有三枚质量分布不均匀的硬币A、B、C，这些硬币正面出现的概率分别是\\(\\pi\\)、\\(p\\)、\\(q\\)。进行如下掷硬币试验： 先掷A，如果A是正面则再掷B，如果A是反面则再掷C。对于B或C的结果，如果是正面则记为1，如果是反面则记为0。进行N次独立重复实验，得到结果。现在只能观测到结果，不能观测到掷硬币的过程，估计模型参数\\(\\theta=(\\pi,p,q)\\)。 由于实验一共进行了N次，每一次都是独立重复实验，那么我们可以将实验结果记录如下，其中每一次的实验结果是已知的： \\[ X = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\quad x^{(i)} \\in \\{0, 1\\} \\] 每次独立实验都会产生一个隐藏变量\\(z^{(i)}\\)，这个隐藏变量是无法被观测到的，我们可以将其记录如下，这个隐藏变量的记录结果是未知的： \\[ Z = \\{z^{(1)}, z^{(2)}, \\cdots, z^{(N)}\\} \\quad z^{(i)} \\in \\{0, 1\\} \\] 对于第\\(i\\)次的独立重复实验，我们有： \\[ P(x^{(i)} = 0;\\theta) = \\pi(1-p)^{1-x^{(i)}} + (1-\\pi)(1-q)^{1-x^{(i)}} \\] \\[ P(x^{(i)}=1;\\theta) = \\pi p^{x^{(i)}} + (1-\\pi)q^{1-x^{(i)}} \\] 故，综合起来看，我们有： \\[ P(x^{(i)};\\theta) = \\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}} \\] 构造极大似然函数 我们可以构造我们的极大似然函数如下： \\[ \\begin{aligned} L(\\theta) &amp;= \\prod_i P(x^{(i)};\\theta) \\\\ &amp;= \\prod_i [\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\end{aligned} \\] 两边同时取对数，有： \\[ ln \\; L(\\theta) = \\sum_i ln\\;[\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\] 构造我们的Q函数 在没有说明的情况下，我们使用下标表示第几次迭代过程，用上标表示第几个样本，\\(\\theta^{(j)}\\)的上标表示第\\(j\\)次迭代。 对于三硬币问题，我们的Q函数可以构造如下： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ \\end{aligned} \\] 故，我们需要求解\\(P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)这四个概率值。 求解极大值 \\[ \\begin{aligned} P(z^{(i)}=1|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=1;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= \\mu_j^{(i)} \\end{aligned} \\] 上式对于迭代过程来说是一个定值，我们使用符号\\(\\mu_j^{(i)}\\)来表示，上标\\((i)\\)表示的是第\\(i\\)个样本，下标\\(j\\)表示的是第\\(j\\)次迭代过程。 那么很明显，我们有： \\[ \\begin{aligned} P(z^{(i)}=0|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=0;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{(1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(j)}}}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= 1 - \\mu_j^{(i)} \\end{aligned} \\] 接着，我们计算\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)： \\[ P(x^{(i)}, z^{(i)}=1;\\theta) = \\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}} \\] \\[ P(x^{(i)}, z^{(i)}=0;\\theta)=(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}} \\] 我们将上面的计算结果都带入到Q函数中，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot ln\\;[\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}] + (1-\\mu_j^{(i)})\\cdot ln\\; [(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}]\\} \\end{aligned} \\] 下一步就是对我们需要求解的变量进行求偏导数的操作，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial \\pi} &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}}{\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}} + (1-\\mu_j^{(i)})\\cdot \\frac{-1 \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}{(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi} + (\\mu_j^{(i)}-1)\\cdot \\frac{1}{1-\\pi}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)}(\\frac{1}{\\pi} + \\frac{1}{1-\\pi}) - \\frac{1}{1-\\pi} \\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi(1-\\pi)}\\} - \\frac{N\\pi}{\\pi(1-\\pi)} \\\\ &amp;= \\frac{1}{\\pi(1-\\pi)}\\{\\sum_i \\mu_j^{(i)} - N\\pi\\} \\end{aligned} \\] 令上式为0，我们有： \\[ \\sum_i \\mu_j^{(i)} - N\\pi = 0 \\] 即： \\[ \\pi = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] 同样的道理，我们可以计算出Q函数相对于\\(p\\)的偏导数，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial p} &amp;= \\sum_i \\mu_j^{(i)} \\frac{x^{(i)} \\cdot p^{x^{(i)}-1} \\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-x^{(i)})\\cdot (1-p)^{-x^{(i)}}\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} + 0 \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\frac{\\frac{x^{(i)}}{p}\\cdot p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} \\cdot \\frac{1}{1-p}\\cdot (1-x^{(i)})\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\{\\frac{x^{(i)}}{p} + \\frac{1-x^{(i)}}{p-1}\\} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\cdot \\frac{(p-1)\\cdot x^{(i)} + p(1-x^{(i)})}{p(p-1)} \\\\ &amp;= \\frac{1}{p(p-1)} \\sum_i \\mu_j^{(i)} \\{p-x^{(i)}\\} \\\\ &amp;= \\frac{1}{p(p-1)}\\{p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)}\\} \\end{aligned} \\] 令上式等于0，我们可以得到： \\[ p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)} = 0 \\] 即： \\[ p = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] 同理，我们对\\(q\\)求偏导数，有： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial q} &amp;= \\sum_i (1-\\mu_j^{(i)})(\\frac{x^{(i)}}{q}+\\frac{1-x^{(i)}}{p-1}) \\\\ &amp;= \\sum_i (1-\\mu_j^{(i)})\\frac{q-x^{(i)}}{q(q-1)} \\\\ &amp;= \\frac{1}{q(q-1)}\\{q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} \\} \\end{aligned} \\] 令上式等于0，我们有： \\[ q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} =0 \\] 即： \\[ q = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 所以，以上就是我们解决三硬币模型的迭代公式的求解过程，公式汇总如下，这里加入了下标，表示新的一轮迭代变量： \\[ \\mu_j^{(i)} = \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\] \\[ \\pi_{j+1} = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] \\[ p_{j+1} = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] \\[ q_{j+1} = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 四、EM算法的收敛性 在之前的过程中，我们都是默认EM算法可以收敛到某一极大值附近，但是并没有给出十分严格的证明，所以，我们需要对EM的收敛性进行一定的验证。 由于我们是利用极大似然估计来估计参数的值，那么，我们只需要保证在每一次的迭代过程中，似然函数的数值都在上升即可，即下面的不等式成立： \\[ ln \\; L(\\theta^{(j+1)}) \\geq ln\\;L(\\theta^{(j)}) \\] 由于： \\[ P(x^{(i)};\\theta) = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}|x^{(i)};\\theta)} \\] 因此，两边取对数，我们有： \\[ ln \\; P(x^{(i)};\\theta) = ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta) \\] 对每一个样本进行累加，我们有： \\[ \\sum_i ln \\; P(x^{(i)};\\theta) = \\sum_i (ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta)) \\] 根据我们构造的Q函数，我们有： \\[ Q(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 另，我们可以构造如下的一个函数，记作\\(H(\\theta, \\theta^{(j)})\\)，如下： \\[ H(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\] 我们将上面的两个函数相减，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) - H(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\&amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) (ln\\;P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;\\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}| x^{(i)};\\theta)} \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;P(x^{(i)};\\theta) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) (\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) ) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) \\\\ &amp;= ln \\;L(\\theta) \\end{aligned} \\] 在上面的式子中，第三行是利用了条件概率的公式，第五行则是利用了\\(\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) = 1\\)的条件。 所以，我们构造出的两个式子，相减之后正好是我们的极大似然函数的对数。 于是，我们将\\(ln\\; L(\\theta^{(j+1)})\\)和\\(ln\\;L(\\theta^{(j)})\\)相减，我们有： \\[ \\begin{aligned} ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j+1)}, \\theta^{(j)})) - (Q(\\theta^{(j)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\\\ &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\end{aligned} \\] 对于第一个括号内部的\\(Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})\\)，由于我们是通过极大化Q函数来更新参数的数值，所以\\(Q(\\theta^{(j+1)}, \\theta^{(j)}) \\geq Q(\\theta^{(j)}, \\theta^{(j)})\\)，故这一部分一定会大于等于0，即： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] 对于第二个括号内部的数值，我们有： \\[ \\begin{aligned} H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln\\; \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} \\\\ &amp;\\leq \\sum_i ln(\\sum_{z^{(i)}} \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} P(z^{(i)}|x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i ln (\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;= \\sum_i ln(1) \\\\ &amp;= 0 \\end{aligned} \\] 在上面的第三步中，我们使用了Jensen不等式，在第五步中，我们使用了\\(\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)}) = 1\\)这一条件。 于是，我们可以有： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] \\[ H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) \\leq 0 \\] 故： \\[ (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\geq 0 \\] 即： \\[ ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) \\geq 0 \\] 所以EM算法是可以逐步收敛到某一极大值附近的。证毕。 五、EM算法的缺陷 EM算法是处理含有隐藏变量模型的重要算法，但是EM算法也有其缺陷，首先，EM算法对初始值敏感，不同的初始值可能会导致不同的结果，这是由于似然函数的性质决定的，如果一个似然函数是凹函数，那么最后会收敛到极大值附近，也就是最大值附近，但是如果函数存在多个极大值，那么算法的初始值就会影响最后的结果。","link":"/2019/05/10/Note2-EM-Algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"AdaBoost","slug":"AdaBoost","link":"/tags/AdaBoost/"},{"name":"深度学习","slug":"深度学习","link":"/tags/深度学习/"},{"name":"目标检测","slug":"目标检测","link":"/tags/目标检测/"},{"name":"YOLOv3","slug":"YOLOv3","link":"/tags/YOLOv3/"},{"name":"反向传播","slug":"反向传播","link":"/tags/反向传播/"},{"name":"支持向量机","slug":"支持向量机","link":"/tags/支持向量机/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"核函数","slug":"核函数","link":"/tags/核函数/"},{"name":"支持向量回归","slug":"支持向量回归","link":"/tags/支持向量回归/"},{"name":"SVR","slug":"SVR","link":"/tags/SVR/"},{"name":"Support Vector Regression","slug":"Support-Vector-Regression","link":"/tags/Support-Vector-Regression/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"主成分分析","slug":"主成分分析","link":"/tags/主成分分析/"},{"name":"Welcome","slug":"Welcome","link":"/tags/Welcome/"},{"name":"卷积","slug":"卷积","link":"/tags/卷积/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/逻辑回归/"},{"name":"反向传播算法","slug":"反向传播算法","link":"/tags/反向传播算法/"},{"name":"奇异值分解","slug":"奇异值分解","link":"/tags/奇异值分解/"},{"name":"SVD","slug":"SVD","link":"/tags/SVD/"},{"name":"EM算法","slug":"EM算法","link":"/tags/EM算法/"}],"categories":[{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"},{"name":"目标检测","slug":"目标检测","link":"/categories/目标检测/"},{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"}]}