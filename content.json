{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"YOLOv3：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族， 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow","link":"/2019/05/22/Article10YOLOv3-part02/"},{"title":"YOLOv3：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族， 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow ##### 二、代码和注释 title: Article11YOLOv3-part03 date: tags: ---","link":"/2019/05/22/Article11YOLOv3-part03/"},{"title":"YOLOv3：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族， 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow ##### 二、代码和注释 title: Article12YOLOv3-part04 date: tags: ---","link":"/2019/05/22/Article12YOLOv3-part04/"},{"title":"YOLOv3：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族， 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow ##### 二、代码和注释 title: Article13YOLOv3-part05 date: tags: ---","link":"/2019/05/22/Article13YOLOv3-part05/"},{"title":"Adaboost算法","text":"前言 提升方法（boosting）是一种常用的机器学习方法，应用十分广泛，而且效果非常好，近几年的很多比赛的优胜选手都或多或少使用了提升方法用以提高自己的成绩。 提升方法的本质是通过对每一个训练样本赋予一个权重，并通过改变这些样本的权重，来学习多个分类器，并按照一定的算法将这些分类器组合在一起，通常是线性组合，因为单个分类器往往效果有限，因此组合多个分类器往往会提高模型的性能。 一、提升方法简介 提升方法（boosting）实际上是集成学习方法的一种，其基于这样的一种朴素思想：对于一个复杂的任务来说，将多个“专家”（这里的“专家”本意是指各种机器学习模型，可以较好地满足实际问题的需要）的意见进行适当的整合，进而得出最后的综合的判断，比其中任何一个单一的“专家”给出的判断要好。实际上，也就是“三个臭皮匠顶过诸葛亮”的意思。因此，boousting的本意就是寻找到合适的“臭皮匠”。 在实际的数据处理的过程中，我们往往可以很容易地发现各种各样的弱机器学习模型，这些模型仅仅比随机猜测好一些，但是还远远不能满足实际作业的精度要求。但是要寻找到一个单一的十分强大的机器学习模型往往会十分困难，虽然可以很好的满足要求，但是寻找这样的模型并不容易。不过好在，我们可以通过整合之前发现的弱机器学习模型，来进行综合考虑，从而形成一个可以媲美单一的强大的机器学习模型。这些弱机器学习模型往往被称之为“弱学习方法”，强机器学习模型往往被称之为“强学习方法”。 二、AdaBoost算法 1、AdaBoost算法的过程 2、AdaBoost的使用","link":"/2019/05/10/Article1AdaBoost/"},{"title":"YOLOv3：test_single_image.py","text":"一、YOLO简介 YOLO（You Only Look Once）是一个高效的目标检测算法，属于One-Stage大家族， 本文参考的源码地址为：https://github.com/wizyoung/YOLOv3_TensorFlow 二、代码和注释 文件目录：test_single_image.py。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# coding: utf-8from __future__ import division, print_functionimport tensorflow as tfimport numpy as npimport argparseimport cv2from utils.misc_utils import parse_anchors, read_class_namesfrom utils.nms_utils import gpu_nmsfrom utils.plot_utils import get_color_table, plot_one_boxfrom model import yolov3# 设置命令行参数，具体可参见每一个命令行参数的含义parser = argparse.ArgumentParser(description=\"YOLO-V3 test single image test procedure.\")parser.add_argument(\"--input_image\", type=str, default=\"./data/demo_data/dog.jpg\", help=\"The path of the input image.\")parser.add_argument(\"--anchor_path\", type=str, default=\"./data/yolo_anchors.txt\", help=\"The path of the anchor txt file.\")parser.add_argument(\"--new_size\", nargs='*', type=int, default=[416, 416], help=\"Resize the input image with `new_size`, size format: [width, height]\")parser.add_argument(\"--class_name_path\", type=str, default=\"./data/coco.names\", help=\"The path of the class names.\")parser.add_argument(\"--restore_path\", type=str, default=\"./data/darknet_weights/yolov3.ckpt\", help=\"The path of the weights to restore.\")args = parser.parse_args()# 处理anchors，这些anchors是通过数据聚类获得，一共9个，shape为：[9, 2]。# 需要注意的是，最后一个维度的顺序是[width, height]args.anchors = parse_anchors(args.anchor_path)# 处理classes， 这里是将所有的class的名称提取了出来，组成了一个列表args.classes = read_class_names(args.class_name_path)# 类别的数目args.num_class = len(args.classes)# 根据类别的数目为每一个类别分配不同的颜色，以便展示color_table = get_color_table(args.num_class)# 读取图片img_ori = cv2.imread(args.input_image)# 获取图片的尺寸height_ori, width_ori = img_ori.shape[:2]# resize，根据之前设定的尺寸值进行resize，默认是[416, 416]，还是[width, height]的顺序img = cv2.resize(img_ori, tuple(args.new_size))# 对图片像素进行一定的数据处理img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)img = np.asarray(img, np.float32)img = img[np.newaxis, :] / 255.# TF会话with tf.Session() as sess: # 输入的placeholder，用于输入图片 input_data = tf.placeholder(tf.float32, [1, args.new_size[1], args.new_size[0], 3], name='input_data') # 定义一个YOLOv3的类，在后面可以用来做模型建立以及loss计算等操作，参数分别是类别的数目和anchors yolo_model = yolov3(args.num_class, args.anchors) with tf.variable_scope('yolov3'): # 对图片进行正向传播，返回多张特征图 pred_feature_maps = yolo_model.forward(input_data, False) # 对这些特征图进行处理，获得计算出的bounding box以及属于前景的概率已经每一个类别的概率分布 pred_boxes, pred_confs, pred_probs = yolo_model.predict(pred_feature_maps) # 将两个概率值分别相乘就可以获得最终的概率值 pred_scores = pred_confs * pred_probs # 对这些bounding boxes和概率值进行非最大抑制（NMS）就可以获得最后的bounding boxes和与其对应的概率值以及标签 boxes, scores, labels = gpu_nms(pred_boxes, pred_scores, args.num_class, max_boxes=30, score_thresh=0.4, nms_thresh=0.5) # Saver类，用以保存和恢复模型 saver = tf.train.Saver() # 恢复模型参数 saver.restore(sess, args.restore_path) # 运行graph，获得对应tensors的具体数值，这里是[boxes, scores, labels]，对应于NMS之后获得的结果 boxes_, scores_, labels_ = sess.run([boxes, scores, labels], feed_dict={input_data: img}) # rescale the coordinates to the original image # 将坐标重新映射到原始图片上，因为前面的计算都是在resize之后的图片上进行的，所以需要进行映射 boxes_[:, 0] *= (width_ori/float(args.new_size[0])) boxes_[:, 2] *= (width_ori/float(args.new_size[0])) boxes_[:, 1] *= (height_ori/float(args.new_size[1])) boxes_[:, 3] *= (height_ori/float(args.new_size[1])) # 输出 print(\"box coords:\") print(boxes_) print('*' * 30) print(\"scores:\") print(scores_) print('*' * 30) print(\"labels:\") print(labels_) # 绘制并展示，保存最后的结果 for i in range(len(boxes_)): x0, y0, x1, y1 = boxes_[i] plot_one_box(img_ori, [x0, y0, x1, y1], label=args.classes[labels_[i]], color=color_table[labels_[i]]) cv2.imshow('Detection result', img_ori) cv2.imwrite('detection_result.jpg', img_ori) cv2.waitKey(0)","link":"/2019/05/22/Article9YOLOv3-part01/"},{"title":"Welcome","text":"WELCOME","link":"/2019/05/12/Welcome/"},{"title":"代码实现二维平面上的卷积及其反向传播","text":"前言 在前面的叙述中，我们都是在二维平面上做卷积操作，并在此基础上进行了反向传播算法的推导和计算，但是，如果仅仅限于理论怕是很难验证我们算法的是否是真的可以让结果收敛，因此，这一篇文章中，我们就通过代码来验证一下算法在实际过程中的收敛情况。 必须要说明的是，之前讲解的算法都是十分朴素的，具有很大的改进空间，代码的实现也是基于这种朴素的计算过程，因此，并未对算法进行优化。 一、代码 在代码中，我们实现了一个20x20大小的矩阵的卷积操作，并在给定的卷积核的情况下计算出了我们需要的结果，然后随机初始化卷积核，希望通过训练，让卷积的结果尽可能和在给定的卷积核的情况下一致。 代码的中每一个函数的详细功能都做了注释。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225# import tensorflow as tfimport numpy as npimport matplotlib.pyplot as plt# 学习率learning_rate = 0.00003def convolution(x, kernel, stride): \"\"\" 二维平面上的卷积，padding为VALID :param x: 被卷积的特征矩阵，是一个二维矩阵 :param kernel: 卷积核参数，为一个二维矩阵 :param stride: 步长信息，一个正整数 :return: 卷积之后的矩阵信息 \"\"\" assert len(x.shape) == 2 assert len(kernel.shape) == 2 assert type(stride) is int assert (x.shape[0] - kernel.shape[0]) % stride == 0 and (x.shape[1] - kernel.shape[1]) % stride == 0 result = np.zeros([(x.shape[0] - kernel.shape[0]) // stride + 1, (x.shape[1] - kernel.shape[1]) // stride + 1]) for i in range(0, x.shape[0] - kernel.shape[0] + 1, stride): for j in range(0, x.shape[1] - kernel.shape[1] + 1, stride): sum = 0 for p in range(kernel.shape[0]): for k in range(kernel.shape[1]): sum += x[i + p][j + k] * kernel[p][k] result[i // stride][j // stride] = sum return result# 对矩阵的上下左右分别进行填补0的操作。def padding_zeros(x, left_right, top_bottom): \"\"\" 对矩阵的外围进行填补0的操作。 :param x: 一个二维矩阵 :param left_right: 一个长度为2的数组，分别表示左侧和右侧需要填补的0的层数 :param top_bottom: 一个长度为2的数组，分别表示上侧和下侧需要填补的0的层数 :return: 填补之后的矩阵 \"\"\" assert len(x.shape) == 2 assert len(left_right) == 2 and len(top_bottom) == 2 new_x = np.zeros([top_bottom[0] + top_bottom[1] + x.shape[0], left_right[0] + left_right[1] + x.shape[1]]) new_x[top_bottom[0]: top_bottom[0] + x.shape[0], left_right[0]: left_right[0] + x.shape[1]] = x return new_xdef insert_zeros(x, stride): \"\"\" 在矩阵的每两个相邻元素之间插入一定数目的0 :param x: 一个二维矩阵 :param stride: 一个非负数 :return: 插入0之后的矩阵 \"\"\" assert len(x.shape) == 2 assert type(stride) is int and stride &gt;= 0 new_x = np.zeros([(x.shape[0] - 1) * stride + x.shape[0], (x.shape[1] - 1) * stride + x.shape[1]]) for i in range(x.shape[0]): for j in range(x.shape[1]): new_x[i * (stride + 1)][j * (stride + 1)] = x[i][j] return new_xdef rotate_180_degree(x): \"\"\" 将矩阵旋转180°，这一步主要是针对卷积核而言。 :param x: 需要被旋转的矩阵 :return: 旋转之后的矩阵 \"\"\" assert len(x.shape) == 2 return np.rot90(np.rot90(x))class conv(object): def __init__(self, kernel, stride=1, bias=None): \"\"\" 表示卷积的类 :param kernel: 卷积核参数，可以是一个整数，表示卷积核尺寸，也可以是一个二维的矩阵。 :param stride: 一个正整数，表示步长信息 :param bias: 偏置量，一个浮点数 \"\"\" if type(kernel) is int: self.kernel = np.random.normal(0, 1, [kernel, kernel]) self.kernel_size = kernel elif type(kernel) is np.ndarray and len(kernel.shape) == 2: assert kernel.shape[0] == kernel.shape[1] self.kernel = kernel self.kernel_size = kernel.shape[0] self.bias = np.random.normal(0, 1) if bias is None else bias self.stride = stride self.x = None def forward(self, x): \"\"\" 前向传播的计算 :param x: 输入矩阵，是一个二维矩阵 :return: 经过卷积并加上偏置量之后的结果 \"\"\" self.x = x return convolution(x, self.kernel, stride=self.stride) + self.bias def backward(self, error): \"\"\" 卷积的反向传播过程 :param error: 接收到的上一层传递来的误差矩阵 :return: 应该传递给下一层的误差矩阵 \"\"\" # 首先在矩阵的每两个元素之间插入合适数目的0 error_inserted = insert_zeros(error, stride=self.stride - 1) # 在上面的矩阵外围填补上合适数目的0 error_ = padding_zeros(error_inserted, [self.kernel_size - 1, self.kernel_size - 1], [self.kernel_size - 1, self.kernel_size - 1]) # 将卷积核旋转180° kernel = self.kernel.copy() kernel = rotate_180_degree(kernel) # 将上面的两个矩阵进行卷积操作，步长为1，求得需要传递给下一层的误差矩阵 error_ = convolution(error_, kernel, 1) # 参数更新 # 将输入矩阵和插入0的矩阵进行步长为1的卷积，得到卷积核的更新梯度 kernel_gradient = convolution(self.x, error_inserted, 1) # 将误差矩阵中每个元素相加得到偏置量的更新梯度 bias_gradient = np.sum(error_inserted) # 利用学习率更新参数 self.kernel -= kernel_gradient * learning_rate self.bias -= bias_gradient * learning_rate # 返回误差矩阵 return error_class sigmoid(object): def __init__(self): self.x = None def forward(self, x): self.x = x return 1.0/(1.0+np.exp(self.x)) def backward(self, error): s = 1.0/(1.0+np.exp(self.x)) return error * s * (1 - s)class relu(object): def __init__(self): self.x = None def forward(self, x): self.x = x return np.maximum(x, 0) def backward(self, error): return error * (self.x &gt; 0)if __name__ == '__main__': map = np.random.normal(1, 1, (20, 20)) print(\"*\"*30) print(\"feature map:\") print(np.round(map, 3)) kernel1 = np.array([[0, 0, 1], [0, 2, 0], [1, 0, 1]], dtype=np.float32) kernel2 = np.array([[1, 0, 0, 1], [1, 0, 0, 1], [0, -1, -1, 0], [0, 1, 1, 0]], dtype=np.float32) print(\"*\"*30) print(\"kernel 1:\\n\", kernel1) print(\"*\" * 30) print(\"kernel 2:\\n\", kernel2) # 这里我们建立两层卷积，卷积核为给定的数值：kernel1和kernel2 conv1 = conv(kernel1, 1, 0) map1 = conv1.forward(map) conv2 = conv(kernel2, 2, 0) target = conv2.forward(map1) print(\"*\"*30) print(\"our target feature:\\n\", np.round(target, 3)) # 建立同样的两层网络，只不过卷积核为随机产生的数值 print(\"\\nBuilding our model...\") conv1 = conv(3, 1, 0) map1 = conv1.forward(map) conv2 = conv(4, 2, 0) map2 = conv2.forward(map1) loss_collection = [] print(\"\\nStart training ...\") for loop in range(2000): # 损失值我们使用简单的差平方和来计算。 loss_value = np.sum(np.square(map2 - target)) loss_collection.append(loss_value) if loop % 100 == 99: print(loop, \": \", loss_value) # 根据损失函数得到最末端的误差矩阵，并逐级向前传递 error_ = 2 * (map2 - target) error_ = conv2.backward(error_) error_ = conv1.backward(error_) # 参数更新之后需要重新进行正向传播，以获得新的输出矩阵 map1 = conv1.forward(map) map2 = conv2.forward(map1) print(\"*\"*30) print(\"Our reconstructed map:\") print(map2) plt.plot(np.arange(len(loss_collection)), loss_collection) plt.show() 二、结果 代码运行如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145******************************feature map:[[-1.493e+00 1.429e+00 4.150e-01 2.294e+00 9.450e-01 1.043e+00 1.115e+00 1.470e+00 1.999e+00 8.890e-01 -1.190e-01 2.020e-01 2.630e-01 1.916e+00 1.292e+00 8.600e-02 2.161e+00 1.212e+00 1.296e+00 3.067e+00] [-9.290e-01 4.850e-01 1.980e+00 2.666e+00 1.507e+00 2.475e+00 7.210e-01 6.480e-01 8.730e-01 2.068e+00 -8.910e-01 7.600e-01 4.270e-01 5.770e-01 5.800e-01 7.930e-01 1.076e+00 1.472e+00 2.294e+00 1.200e-01] [ 1.356e+00 -8.810e-01 1.626e+00 -6.420e-01 4.580e-01 4.310e-01 1.639e+00 5.760e-01 1.101e+00 -1.714e+00 1.070e-01 1.200e+00 1.862e+00 -2.820e-01 2.282e+00 1.285e+00 2.940e-01 8.400e-01 3.070e-01 9.910e-01] [ 4.910e-01 3.052e+00 4.760e-01 7.610e-01 1.366e+00 1.263e+00 2.353e+00 -2.200e-02 -5.740e-01 2.357e+00 2.029e+00 1.604e+00 -2.630e-01 7.030e-01 1.383e+00 1.858e+00 1.831e+00 2.824e+00 9.800e-01 2.470e+00] [ 1.521e+00 3.700e-01 7.260e-01 3.117e+00 -7.180e-01 1.144e+00 1.514e+00 1.212e+00 1.540e+00 1.202e+00 1.326e+00 9.830e-01 7.290e-01 2.144e+00 1.885e+00 3.084e+00 7.270e-01 1.983e+00 1.162e+00 6.660e-01] [ 1.758e+00 1.107e+00 2.234e+00 3.171e+00 1.200e+00 1.257e+00 1.659e+00 1.835e+00 -1.299e+00 1.283e+00 1.034e+00 4.140e-01 1.478e+00 1.042e+00 3.028e+00 9.400e-01 2.014e+00 -9.420e-01 1.578e+00 -4.270e-01] [ 3.420e-01 -9.200e-02 1.097e+00 -1.641e+00 3.155e+00 2.114e+00 8.570e-01 9.910e-01 1.147e+00 1.006e+00 1.065e+00 -4.620e-01 1.560e+00 1.895e+00 -8.060e-01 7.630e-01 3.091e+00 1.262e+00 -1.840e-01 1.220e-01] [ 3.828e+00 2.010e+00 2.188e+00 6.040e-01 1.420e+00 9.900e-02 1.033e+00 7.460e-01 2.611e+00 1.258e+00 9.510e-01 9.210e-01 7.440e-01 7.300e-01 2.780e-01 1.717e+00 2.000e-01 1.595e+00 1.389e+00 1.656e+00] [ 3.350e-01 8.200e-01 3.018e+00 1.879e+00 -2.100e-02 -6.250e-01 1.573e+00 6.390e-01 1.452e+00 -4.680e-01 1.200e+00 1.833e+00 1.526e+00 1.907e+00 1.785e+00 2.051e+00 8.190e-01 1.600e-01 2.093e+00 9.290e-01] [ 9.590e-01 -1.270e-01 2.545e+00 1.877e+00 1.014e+00 5.830e-01 1.600e+00 1.722e+00 1.240e-01 1.378e+00 1.370e-01 1.201e+00 6.540e-01 6.960e-01 1.701e+00 -8.420e-01 1.627e+00 -1.674e+00 -7.400e-02 1.127e+00] [ 7.030e-01 3.290e-01 1.841e+00 1.385e+00 3.680e-01 1.174e+00 1.875e+00 8.580e-01 8.540e-01 1.696e+00 -2.330e-01 -8.310e-01 1.079e+00 1.012e+00 5.040e-01 1.632e+00 2.550e-01 -2.090e-01 8.530e-01 -5.290e-01] [ 4.100e-01 1.175e+00 1.506e+00 -8.640e-01 1.488e+00 1.113e+00 7.790e-01 8.280e-01 1.074e+00 -5.560e-01 -2.760e-01 2.561e+00 1.135e+00 1.375e+00 -8.140e-01 -8.700e-02 -1.120e-01 5.220e-01 -8.720e-01 5.610e-01] [ 1.845e+00 1.736e+00 -3.690e-01 1.212e+00 1.269e+00 1.774e+00 -1.240e+00 -3.770e-01 4.360e-01 9.160e-01 2.761e+00 -1.470e-01 1.095e+00 3.250e-01 -2.150e-01 -2.332e+00 9.290e-01 1.240e+00 5.680e-01 1.836e+00] [-4.660e-01 6.710e-01 1.191e+00 2.862e+00 3.050e-01 1.861e+00 4.020e-01 1.327e+00 7.300e-01 6.270e-01 2.682e+00 3.048e+00 3.630e-01 -3.140e-01 -3.890e-01 2.380e+00 -5.500e-01 1.491e+00 3.220e-01 4.970e-01] [ 7.290e-01 2.541e+00 1.011e+00 -3.260e-01 1.789e+00 6.700e-02 3.560e-01 1.883e+00 2.602e+00 4.550e-01 1.768e+00 3.350e-01 1.590e-01 2.364e+00 3.659e+00 7.420e-01 2.127e+00 2.639e+00 5.050e-01 3.774e+00] [ 2.620e+00 -5.000e-03 1.250e+00 1.045e+00 1.090e+00 1.300e-02 1.816e+00 1.499e+00 -6.680e-01 1.314e+00 5.150e-01 1.111e+00 4.190e-01 1.564e+00 1.990e+00 3.700e-01 -2.270e-01 2.090e-01 1.865e+00 -8.290e-01] [-8.800e-02 2.931e+00 9.130e-01 4.410e-01 -2.790e-01 2.880e-01 -2.830e-01 1.576e+00 1.374e+00 1.437e+00 7.210e-01 2.540e-01 7.540e-01 7.900e-02 8.110e-01 2.291e+00 4.300e-02 1.173e+00 2.160e-01 1.258e+00] [ 6.200e-01 -7.400e-02 -6.030e-01 -2.650e-01 6.640e-01 2.018e+00 -1.574e+00 2.530e-01 1.260e+00 8.890e-01 -2.350e-01 1.029e+00 1.000e-03 2.490e-01 1.300e-02 7.250e-01 1.890e-01 2.984e+00 9.790e-01 9.130e-01] [ 1.443e+00 3.438e+00 8.000e-01 3.240e-01 8.100e-01 4.750e-01 1.041e+00 5.130e-01 1.329e+00 -4.180e-01 2.237e+00 3.070e-01 1.488e+00 1.637e+00 -2.300e-01 1.642e+00 2.034e+00 -9.670e-01 1.079e+00 1.006e+00] [ 2.653e+00 4.840e-01 4.790e-01 -1.493e+00 4.030e-01 -5.900e-02 3.610e-01 -9.410e-01 -6.060e-01 2.276e+00 4.180e+00 3.480e-01 -4.930e-01 1.783e+00 -4.270e-01 1.799e+00 2.069e+00 1.685e+00 1.172e+00 1.912e+00]]******************************kernel 1: [[0. 0. 1.] [0. 2. 0.] [1. 0. 1.]]******************************kernel 2: [[ 1. 0. 0. 1.] [ 1. 0. 0. 1.] [ 0. -1. -1. 0.] [ 0. 1. 1. 0.]]******************************our target feature: [[26.553 18.88 18.124 18.16 8.258 23.685 24.708 24.097] [20.64 31.311 14.439 25.2 16.984 30.628 19.429 32.712] [29.07 19.528 25.637 14.765 27.227 21.041 27.518 10.76 ] [22.549 24.207 16.272 20.756 14.157 24.426 12.429 19.15 ] [16.412 28.147 14.357 18.035 25.29 12.691 9.955 13.582] [19.198 13.485 16.587 13.285 12.428 16.192 11.528 3.429] [15.984 17.609 12.73 27.705 16.503 17.337 16.276 26.504] [11.86 11.337 8.349 13.658 25.007 17.74 20.237 23.013]]Building our model...Start training ...99 : 641.0993619453625199 : 428.8371676892213299 : 297.4697981028265399 : 233.14519479024102499 : 192.98557066294234599 : 162.86663169198815699 : 135.89478966699724799 : 112.77961888797242899 : 92.23756791064116999 : 74.446621074744971099 : 59.2064164734652961199 : 46.576494244467821299 : 36.5031660070200351399 : 28.753778125126171499 : 22.914096973997211599 : 18.4844259118159081699 : 15.0258518201786371799 : 12.2342613631377191899 : 9.928595283175861999 : 8.00538026504608******************************Our reconstructed map:[[26.03217516 18.57601188 17.6417854 17.74106052 8.00899738 23.45962739 24.25683639 23.52688692] [20.34532322 30.86283096 14.08412555 24.79283798 16.8623232 30.0709141 19.06563574 32.19093566] [28.79064215 18.94645701 25.26600894 14.3734212 26.84098616 20.56125496 27.22489939 10.43946569] [22.32314845 23.72247534 15.95624896 20.26777129 13.74630188 24.21656068 12.33211161 18.89299591] [16.02570137 27.59958699 14.17716638 17.78058347 25.12238011 12.49517613 9.7392468 13.45919031] [18.80339915 13.04877141 16.39073949 12.79776179 11.97347116 15.91895708 11.3292454 3.33631504] [15.67305314 17.20668751 12.43561879 27.23227887 16.12905207 17.00011351 15.94369672 26.2171221 ] [11.53226582 11.37091115 8.17071381 13.7191314 24.74154248 17.43440331 19.80145666 22.61327712]] 从上面的结果可以看出，我们经过训练之后得到的重建之后的输出矩阵和原始的目标矩阵已经十分接近。 在训练过程中的loss值绘制成折线图如下： 可以看出，损失值loss在某些地方会存在一些小的波动，但在整体上，损失值loss是处在一直在减小的过程中的，这说明我们的算法是正确的。","link":"/2019/05/24/Article17ConvBackProp-part4/"},{"title":"逻辑回归算法","text":"前言 前面主要是讲反向传播和梯度下降的方法，那么其实涉及梯度的机器学习方法并不是只有深度学习一种，逻辑回归也是可以利用梯度的信息进行参数的更新，使得模型逐步满足我们的数据要求。注意，逻辑回归输出的是属于某一种类别的概率，利用阈值的控制来进行判别，因此逻辑回归本质上是一种分类方法。 一、逻辑斯蒂回归 逻辑斯蒂回归（logistic regression，下面简称逻辑回归），是一种十分经典的分类方法。我们首先介绍一下逻辑回归的定义。 假设我们有一个数据集 \\(S\\)，一共包含\\(m\\)个样本数据，即： \\(S = \\{(x_1,y_1),(x_2,y_2),...,(x_m，y_m)\\}\\)，其中，\\(y_i \\in \\{0, 1\\}\\)。为了表示的方便，我们不妨设当\\(y_i = 1\\)时为正样本，当\\(y_i = 0\\)时为负样本，当然，反过来也是可以的，这个并不重要，只不过一般习惯这样表达。 在SVM中，我们根据数据集的分布，求解出了一个二分的超平面 \\(f(x) = \\omega \\cdot x+b​\\)，现在我们要对一个新的样本点\\(x_0​\\)进行分类预测，需要将这个样本点带入上面的超平面公式。当\\(f(x_0) = \\omega \\cdot x_0 + b &gt; 0​\\)时，我们将这个样本点标记为1，当\\(f(x_0) = \\omega \\cdot x_0 + b \\leq 0​\\)时，我们将这个样本点标记为-1。观察到SVM只能对新样本输出 \\(\\pm1​\\)，无法较为准确的输出样本属于每一个类别的概率究竟是多少。SVM结果的正确性在于它保证了找到的是样本间隔最大的那个超平面，这样就可以保证以最高的精确度区分新的样本。然而SVM却无法对一个新样本的概率进行求解。而这就是逻辑回归主要做的事情，它输出的是一个概率值，当这个概率值大于一定的阈值时，样本标记为1，反之则标记为0。 二、sigmoid函数 熟悉深度学习的人肯定对这个函数非常了解，它是早期深度学习网络经常使用的激活函数之一。它的定义公式如下： \\[ sigmoid(x) = \\frac{1}{1 + e^{-x}} \\quad x \\in R \\] 它的函数图像是一个典型的S型曲线，定义域是全体实数。我们根据公式可以发现，这个函数将全体实数映射到了 \\((0, 1)\\) 区间上，并在这个区间上单调递增，\\(x\\)越大，函数值越大。而这正好符合我们需要的概率分布的规律。 三、逻辑回归模型 二项逻辑回归模型本质上是一个类似下面公式的条件分布： \\[ P(Y = 1 | x) = \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{1} \\] \\[ P(Y = 0 | x) = 1 - \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{2} \\] 其中，第一个式子是我们需要重点关注的。我们对第一个式子右边的分数，上下同时乘以 \\(e^{\\omega \\cdot x + b}\\)，得到： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x + b}}{1 + e^{\\omega \\cdot x + b}} \\tag{3} \\] 以上就是我们经常可以看到的逻辑回归的公式了。 现在我们将偏置量也放进参数 \\(\\omega\\)中，所以变量\\(x\\)的尾部会增加一个多余的维度1来和偏置量进行匹配，于是，我们有如下的表示方式： \\[ \\omega = \\begin{bmatrix} \\omega_1 &amp; \\omega_2 &amp; \\cdots &amp; \\omega_n &amp; b\\end{bmatrix} \\] \\[ x = \\begin{bmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n &amp; 1\\end{bmatrix} \\] 于是，原逻辑回归公式可以有以下的表达： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} \\tag{4} \\] 四、损失函数 很容易想到，损失函数我们仍然可以使用前面介绍的差方和的方法计算。当距离目标越近时，差方和越小，损失就会越小。事实上并不能这样进行处理。 \\[ J(\\omega, b) = \\sum_i( \\frac{1}{1 + e^{-(\\omega \\cdot x_i)}} - y_i)^2 = \\sum_i (\\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} - y_i) ^2\\tag{4} \\] 原因是如果使用差方和作为最后的损失函数，那么我们得到的最后的损失函数并不是一个简单的凹函数（或者凸函数），这个函数存在许多的局部极小值，因此很难通过梯度下降的方法收敛到全局最小值附近，这样导致的结果就是训练出来的模型并不能很好的满足我们的需要，误差较大。 所以我们必须要重新定义一个满足我们条件的损失函数，或者称为目标函数。我们考虑使用极大似然估计的方法进行参数估计。 对于其中的某一个样本，如果该样本的标签为1，那么我们需要极大化\\(P(Y = 1|x)\\)，如果该样本的标签为0，那么我们需要极大化\\(1 - P(Y = 1|x)\\)，于是对于每一个样本数据，综合来看，我们只需要极大化以下的式子即可： \\[ P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\] 上面的式子看起来很吓人，其实本质很简单。于是我们的似然函数可以表示为 \\[ L(\\omega) = \\prod_i P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\tag{5} \\] 由于这里涉及指数，而且是连乘运算，计算不方便，于是我们可以用取对数的方式进行处理，这里可以这样处理的原因是上式取最大的时候，其对数也一定是取最大值，因为对数函数是一个单调函数。于是有： \\[ log L(\\omega) = \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\tag{6} \\] 现在我们可以将之前的计算结果带入到公式(6)中进行化简： \\[ \\begin{aligned} logL(\\omega) &amp;= \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log(1 - \\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) - y_i log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i (log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) - log(\\frac{1}{1 + e^{\\omega \\cdot x_i}})) + log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(e^{\\omega \\cdot x_i}) - log(1 + e^{\\omega \\cdot x_i}) \\\\ &amp;= \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\end{aligned} \\tag{7} \\] 于是我们需要的似然函数就变成了： \\[ logL(\\omega) = \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\tag{8} \\] 上面的似然函数并不能直接进行最优化求解，于是我们常常利用梯度下降的方法进行逐步迭代求解，这就需要对上面的公式进行求导的操作，我们对参数\\(\\omega\\)求导如下： \\[ \\begin{aligned} \\frac{\\partial logL(\\omega)}{\\partial \\omega} &amp;= \\sum_i y_i x_i - \\frac{1}{1+e^{\\omega \\cdot x_i}} e^{\\omega \\cdot x_i} x_i \\\\ &amp;= \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\end{aligned} \\tag{9} \\] 以上就是利用梯度下降算法进行逻辑回归问题的求解的（偏）导数计算公式，在每一轮的迭代过程中，我们对所有的样本进行梯度计算，最后累加梯度，然后按照计算出的梯度信息更新需要的参数。 由于我们这里需要将似然函数极大化，因此和反向传播的梯度下降不同，这里使用的是类似的梯度上升的方法，于是我们有如下的迭代公式：这里的\\(\\alpha\\)指的是学习率（或者说是步长信息） \\[ \\omega := \\omega + \\alpha \\frac{\\partial logL(\\omega)}{\\partial \\omega} \\tag{10} \\] 五、关于逻辑回归的似然函数 在利用不同的方式计算损失函数时，我们总是希望得到的损失函数时一个凸函数（或者凹函数），这样我们就可以保证只有一个全局的最优解，而且不存在局部极小值或者极大值，而这些条件都对我们使用梯度下降方法来求解最优化问题十分有利。如果一个函数的二阶导数总是恒大于0，我们称这个函数为凸函数，如果一个函数的二阶导数总是恒小于0，我们称这个函数为凹函数，凸函数一定存在一个全局最小值，凹函数一定存在一个全局最大值，并且不管是凹函数还是凸函数都不存在局部极小值或者局部最大值。 我们以\\(y = x^2\\)为例，经过计算我们得到它的二阶导数为\\(y\\prime\\prime = 2\\)，是一个大于0的常数，因此该函数是一个凸函数，其不存在各种局部最小值或者局部最大值，只有一个全局最小值0（当然这个全局最小值也可以看作是一个某一个区域内的局部最小值）。 现在我们重新审视一下我们的似然函数，之前我们已经求解出了似然函数的梯度信息（一阶导数），即： \\[ \\frac{\\partial logL(\\omega)}{\\partial \\omega} = \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\tag{11} \\] 我们继续对上式进行求导的操作，有： \\[ \\begin{aligned} \\frac{\\partial^2 logL(\\omega)}{\\partial \\omega^2} &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i (1 + e^{e^{\\omega \\cdot x_i}}) - e^{\\omega \\cdot x_i}(e^{\\omega \\cdot x_i} x_i)}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - \\frac{e^{\\omega \\cdot x_i}}{(1 + e^{\\omega \\cdot x_i})^2} x_i \\cdot x_i \\end{aligned} \\tag{12} \\] 可以发现的是，上式对于任何的一个数据集合来说都是恒小于0的，因此可以说当我们使用极大似然估计作为损失函数时，该函数是一个凹函数，有一个最大值，可以利用梯度下降（严格来说这个应该是梯度上升的方法）进行求解。 通常，当我们求解出最终的参数时，可以获得一个超平面。往往我们将逻辑回归的阈值设置为0.5，将输出值高于0.5的样本标记为正样本，将输出值低于0.5的样本标记为负样本，于是，我们可以得到分类的超平面为： \\[ \\frac{1}{1 + e^{\\omega \\cdot x}} = \\frac{1}{2} \\tag{13} \\] 化简之后，我们可以得到最终的超平面的表达式为： \\[ \\omega \\cdot x = 0 \\tag{14} \\] 六，代码 这里的数据使用的是《机器学习实战》的数据，一共有100组数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100-0.017612 14.053064 0-1.395634 4.662541 1-0.752157 6.538620 0-1.322371 7.152853 00.423363 11.054677 00.406704 7.067335 10.667394 12.741452 0-2.460150 6.866805 10.569411 9.548755 0-0.026632 10.427743 00.850433 6.920334 11.347183 13.175500 01.176813 3.167020 1-1.781871 9.097953 0-0.566606 5.749003 10.931635 1.589505 1-0.024205 6.151823 1-0.036453 2.690988 1-0.196949 0.444165 11.014459 5.754399 11.985298 3.230619 1-1.693453 -0.557540 1-0.576525 11.778922 0-0.346811 -1.678730 1-2.124484 2.672471 11.217916 9.597015 0-0.733928 9.098687 0-3.642001 -1.618087 10.315985 3.523953 11.416614 9.619232 0-0.386323 3.989286 10.556921 8.294984 11.224863 11.587360 0-1.347803 -2.406051 11.196604 4.951851 10.275221 9.543647 00.470575 9.332488 0-1.889567 9.542662 0-1.527893 12.150579 0-1.185247 11.309318 0-0.445678 3.297303 11.042222 6.105155 1-0.618787 10.320986 01.152083 0.548467 10.828534 2.676045 1-1.237728 10.549033 0-0.683565 -2.166125 10.229456 5.921938 1-0.959885 11.555336 00.492911 10.993324 00.184992 8.721488 0-0.355715 10.325976 0-0.397822 8.058397 00.824839 13.730343 01.507278 5.027866 10.099671 6.835839 1-0.344008 10.717485 01.785928 7.718645 1-0.918801 11.560217 0-0.364009 4.747300 1-0.841722 4.119083 10.490426 1.960539 1-0.007194 9.075792 00.356107 12.447863 00.342578 12.281162 0-0.810823 -1.466018 12.530777 6.476801 11.296683 11.607559 00.475487 12.040035 0-0.783277 11.009725 00.074798 11.023650 0-1.337472 0.468339 1-0.102781 13.763651 0-0.147324 2.874846 10.518389 9.887035 01.015399 7.571882 0-1.658086 -0.027255 11.319944 2.171228 12.056216 5.019981 1-0.851633 4.375691 1-1.510047 6.061992 0-1.076637 -3.181888 11.821096 10.283990 03.010150 8.401766 1-1.099458 1.688274 1-0.834872 -1.733869 1-0.846637 3.849075 11.400102 12.628781 01.752842 5.468166 10.078557 0.059736 10.089392 -0.715300 11.825662 12.693808 00.197445 9.744638 00.126117 0.922311 1-0.679797 1.220530 10.677983 2.556666 10.761349 10.693862 0-2.168791 0.143632 11.388610 9.341997 00.317029 14.739025 0 前两列为数据，最后一列为对应数据的标签。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import matplotlib.pyplot as pltimport numpy as npdef read_file(): positive = [] negative = [] f = open(\"testSet.txt\", \"r\") for line in f: l = line.strip() l = l.split(\"\\t\") l = [float(i) for i in l] if l[-1] == 1.0: positive.append([l[0], l[1], 1]) else: negative.append([l[0], l[1], 1]) positive = np.array(positive, dtype=np.float) negative = np.array(negative, dtype=np.float) return positive, negativedef cost(positive, negative, w): c = 0 for i in positive: c += 1.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: c += 0.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) return cdef loop(): positive, negative = read_file() w = np.array([1, 1, 1]) w_copy = w.copy() alpha = 0.001 for i in range(2000): print(cost(positive, negative, w)) gradient = np.array([0., 0., 0.]) for i in positive: gradient += 1.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: gradient += 0.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) w = w + alpha * gradient return positive, negative, w, w_copyif __name__ == '__main__': positive, negative, w, w_origin = loop() plt.scatter(positive[:, 0], positive[:, 1], c=\"red\") plt.scatter(negative[:, 0], negative[:, 1], c='blue') xs = np.linspace(-4, 3.5, 300) ys1 = (w[0] * xs + w[2]) / (- w[1]) ys2 = (w_origin[0] * xs + w_origin[2]) / (- w_origin[1]) plt.plot(xs, ys1, c=\"green\") plt.plot(xs, ys2, c=\"yellow\") plt.show() 代码运行结果如下，其中黄色直线表示的是初始的分隔超平面，绿色的直线表示为分类超平面，可以看出，样本数据可以较好地被分隔开，第二幅图表示的是梯度上升的情况，可以看到，算法可以较好地收敛于全局最优解附近：","link":"/2019/05/10/Article3Logic Regression/"},{"title":"主成分分析PCA","text":"数据降维 在很多时候，我们收集的数据样本的维度很多，而且有些维度之间存在某一些联系，比如，当我们想要收集用户的消费情况时，用户的收入和用户的食物支出往往存在一些联系，收入越高往往食物的支出也越高（这里只是举个例子，不一定正确。）。那么在拿到这样的数据的时候，我们首先想到的就是我们需要对其中的信息做一些处理，排除掉一些冗余的维度，保留必要的维度信息。这样一来，我们可以大大减小我们的后期处理的工作量，这就是数据降维的基本要求。 主成分分析PCA 当拿到用户的数据时，如何确定到两个维度之间是否存在联系呢？这个就是我们需要用的协方差矩阵所作的工作。所以在使用PCA之前，我们必须对协方差有个简单的了解。 方差和协方差 我们之前遇到的大多数情况是获取到的都是一维的数据，比如数学课程的考试成绩，每个人都能获得一个分数，这些分数形成了一个一维的数组（向量），如果我们用 \\(X\\) 表示考试成绩的分布，然后我们可以估计出数学课程的考试成绩的均值 \\(\\bar{X}\\) 和方差 \\(S^2\\)，如下： \\[ \\bar{X} = \\frac{1}{N} \\sum_{i = 1}^{N} X_i \\tag{1} \\] \\[ S^2 = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X}) ^2 \\tag{2} \\] 需要注意的是在求解方差的时候，我们使用的分母是 \\(N- 1\\)，而不是 \\(N\\)，这是因为我们以上的数学成绩对真实成绩分布的一次简单抽样，为了获得更为准确的关于原分布的方差估计，我们必须使用这种所谓的“无偏估计”。 以上都是关于单一变量的方差估计，通常，我们记单变量的方差为 \\(var(X)\\)，于是我们有: \\[ var(X) = S^2 = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X}) ^2 \\tag{3} \\] 但是考虑到更普遍的情况，我们不止有数学考试（X），我们还有物理考试（Y），英语考试（Z），我们就需要考虑一个问题，不同的考试的成绩之间是否存在某种联系，于是我们就利用协方差来定义两个随机变量之间的关系。现在我们有两个随机变量，\\(X\\) 和 \\(Y\\)的抽样数据分布，我们将他们之间的协方差定义如下： \\[ cov(X, Y) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\tag{4} \\] 很明显，这里的分母依然采用的是 \\(N - 1\\)，是对原分布的无偏估计。对于只有一个变量的情况，我们有： \\[ cov(X, X) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})(X_i - \\bar{X}) = \\frac{1}{N - 1} \\sum_{i = 1}^{N} (X_i - \\bar{X})^2 = var(X) \\tag{5} \\] 所以方差算是协方差的一个特例。协方差本质上是对两个随机变量的相关性的考察，当两个随机变量之间的协方差大于0时，表示这两个随机变量正相关；当两个随机变量之间的协方差等于0时，表示这两个随机变量没有相关关系；当两个随机变量之间的协方差小于0时，表示这两个随机变量负相关。 需要注意的是，两个随机变量相互独立和两个随机变量不相关是两个不同的概念，两个随机变量独立则一定有这两个随机变量不相关，但是两个随机变量不相关并不一定有这两个随机变量相互独立。 协方差矩阵 假设我们现在有一张数据统计的表格，每一列的数据都已经去中心化，即每一列的数据都已经减去了该列的均值。如下： No. \\(X_1\\) \\(X_2\\) ... \\(X_m\\) 1 \\(a_{11}\\) \\(a_{12}\\) ... \\(a_{1m}\\) 2 \\(a_{21}\\) \\(a_{22}\\) ... \\(a_{2m}\\) ... ... ... ... ... n \\(a_{n1}\\) \\(a_{n2}\\) ... \\(a_{nm}\\) 一共有 \\(n\\) 个样本数据，每个样本数据包括 \\(m\\) 个统计信息。对于每一列，都是一个随机变量的简单抽样，不妨我们将上面的矩阵记作 \\(X\\)，于是，我们有： \\[ X = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nm} \\end{bmatrix} = \\begin{bmatrix} c_{1} &amp; c_{2} &amp; \\cdots &amp; c_{m} \\end{bmatrix} \\tag{6} \\] 其中， \\(c_i\\) 表示的是每一个随机变量的取值分布情况，那么，第 \\(i\\) 个随机变量和第 \\(j\\) 个随机变量之间的协方差就可以表示为： \\[ cov(c_i, c_j) = \\frac{1}{n-1} c_i \\cdot c_j = \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{ki} a_{kj} \\tag{7} \\] 那么每两个随机变量之间的协方差矩阵（不妨叫做 \\(CovMatrix\\) ）可以表示如下： \\[ \\begin{aligned} CovMatrix &amp;= \\begin{bmatrix} cov(c_1, c_1) &amp; cov(c_1, c_2) &amp; \\cdots &amp; cov(c_1, c_m) \\\\ cov(c_2, c_1) &amp; cov(c_2, c_2) &amp; \\cdots &amp; cov(c_2, c_m) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ cov(c_m, c_1) &amp; cov(c_m, c_2) &amp; \\cdots &amp; cov(c_m, c_m) \\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k1} a_{km} \\\\ \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{k2} a_{km} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{k1} &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{k2}&amp; \\cdots &amp; \\frac{1}{n - 1} \\sum_{k = 1}^{n} a_{km} a_{km} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} \\begin{bmatrix} \\sum_{k = 1}^{n} a_{k1} a_{k1} &amp; \\sum_{k = 1}^{n} a_{k1} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{k1} a_{km} \\\\ \\sum_{k = 1}^{n} a_{k2} a_{k1} &amp; \\sum_{k = 1}^{n} a_{k2} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{k2} a_{km} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum_{k = 1}^{n} a_{km} a_{k1} &amp; \\sum_{k = 1}^{n} a_{km} a_{k2}&amp; \\cdots &amp; \\sum_{k = 1}^{n} a_{km} a_{km} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} \\begin{bmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{n1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{n2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1m} &amp; a_{2m} &amp; \\cdots &amp; a_{nm} \\\\ \\end{bmatrix} \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1m} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nm} \\\\ \\end{bmatrix} \\\\ &amp;= \\frac{1}{n - 1} X^T X \\end{aligned} \\tag{8} \\] 故，对于上述的矩阵 \\(X\\) ，我们可以求得它的协方差矩阵为： \\[ CovMatrix = \\frac{1}{n - 1} X^T X \\tag{10} \\] 其中，\\(n\\) 为样本数目，\\(X^T\\) 表示矩阵 \\(X\\) 的转置。 在处理表格时，我们已经先做了去中心化的操作，目的是使得协方差公式中的 \\(\\bar{X}\\) 为0，这样的话可以直接进行矩阵的相乘，不必在求解协方差的时候一个一个去计算。 特征值和特征向量 在很多时候，我们需要对一个矩阵求解它的特征值和特征向量，特征值和特征向量的定义是，对于一个对称方阵 \\(A\\) ，如果存在一个值 \\(\\lambda\\) 和一个向量 \\(x\\) 满足如下的条件： \\[ Ax = \\lambda x \\tag{11} \\] 那么我们就将 \\(\\lambda\\) 称为该矩阵的特征值，对应的 \\(x\\) 则称之为特征向量。 根据线性代数的相关知识，我们可以知道，对于一个实对称矩阵 \\(A\\)，一定存在 \\(n\\) 个相正交的特征向量，其中 \\(n\\) 为该矩阵的行数（或者说列数）。那么如果我们将对称矩阵的特征值按照从大到小的顺序进行排列，同时，我们对特征向量也按照其对应特征值的大小进行排列，于是，我们有如下的排列： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\\\ x_1, x_2, \\cdots, x_n \\] 对于上面的每一对特征值和特征向量，我们都能满足等式(11)。于是，我们可以将所有的特征向量组合成一个矩阵，记作 \\(W\\)，将所有的特征值依次放入一个 \\(n\\) 阶的全零方阵的对角线元素中，记作 \\(\\Sigma\\)，于是，我们可以获得下面的等式： \\[ AW = W\\Sigma \\tag{12} \\] 在上面的式子右边同时乘以 \\(W^{-1}\\) ，有： \\[ A = W \\Sigma W^{-1} \\tag{13} \\] 现在我们发现对于矩阵 \\(W\\) ，我们可以得到下面的式子： \\[ W W^T = E \\tag{14} \\] 有以上式子的原因是因为我们对所有的特征向量进行了规范化，使得所有的特征向量模长为1，而且由于\\(A\\)是实对称矩阵，所以\\(A\\)的特征向量两两正交，所以有以下的两个式子： \\[ \\lambda_i \\cdot \\lambda_i = 1 \\\\ \\lambda_i \\cdot \\lambda_j = 0, \\quad i \\ne j \\tag{15} \\] 由公式(14)，我们就可以得到： \\[ W^T = W^{-1} \\tag{16} \\] 故，我们重写式(13)，有： \\[ A = W \\Sigma W^T \\tag{17} \\] 需要注意的是，上述的公式需要满足的条件是 \\(A\\)是一个实对称方矩。 主成分分析 很明显，我们需要对原始的数据进行处理，我们希望是两个不同的数据维度之间的相关性越小越好，而同一个维度内部的方差越大越好，因为只有这样，我们才有很好的排除数据维度的相关性并进行数据的降维操作。我们这里设\\(Y\\)是经过我们变换之后的数据矩阵，那么我们的要求就是\\(Y\\)的协方差矩阵是一个对角矩阵，其中所有的非对角线上的元素都是0，而在对角线上的元素是可以为非0的。同时，我们可以设矩阵\\(P\\)是一组线性变换的相互正交的单位基，按照列进行排列。那么对于原始的数据矩阵，我们可以有： \\[ Y = XP \\tag{18} \\] 上面的式子表示的是\\(X\\)矩阵映射到以矩阵\\(P\\)为基的线性变换。我们假设原始数据\\(X\\)的协方差矩阵为\\(B\\)，经过变化之后的数据\\(Y\\)的协方差矩阵为\\(D\\)，那么我们可以有： \\[ \\begin{aligned} D &amp;= \\frac{1}{n - 1}Y^T Y \\\\ &amp;= \\frac{1}{n - 1}(XP)^T (XP) \\\\ &amp;= \\frac{1}{n - 1}P^T X^T XP \\\\ &amp;=\\frac{1}{n - 1}P^T (X^T X) P \\\\ &amp;= P^T (\\frac{1}{n - 1}X^T X) P \\\\ &amp;= P^T B P \\end{aligned} \\tag{19} \\] 从上面的式子中我们不难发现，我们需要找的是这样的一种变换，\\(D\\)是一个对角矩阵，不妨这里假设对角矩阵\\(D\\)的对角线上的元素是从大到小排列的，\\(B\\)是原始数据矩阵的协方差矩阵，因此，我们需要的找到的变换\\(P\\)可以将\\(B\\)映射成对角矩阵\\(D\\)。 此时，我们考虑到对于一个实对称矩阵\\(A\\)，我们有\\(A = W \\Sigma W^T\\)，其中，\\(\\Sigma\\)是矩阵\\(A\\)的特征值组成的对角矩阵，\\(W\\)是矩阵\\(A\\)的特征向量按照对应特征值的排列组成的特征向量矩阵，所以，结合上面的推导，我们不难发现，\\(D\\)矩阵就可以对应于这里的\\(\\Sigma\\)矩阵，\\(W\\)矩阵就可以对应于我们问题中的\\(P\\)矩阵。 \\[ D = P^T B P \\\\ (P^T)^{-1}DP^{-1} = B \\\\ \\] 考虑到\\(P\\)矩阵是由一组正交单位基组成的，所以满足： \\[ P^T P = E \\] 所以有： \\[ P^T = P^{-1} \\] 代入上面的式子，我们有： \\[ (P^T)^{-1}DP^{-1} = P D P^T = B \\] 我们可以发现，我们需要的变换矩阵\\(P\\)就是由\\(B\\)的特征向量组成的。因此，问题就转换成了求解\\(B\\)的特征值和特征向量，而\\(B\\)又是原始矩阵\\(X\\)的协方差矩阵。 由于我们这里主要关心的是维度之间的相关性的大小，而且在后面的处理中，我们需要求解协方差矩阵的特征向量，所以我们可以将求解协方差矩阵的系数省略。 在后面的代码中，我们就直接省略了协方差矩阵的前系数，进一步简化了计算。 数据降维 在前面的式子中，我们并没有对数据的维度进行操作，即我们保留了所有的数据维度，但是并不是所有的数据维度都是足够必要的。由于矩阵的特征值往往会有比较大的差距，当我们求解出了原始数据的协方差矩阵的特征值和特征向量之后，我们可以舍弃掉太小的特征值以及对应的特征向量，这是因为特征值太小，转换之后的对应的数据维度的方差就会很小，也就是说该维度数据之间的差距不大，不能帮我们很好的区分数据之间的差别，舍弃之后也不会丢失太多的信息，舍弃之后反而可以减小数据的维度信息，更好的节省数据的占用空间，这对于一些大量数据的处理是十分有益的。 我们求出原始数据矩阵的协方差矩阵的特征值和特征向量之后，我们将特征向量进行从大到小进行排序，并按照特征值的顺序将特征向量进行排序。我们可以取前\\(K\\)个特征值和特征向量，并将这些向量组合成一个变换矩阵。将这个变换矩阵应用到原始数据矩阵上即可对原始数据进行降维。这里的\\(K\\)按照任务的要求可以有不同的取值。还有一种选取特征值的方法就是选取前\\(K\\)个特征值，这些特征值之和占全部特征值之和的90%或者95%以上。 PCA实例 接下来就是一个PCA的实例，在sklearn的python包中，有一个鸢尾花数据集，包括了150组鸢尾花的数据，分为三类，每一组数据包含四个数据维度，我们的目标就是将这个数据集的数据维度减少到两维以便我们可以在二维平面上进行绘制。 这里使用了numpy自带的特征值和特征向量的求解函数，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import matplotlib.pyplot as pltfrom sklearn.datasets import load_irisimport numpy as npdef load_data(): data = load_iris() y = data.target x = data.data return x, ydef pca(x, dimensions=2): x = np.array(x) assert len(x.shape) == 2, \"The data must have two dimensions.\" # 求解协方差矩阵 matrix = np.matmul(np.transpose(x), x) # 求解特征值和特征向量 values, vectors = np.linalg.eig(matrix) # 按照从大到小的顺序给特征值排序 index = np.argsort(values)[::-1] # 选取前几个特征值较大的特征向量，并组成一个变换矩阵 vectors = vectors[:, index[:dimensions]] # 应用变换矩阵，将原始数据进行映射 x_dimension_reduction = np.matmul(x, vectors) return x_dimension_reductiondef draw(x, y): red_x, red_y = [], [] blue_x, blue_y = [], [] green_x, green_y = [], [] for i in range(len(x)): if y[i] == 0: red_x.append(x[i][0]) red_y.append(x[i][1]) elif y[i] == 1: blue_x.append(x[i][0]) blue_y.append(x[i][1]) else: green_x.append(x[i][0]) green_y.append(x[i][1]) plt.scatter(red_x, red_y, c='r', marker='x') plt.scatter(blue_x, blue_y, c='b', marker='D') plt.scatter(green_x, green_y, c='g', marker='.') plt.show()if __name__ == '__main__': x, y = load_data() # 去中心化 x = x - np.mean(x, 0) x_dimension_reduction = pca(x, 2) draw(x_dimension_reduction, y) 结果如下。可以看到，数据被较好地区分开了，其中一组数据已经完全和其他的两组数据脱离，另外两组数据也有了明显的分隔界限。这证明我们的PCA方法是正确的。","link":"/2019/05/12/Article5PCA/"},{"title":"反向传播算法（一）之反向传播入门","text":"一、反向传播算法 近年来，深度学习的快速发展带来了一系列喜人的成果，不管是在图像领域还是在NLP领域，深度学习都显示了其极其强大的能力。而深度学习之所以可以进行大规模的参数运算，反向传播算法功不可没，可以说，没有反向传播算法，深度学习就不可能得以快速发展，因此在此之前，有必要了解一下反向传播算法的具体原理和公式推导。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 二、梯度下降 首先反向传播算法的核心思路就是梯度下降，那么我们必须要明白什么是梯度，从几何上理解，一个函数（此处默认该函数处处可导）的图像会在其空间内呈现出一个曲面（曲线）。以 \\(f(x) = x^2 + y ^2\\) 为例，该函数会在三维空间（x, y, z）中形成一个曲面，其中，x, y可看作相互独立的两个变量，那么我们分别对x,y求偏导数，会有 \\(\\frac{\\partial f}{\\partial x} = 2x\\)，\\(\\frac{\\partial f}{\\partial y} = 2y\\)，因此，该函数的梯度可以表示为 \\((2x, 2y)\\)，如在坐标（1， 3）处的梯度，代入公式可以得到（2， 6）。该数值表示在x, y构成的平面上，我们首先所处的位置是（1， 3）点，该处的函数值是10。如果我需要以最快的速度增大函数值，那么我需要根据 向量（2，6） 的方向前进。 因此梯度在几何上的直观理解是一个表明方向的向量，只有朝着这个向量所指示的方向前进，函数值才会增加的最快。可以这么说（不算很严谨），梯度所在的空间是所有的自变量构成的空间，并指示着自变量需要变化的方向。 由于梯度指示的是函数值增大最快的方向，那么我们朝着相反的方向前进，函数值也必定会下降最快（所以我们在公式中是减去梯度，而不是加上梯度），这就是梯度下降算法的核心。由于梯度值只在一个很小的范围内近似保持不变，所以我们需要进行迭代，并且需要用一个步长变量来控制下降的幅度，这个步长变量就是我们经常谈到的学习率。 三、单层全连接层以及单个输出，不使用激活函数 在所有的矩阵相乘的情况中，输入输出之间只有一个全连接层并且该全连接层的输出仅仅是一个常数（或者说是一个1x1大小的矩阵），同时并不使用非线性激活函数的情况是最简单的，因此，首先可以考虑这种最简单的情况。 我们这里假设输入\\(x\\)是一个长度为3的向量，按照Tensor Flow所设定的数据格式，我们这里设定该输入向量为行向量，即\\(x = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix}\\)，输出为一个回归值 \\(\\hat{y}\\)，目标回归值为\\(y\\)，全连接层权值记为 \\(\\omega\\)，（\\(\\omega\\)是一个矩阵，大小为3x1），偏置项记为\\(b\\)，（\\(b\\)也可以视作一个矩阵，大小为1x1，也可以看作是一个数值，因为这里不关心\\(b\\)的维度信息，因此不做严格的区分。）。当我们定义了以上的相关参数之后，就可以进行如下的运算： \\[ x * \\omega + b = \\hat{y} \\tag{1} \\] 即： \\[ \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} * \\begin{bmatrix} \\omega_1 \\\\ \\omega_2 \\\\ \\omega_3 \\end{bmatrix} + b = \\hat{y} \\tag{2} \\] 这里取损失cost的计算方式为差值的平方，即 \\(C = cost(\\hat{y}, y) = (\\hat{y} - y)^2\\)，很显然，我们对 \\(\\hat{y}\\) 计算偏导数（导数）可得到： \\(\\frac{\\partial C}{\\partial \\hat{y}} = 2 (\\hat{y} - y)\\)。 将上面的(2)式展开，可以得到下面的多项式： \\[ \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3 + b = \\hat{y} \\tag{3} \\] 因为我们关心的是 \\(\\omega_1\\)，\\(\\omega_2\\)，\\(\\omega_3\\) 以及 \\(b\\) 的更新梯度，因此我们需要对这四个变量求偏导数。根据 \\(\\hat{y}\\) 的计算公式，我们不难看出这四个变量的偏导数计算公式如下： \\[ \\frac{\\partial \\hat{y}}{\\partial \\omega_1} = x_1，\\frac{\\partial \\hat{y}}{\\partial \\omega_2} = x_2，\\frac{\\partial \\hat{y}}{\\partial \\omega_3} = x_3，\\frac{\\partial \\hat{y}}{\\partial b} = 1 \\tag{4} \\] 本质上，我们需要将\\(C\\)的数值降低到全局最小值（或者局部最小值），因此我们需要根据 \\(\\frac{\\partial C}{\\partial \\omega_1}\\)，\\(\\frac{\\partial C}{\\partial \\omega_2}\\)，\\(\\frac{\\partial C}{\\partial \\omega_3}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\) 这四个参数的梯度信息来更新相关参数的数值。根据求导公式的链式法则（chain rule），我们有： \\[ \\frac{\\partial C}{\\partial \\omega_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_1} = 2 (\\hat{y} - y) \\cdot x_1 \\tag{5} \\] \\[ \\frac{\\partial C}{\\partial \\omega_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_2} = 2 (\\hat{y} - y) \\cdot x_2 \\tag{6} \\] \\[ \\frac{\\partial C}{\\partial \\omega_3} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial \\omega_3} = 2 (\\hat{y} - y) \\cdot x_3 \\tag{7} \\] \\[ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b} = 2 (\\hat{y} - y) \\cdot 1 \\tag{8} \\] 上式(5)~(8)就是我们计算得到的梯度信息，根据梯度信息，我们就可以更新相关的参数了，以下公式中的 \\(\\alpha\\) 表示的是学习率，为人为设置的一个超参数。 \\[ \\omega_1 := \\omega_1 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_1} = \\omega_1 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_1) \\tag{9} \\] \\[ \\omega_2 := \\omega_2 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_2} = \\omega_2 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_2) \\tag{10} \\] \\[ \\omega_3 := \\omega_3 - \\alpha \\cdot \\frac{\\partial C}{\\partial \\omega_3} = \\omega_3 - \\alpha \\cdot (2 (\\hat{y} - y) \\cdot x_3) \\tag{11} \\] \\[ b := b - \\alpha \\cdot \\frac{\\partial C}{\\partial b} = b - \\alpha \\cdot (2 (\\hat{y} - y)) \\tag{12} \\] 以上的公式就已经可以用来进行反向传播，或者说梯度下降了，但是实际上，在代码编写的时候，直接使用上面的公式会显得非常繁琐，因此，我们常常使用上面公式的向量化表达，这样可以使代码编写简洁高效，并且由于numpy等python包对向量和矩阵运算进行了很大程度的优化，因此运算速度也比直接使用上述公式要快。 我们将每个变量的梯度按照次序排好，放入一个矩阵中，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_1} \\\\ \\frac{\\partial C}{\\partial \\omega_2} \\\\ \\frac{\\partial C}{\\partial \\omega_3} \\end{bmatrix} = \\begin{bmatrix} 2(\\hat{y} - y) \\cdot x_1 \\\\ 2(\\hat{y} - y) \\cdot x_2 \\\\ 2(\\hat{y} - y) \\cdot x_3 \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} = x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] 化简之后，\\(\\omega\\) 权值梯度更新的公式如下： \\[ \\frac{\\partial C}{\\partial \\omega} = x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] \\[ \\omega := \\omega - \\alpha * x^T \\begin{bmatrix} 2(\\hat{y} - y) \\end{bmatrix} \\] 四、代码 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npparam = {}nodes = {}learning_rate = 0.001def forward(x): nodes[\"matmul\"] = np.matmul(x, param[\"w\"]) nodes['add'] = nodes['matmul'] + param[\"b\"] return nodes['add']def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def backward(x, y_pred, y): param['w'] -= learning_rate * np.matmul(np.transpose(x), cost_gradient(y_pred, y)) param['b'] -= learning_rate * cost_gradient(y_pred, y)def setup(): param[\"w\"] = np.random.random([3, 1]) param[\"b\"] = np.random.random([1, 1]) x = np.array([[1., 2., 3.]]) y = np.array([[1]]) for i in range(200): y_pred = forward(x) backward(x, y_pred, y) print(\"预测结果：\", y_pred, \" 梯度下降之后：\", forward(x), \" 真实回归值：\", y, \" Loss：\", cost(y_pred, y)) passif __name__ == '__main__': setup() 结果如下： 12345678910111213141516171819202122232425262728293031323334353637预测结果： [[3.35507743]] 梯度下降之后： [[3.28442511]] 真实回归值： [[1]] Loss： 5.546389698793334预测结果： [[3.28442511]] 梯度下降之后： [[3.21589235]] 真实回归值： [[1]] Loss： 5.218598067594647预测结果： [[3.21589235]] 梯度下降之后： [[3.14941558]] 真实回归值： [[1]] Loss： 4.910178921799803预测结果： [[3.14941558]] 梯度下降之后： [[3.08493312]] 真实回归值： [[1]] Loss： 4.619987347521437预测结果： [[3.08493312]] 梯度下降之后： [[3.02238512]] 真实回归值： [[1]] Loss： 4.346946095282921预测结果： [[3.02238512]] 梯度下降之后： [[2.96171357]] 真实回归值： [[1]] Loss： 4.090041581051698预测结果： [[2.96171357]] 梯度下降之后： [[2.90286216]] 真实回归值： [[1]] Loss： 3.8483201236115456......预测结果： [[2.16884003]] 梯度下降之后： [[2.13377483]] 真实回归值： [[1]] Loss： 1.366187026290616预测结果： [[2.13377483]] 梯度下降之后： [[2.09976159]] 真实回归值： [[1]] Loss： 1.2854453730368411预测结果： [[2.09976159]] 梯度下降之后： [[2.06676874]] 真实回归值： [[1]] Loss： 1.2094755514903637预测结果： [[2.06676874]] 梯度下降之后： [[2.03476568]] 真实回归值： [[1]] Loss： 1.137995546397283预测结果： [[2.03476568]] 梯度下降之后： [[2.00372271]] 真实回归值： [[1]] Loss： 1.0707400096052042预测结果： [[2.00372271]] 梯度下降之后： [[1.97361103]] 真实回归值： [[1]] Loss： 1.0074592750375366预测结果： [[1.97361103]] 梯度下降之后： [[1.9444027]] 真实回归值： [[1]] Loss： 0.9479184318828179预测结果： [[1.9444027]] 梯度下降之后： [[1.91607062]] 真实回归值： [[1]] Loss： 0.8918964525585433预测结果： [[1.91607062]] 梯度下降之后： [[1.8885885]] 真实回归值： [[1]] Loss： 0.8391853722123339......预测结果： [[1.6356086]] 梯度下降之后： [[1.61654034]] 真实回归值： [[1]] Loss： 0.40399829055941733预测结果： [[1.61654034]] 梯度下降之后： [[1.59804413]] 真实回归值： [[1]] Loss： 0.38012199158735577预测结果： [[1.59804413]] 梯度下降之后： [[1.58010281]] 真实回归值： [[1]] Loss： 0.3576567818845428预测结果： [[1.58010281]] 梯度下降之后： [[1.56269972]] 真实回归值： [[1]] Loss： 0.3365192660751663预测结果： [[1.56269972]] 梯度下降之后： [[1.54581873]] 真实回归值： [[1]] Loss： 0.316630977450124预测结果： [[1.54581873]] 梯度下降之后： [[1.52944417]] 真实回归值： [[1]] Loss： 0.29791808668282155预测结果： [[1.52944417]] 梯度下降之后： [[1.51356084]] 真实回归值： [[1]] Loss： 0.2803111277598668预测结果： [[1.51356084]] 梯度下降之后： [[1.49815402]] 真实回归值： [[1]] Loss： 0.2637447401092591......预测结果： [[1.00722162]] 梯度下降之后： [[1.00700497]] 真实回归值： [[1]] Loss： 5.215181191515595e-05预测结果： [[1.00700497]] 梯度下降之后： [[1.00679482]] 真实回归值： [[1]] Loss： 4.906963983096906e-05预测结果： [[1.00679482]] 梯度下降之后： [[1.00659098]] 真实回归值： [[1]] Loss： 4.616962411696138e-05预测结果： [[1.00659098]] 梯度下降之后： [[1.00639325]] 真实回归值： [[1]] Loss： 4.344099933164832e-05预测结果： [[1.00639325]] 梯度下降之后： [[1.00620145]] 真实回归值： [[1]] Loss： 4.087363627114841e-05预测结果： [[1.00620145]] 梯度下降之后： [[1.00601541]] 真实回归值： [[1]] Loss： 3.84580043675206e-05预测结果： [[1.00601541]] 梯度下降之后： [[1.00583495]] 真实回归值： [[1]] Loss： 3.618513630940355e-05预测结果： [[1.00583495]] 梯度下降之后： [[1.0056599]] 真实回归值： [[1]] Loss： 3.4046594753515e-05预测结果： [[1.0056599]] 梯度下降之后： [[1.0054901]] 真实回归值： [[1]] Loss： 3.203444100358307e-05预测结果： [[1.0054901]] 梯度下降之后： [[1.0053254]] 真实回归值： [[1]] Loss： 3.0141205540271894e-05 可以发现，经过梯度下降之后，预测的回归值逐渐接近真实的回归值，loss也一直在不断降低，证明我们的算法是正确的。","link":"/2019/05/12/Article6BackProp-1/"},{"title":"步长stride为s的二维卷积方法的反向传播算法","text":"前言 在之前讨论了步长stride为1的卷积方式的反向传播，但是很多时候，使用的卷积步长会大于1，这个情况下的卷积方式的反向传播和步长为1的情况稍稍有些区别，不过区别并没有想象中那么大，因此下面就对步长stride大于1的情况进行简单的阐述。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，不涉及十分复杂的公式推导，争取可以以一种简单的方式来理解卷积的反向传播。希望可以很好的帮助理解反向传播算法。 需要注意的是，在本文中，所有的正向传播过程中，卷积的步长stride均固定为2。 一，参数设置 这里我们设置我们的数据矩阵（记作\\(x\\)）大小为5x5，卷积核（记作\\(k\\)）大小为3x3，由于步长是2，因此，卷积之后获得的结果是一个2x2大小的数据矩阵（不妨我们记作\\(u\\)）。偏置项我们记为\\(b\\)，将和卷积之后的矩阵进行相加。 我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为5x5 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 设置为2 padding VALID 偏置项\\(b\\) 一个浮点数 和前面一样，我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为（需要注意的是这里步长选取为2）： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} \\] 将矩阵\\(u\\)进一步展开，我们有： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 3}k_{1, 1} + x_{1, 4}k_{1, 2} +x_{1, 5}k_{1, 3} + \\\\ x_{2, 3}k_{2, 1} + x_{2, 4}k_{2, 2} +x_{2, 5}k_{2, 3} + \\\\ x_{3, 3}k_{3, 1} + x_{3, 4}k_{3, 2} +x_{3, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{3, 1}k_{1, 1} + x_{3, 2}k_{1, 2} +x_{3, 3}k_{1, 3} + \\\\ x_{4, 1}k_{2, 1} + x_{4, 2}k_{2, 2} +x_{4, 3}k_{2, 3} + \\\\ x_{5, 1}k_{3, 1} + x_{5, 2}k_{3, 2} +x_{5, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 3}k_{1, 1} + x_{3, 4}k_{1, 2} +x_{3, 5}k_{1, 3} + \\\\ x_{4, 3}k_{2, 1} + x_{4, 4}k_{2, 2} +x_{4, 5}k_{2, 3} + \\\\ x_{5, 3}k_{3, 1} + x_{5, 4}k_{3, 2} +x_{5, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差传递 步长为2的二维卷积已经在上面的式子中被完整的表示出来了，因此，下一步就是需要对误差进行传递，和前面步长为1的情况一样，我们可以将上面的结果保存在一张表格中，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。于是，表格如下： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{i, j}}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} k_{1, 1}\\) \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} k_{1, 2}\\) \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) \\(k_{1, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} k_{1, 3} + \\delta_{1, 2}k_{1, 1}\\) \\(\\partial x_{1, 4}\\) 0 \\(k_{1, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 4}} = \\delta_{1, 2}k_{1, 2}\\) \\(\\partial x_{1, 5}\\) 0 \\(k_{1, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 5}} = \\delta_{1, 2}k_{1, 3}\\) \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} k_{2, 1}\\) \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} k_{2, 2}\\) \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) \\(k_{2, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} k_{1, 3} + \\delta_{1, 2}k_{2, 1}\\) \\(\\partial x_{2, 4}\\) 0 \\(k_{2, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 4}} = \\delta_{1, 2}k_{2, 2}\\) \\(\\partial x_{2, 5}\\) 0 \\(k_{2, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 5}} = \\delta_{1, 2}k_{2, 3}\\) \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 \\(k_{1, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1}k_{3, 1} + \\delta_{2, 1}k_{1, 1}\\) \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) 0 \\(k_{1, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1}k_{3, 2} + \\delta_{2, 1}k_{1, 2}\\) \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) \\(k_{3, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 1}\\) \\(\\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1}k_{3, 3} + \\delta_{1, 2}k_{3, 1} + \\delta_{2, 1}k_{1, 3} + \\delta_{2, 2}k_{1, 1}\\) \\(\\partial x_{3, 4}\\) 0 \\(k_{3, 2}\\) 0 \\(k_{1, 2}\\) \\(\\frac{\\partial L}{\\partial x_{3, 4}} = \\delta_{1, 2}k_{3, 2} + \\delta_{2, 2}k_{1, 2}\\) \\(\\partial x_{3, 5}\\) 0 \\(k_{3, 3}\\) 0 \\(k_{1, 3}\\) \\(\\frac{\\partial L}{\\partial x_{3, 5}} = \\delta_{1, 2}k_{3, 3} + \\delta_{2, 2}k_{1, 3}\\) \\(\\partial x_{4, 1}\\) 0 0 \\(k_{2, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{4, 1}} = \\delta_{2, 1}k_{2, 1}\\) \\(\\partial x_{4, 2}\\) 0 0 \\(k_{2, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{4, 2}} = \\delta_{2, 1}k_{2, 2}\\) \\(\\partial x_{4, 3}\\) 0 0 \\(k_{2, 3}\\) \\(k_{2, 1}\\) \\(\\frac{\\partial L}{\\partial x_{4, 3}} = \\delta_{2, 1}k_{2, 3} + \\delta_{2, 2}k_{2, 1}\\) \\(\\partial x_{4, 4}\\) 0 0 0 \\(k_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{4, 4}} = \\delta_{2, 2}k_{2, 2}\\) \\(\\partial x_{4, 5}\\) 0 0 0 \\(k_{2, 3}\\) \\(\\frac{\\partial L}{\\partial x_{4, 5}} = \\delta_{2, 2}k_{2, 3}\\) \\(\\partial x_{5, 1}\\) 0 0 \\(k_{3, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{5, 1}} = \\delta_{2, 1}k_{3, 1}\\) \\(\\partial x_{5, 2}\\) 0 0 \\(k_{3, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{5, 2}} = \\delta_{2, 1}k_{3, 2}\\) \\(\\partial x_{5, 3}\\) 0 0 \\(k_{3, 3}\\) \\(k_{3, 1}\\) \\(\\frac{\\partial L}{\\partial x_{5, 3}} = \\delta_{2, 1}k_{3, 3} + \\delta_{2, 2}k_{3, 1}\\) \\(\\partial x_{5, 4}\\) 0 0 0 \\(k_{3, 2}\\) \\(\\frac{\\partial L}{\\partial x_{5, 4}} = \\delta_{2, 2}k_{3, 2}\\) \\(\\partial x_{5, 5}\\) 0 0 0 \\(k_{3, 3}\\) \\(\\frac{\\partial L}{\\partial x_{5, 5}} = \\delta_{2, 2}k_{3, 3}\\) 可以看出，数据依然都是很规律的进行着重复。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。那么，根据求导的链式法则，我们有： 根据求偏导数的链式法则，我们可以有： \\[ \\frac{\\partial L}{\\partial x_{i, j}} = \\sum_{p = 1} \\sum_{k = 1} \\frac{\\partial L}{\\partial u_{p, k}} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{i, j}} = \\sum_{p = 1} \\sum_{k = 1} \\delta_{p, k} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{i, j}} \\] 我们以\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)为例，我们有： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_{3, 3}} &amp;= \\sum_{p = 1} \\sum_{k = 1} \\frac{\\partial L}{\\partial u_{p, k}} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{3, 3}} \\\\ &amp;= \\sum_{p = 1} \\sum_{k = 1} \\delta_{p, k} \\cdot \\frac{\\partial u_{p, k}}{\\partial x_{3, 3}} \\\\ &amp;= \\delta_{1, 1}\\frac{\\partial u_{1, 1}}{\\partial x_{3, 3}} + \\delta_{1, 2}\\frac{\\partial u_{1, 2}}{\\partial x_{3, 3}} + \\delta_{2, 1}\\frac{\\partial u_{2, 1}}{\\partial x_{3, 3}} + \\delta_{2, 2}\\frac{\\partial u_{2, 2}}{\\partial x_{3, 3}} \\\\ &amp;= \\delta_{1, 1}k_{3, 3} + \\delta_{1, 2}k_{3, 1} + \\delta_{2, 1}k_{1, 3} + \\delta_{2, 2}k_{1, 1} \\end{aligned} \\] 类似地，我们可以计算出所有的输入矩阵中的元素所对应的偏导数信息，所有的偏导数计算结果均在上表中列出。 和前面步长stride为1的卷积方式的误差传递类似，我们需要对传递来的误差矩阵和卷积核进行一定的处理，然后再进行卷积，得到应该传递到下一层的网络结构中，所以我们需要的解决问题的问题有三个，即：1.误差矩阵如何处理，2.卷积核如何处理，3.如何进行卷积。 同样，我们将\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)单独拿出来进行考察，如果需要用到全部的卷积核的元素的话，并不能和传递来的误差矩阵相匹配，为了使得两者可以再维度上相匹配，我们再误差矩阵中添加若干0，和步长stride为1的卷积反向传播一样，我们也将卷积核进行180°翻转，于是，我们可以得到： \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 由于padding策略一直默认为是VALID，而且上面的两个矩阵形状相同，所以此时的步长stride参数不会影响到最终的结果。 如果按照我们之前的策略，再在添加0之后的误差矩阵外面填补上合适数目的0的话，有： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 同样，上面的卷积过程步长stride参数为1。 不妨将上面的卷积的结果记为\\(conv1\\)，然后，我们将\\(\\frac{\\partial L}{\\partial x_{i, j}}\\)按照对应的顺序进行排列，我们将结果记作\\(conv2\\)，即： \\[ conv2 = \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} \\] 经过计算，我们发现\\(conv1\\)和\\(conv2\\)正好相等。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv \\;\\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 可以发现，在前面提出的三个问题中，有两个问题的答案是和步长stride为1的二维卷积相同的，唯一不同的是，我们需要在误差矩阵的相邻元素之间插入若干0来完成卷积误差的产生。 误差矩阵插入0的方式 很明显，我们唯一需要解决的问题就是如何在误差矩阵中插入0。在这里直接给出结论，那就是每个相邻的元素之间应该插入（步长stride - 1）个0，或者说每个元素之间的距离是卷积的步长。因为在这个模型中，唯一和前面的卷积方式不同的变量就是步长stride，那么需要满足的条件也必然和步长有关。 当我们在元素之间插入合适数目的0之后，接下来就是在误差矩阵周围填补上合适数目的0层，然后将卷积核旋转180°，最后按照步长为1的方式进行卷积，最后得到应该向前传递的误差矩阵。 这两步和步长stride为1的反向传播算法相同。 三、参数更新 当我们解决了误差的向前传递之后，下一步就是解决参数的更新的问题。和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， 即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以有下面的式子：这里以求解\\(\\frac{\\partial L}{\\partial k_{1, 1}}\\) 为例： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial k_{1, 1}} =&amp; \\frac{\\partial L}{\\partial u_{1, 1}} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 2}} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 1}} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 2}} \\frac{\\partial u_{2, 2}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\delta_{1, 2} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\delta_{2, 1} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\delta_{2, 2} \\frac{\\partial u_{2, 2}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 3} \\end{aligned} \\] 类似地，我们将所有地偏导数信息都求出来，汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = \\delta_{1, 1} x_{1, 2} + \\delta_{1, 2} x_{1, 4} + \\delta_{2, 1} x_{3, 2} + \\delta_{2, 2} x_{3, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = \\delta_{1, 1} x_{1, 3} + \\delta_{1, 2} x_{1, 5} + \\delta_{2, 1} x_{3, 3} + \\delta_{2, 2} x_{3, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = \\delta_{1, 1} x_{2, 1} + \\delta_{1, 2} x_{2, 3} + \\delta_{2, 1} x_{4, 1} + \\delta_{2, 2} x_{4, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = \\delta_{1, 1} x_{2, 2} + \\delta_{1, 2} x_{2, 4} + \\delta_{2, 1} x_{4, 2} + \\delta_{2, 2} x_{4, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = \\delta_{1, 1} x_{2, 3} + \\delta_{1, 2} x_{2, 5} + \\delta_{2, 1} x_{4, 3} + \\delta_{2, 2} x_{4, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = \\delta_{1, 1} x_{3, 1} + \\delta_{1, 2} x_{3, 3} + \\delta_{2, 1} x_{5, 1} + \\delta_{2, 2} x_{5, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = \\delta_{1, 1} x_{3, 2} + \\delta_{1, 2} x_{3, 4} + \\delta_{2, 1} x_{5, 2} + \\delta_{2, 2} x_{5, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = \\delta_{1, 1} x_{3, 3} + \\delta_{1, 2} x_{3, 5} + \\delta_{2, 1} x_{5, 3} + \\delta_{2, 2} x_{5, 5} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1} + \\delta_{1, 2} + \\delta_{2, 1} + \\delta_{2, 2} \\] 和前面地误差传递类似，我们发现可以在误差矩阵中插入若干个0来和输入矩阵\\(x\\)来保持维度上的匹配。即有： \\[ \\frac{\\partial L}{\\partial k} = [\\frac{\\partial L}{\\partial k_{i, j}}] = \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 据此，我们可以发现，在卷积核参数更新的过程中，我们也需要对误差矩阵进行插入0的操作。而且插入0的方式和误差传递过程中的方式完全相同。所以，我们可以总结出步长为s的时候卷积反向传播的卷积核参数更新的方法，即：1.首先在接收到的误差矩阵中插入合适数目的0，2.在输入矩阵\\(x\\)上应用误差矩阵进行步长为1的卷积，从而得到卷积核的更新梯度。 同样，我们由上面的推导可以发现，无论是何种方式的卷积操作，偏置项\\(b\\)的更新梯度都是接收到的误差矩阵中的元素之和。 四、总结 我们将上面的求解过程总结如下有： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵 输入卷积核\\(k\\) 一个二维矩阵 步长\\(stride\\) 一个正整数s padding VALID 偏置项\\(b\\) 一个浮点数 正向传播： 1conv(x, kernel, bias, &quot;VALID&quot;) 反向传播： 12345678910111213141516conv_backward(error, x, kernel, bias): # 计算传递给下一层的误差 1.在接收到的error矩阵的矩阵中插入合适数目的0，使得每个元素之间的0的数目为(stride - 1) 2.在error周围填补上合适数目的0 3.将kernel旋转180° 4.将填补上0的误差和旋转之后的kernel进行步长为1的卷积，从而得到传递给下一层的误差new_error。 # 更新参数 1.在接收到的error矩阵的矩阵中插入合适数目的0，使得每个元素之间的0的数目为(stride - 1) 2.将输入矩阵x和插入0之后的误差矩阵error进行步长为1的卷积，得到kernel的更新梯度 3.将上一层传递来的误差矩阵error所有元素求和，得到bias的更新梯度 4.kernel := kernel - 学习率 * kernel的更新梯度 5.bias := bias - 学习率 * bias的更新梯度 # 返回误差，用以传递到下一层 return new_error","link":"/2019/05/24/Article15ConvBackProp-part2/"},{"title":"奇异值分解SVD","text":"前言 奇异值分解(Singular Value Decomposition，以下简称SVD)是在一种十分经典的无监督的机器学习算法，它可以用于处理降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。 特征值和特征向量 再了解SVD之前，对于矩阵的特征值和特征向量还是有必要做一个简单的回顾。 假设矩阵\\(A\\)是一个实对称矩阵，大小为\\(n \\times n\\)，如果有一个实数\\(\\lambda\\)和一个长度为\\(n\\)的向量\\(\\alpha\\)，满足下面的等式关系： \\[ A \\alpha = \\lambda \\alpha \\tag{1} \\] 我们就将\\(\\lambda\\)称为矩阵\\(A\\)的特征值，将\\(\\alpha\\)称为矩阵\\(A\\)的特征向量，是一个列向量。 由线性代数的相关知识我们可以知道，对于一个\\(n\\)阶的实对称矩阵，一定存在\\(n\\)个相互正交的特征向量。对于每一个特征向量，我们可以将其规范化，使其模长\\(|\\alpha|\\)为1。于是，我们有： \\[ \\alpha_i^T \\alpha_i = 1 \\\\ \\alpha_i^T \\alpha_j = 0, \\quad i \\ne j \\] 我们将所有的特征值按照从大到小的顺序进行排列，并将对应的特征的特征向量也按照特征值的大小进行排列，于是，我们会有以下的个排列： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\\\ \\alpha_1, \\alpha_2, \\cdots, \\alpha_n \\] 上面的所有的特征值可以依次被放入一个$ n n\\(的对角矩阵的对角线元素中，我们不妨称这个对角矩阵为\\)​$，于是我们有： \\[ \\Sigma = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ddots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ddots &amp; \\lambda_n \\\\ \\end{bmatrix} \\] 同样，我们也将上述的特征向量按照特征值的顺序进行排列，也组成了一个\\(n \\times n\\)的矩阵，不妨记作\\(W\\)，于是有： \\[ W = [\\alpha_1, \\alpha_2, \\cdots, \\alpha_n] \\] 综上，我们可以得到如下的关系式： \\[ A W = W \\Sigma \\tag{2} \\] 在等式的两边同时乘以\\(W^{-1}\\)，我们可以得到： \\[ A = W \\Sigma W^{-1} \\tag{3} \\] 考虑到特征向量彼此两两正交，模长为1，我们就有： \\[ W W^T = E \\tag{4} \\] 即： \\[ W^{-1} = W^T \\tag{5} \\] 将式(5)代入式(3)中，我们可以得到： \\[ A = W \\Sigma W^{-1} = W \\Sigma W^T \\tag{6} \\] 这是一个非常重要的关系，在前面我们就是利用这个关系来计算主成分分析PCA的。 奇异值分解SVD 在前面已经找到了一个重要的关系，在我们之前的讨论过程中，我们已经知道满足这个条件的矩阵的要求是该矩阵是一个实对称矩阵，如果只是一个普通的矩阵\\(A_{m \\times n}\\)，我们其实也可以通过类似的方法进行矩阵分解，这时候我们需要构造的矩阵等式如下： \\[ A = U \\Sigma V^T \\tag{7} \\] 其中，\\(U\\)是一个大小为$m m \\(的矩阵，\\)\\(是一个大小为\\)m n\\(的矩阵，\\)V^T\\(是一个大小为\\)n n$的矩阵。 矩阵\\(U\\) 的求解 定义了相关的矩阵等式关系之后，我们既需要求解其中的每一个矩阵的具体数值。首先来求解矩阵\\(U​\\)的数值。 首先构造矩阵\\(AA^T\\)，该矩阵是一个大小为\\(m \\times m\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_m \\\\ u_1, u_2, \\cdots, u_m \\] 其中每一对特征值和特征向量都满足： \\[ (AA^T) u_i = \\lambda_i u_i \\tag{7} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(U\\)， 即： \\[ U = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\] 矩阵\\(V\\) 的求解 和求解矩阵\\(U\\)类似，首先构造矩阵\\(A^T A\\)，该矩阵是一个大小为\\(n \\times n\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\sigma_1, \\sigma_2, \\cdots, \\sigma_n \\\\ v_1, v_2, \\cdots, v_n \\] 其中每一对特征值和特征向量都满足： \\[ (A^TA) v_i = \\sigma_i v_i \\tag{8} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(V​\\)， 即： \\[ V = \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} \\] 矩阵\\(\\Sigma\\) 的求解 当求出矩阵\\(U​\\)和矩阵\\(V​\\)时，我们就可以求出矩阵\\(\\Sigma​\\)了： \\[ U \\Sigma V^T = A\\\\ \\Sigma = U^{-1} A (V^T)^{-1} \\] 考虑到矩阵\\(U\\)和矩阵\\(V\\)的特殊性质，即\\(U^{-1} = U^T\\)，\\(V^{-1} = V^T\\)，代入上面的式子中，我们有： \\[ \\Sigma = U^T A V \\tag{9} \\] 原理 其实，以上的步骤并不是随便得来的，是有严格证明的，下面以证明矩阵\\(V​\\)为例进行说明。 首先，我们等式\\(A = U \\Sigma V^T\\)左右两边分别求矩阵转置，得到： \\[ A^T = V \\Sigma^T U^T \\] 接着有： \\[ \\begin{aligned} A^T A &amp;= (V \\Sigma^T U^T)(U \\Sigma V^T) \\\\ &amp;= V \\Sigma^T U^T U \\Sigma V^T \\\\ &amp;= V \\Sigma^T \\Sigma V^T \\\\ &amp;= V (U^T A V)^T (U^T A V) V^T \\\\ &amp;= V (V^T A^T U)(U^T A V) V^T \\\\ &amp;= V V^T A^T U U^T A V V^T \\\\ &amp;= A^T A \\end{aligned} \\] 注意到上面证明过程的第三行，我们有$A^T A = V ^T V^T \\(，而矩阵\\)V\\(是矩阵\\)A^T A\\(的特征向量组成的矩阵，因此，矩阵\\)^T \\(就是矩阵\\)A^T A\\(的特征值组成的对角矩阵，这又给了我们一个新的求解\\)$矩阵的方法。 当我们求出所有的 \\(A^T A\\) 矩阵的特征值之后，我们在这些特征值的基础上进行开方操作，并将结果组成一个对角矩阵，这个对角矩阵就是我们需要的 \\(\\Sigma\\) 矩阵。 所以我们发现，实际上\\(\\Sigma\\)是一个对角矩阵，只在对角线上存在着有效元素，其他部位的元素都为0。我们将矩阵\\(\\Sigma\\)的对角线上的元素称为奇异值。 同样的道理，如果我们计算\\(A A^T\\)，我们也能得到相同的结果，而计算过程就是证明矩阵\\(U\\)的正确性的过程。 接下来，我们考虑这样一件事，我们已经求解出了所有的矩阵信息，有\\(A = U \\Sigma V^T\\)，我们同时在等式的左右两边乘以\\(V\\)，有： \\[ AV = U \\Sigma V^T V = U \\Sigma \\] 即： \\[ A \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\begin{bmatrix} a_1 &amp; \\; &amp; \\; \\\\ \\; &amp; a_2 \\; &amp; \\\\ \\; &amp; \\; &amp; \\ddots \\end{bmatrix}_{m \\times n} \\] 其中，\\(a_i\\) 表示的是 \\(\\Sigma\\) 矩阵的对角线元素，所以有： \\[ A v_i = u_i a_i \\] 故： \\[ a_i = \\frac{A v_i}{u_i} \\tag{9} \\] 上述也是一种求解\\(\\Sigma\\)矩阵的方法。 SVD的使用 接下来，我们通过一个简单的例子来实际使用以下SVD。假设我们的数据矩阵\\(A\\)如下： \\[ A = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] 接下来我们计算\\(A^T A\\)和\\(A A^T\\)，有： \\[ AA^T = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\\\ A^TA = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 2 \\\\2 &amp; 5 \\end{bmatrix} \\] 对于矩阵\\(AA^T\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\quad \\lambda_1 = 6, u_1 = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{30}} \\\\ \\frac{2}{\\sqrt{30}} \\end{bmatrix} \\quad \\lambda_2 = 1, u_2 = \\begin{bmatrix} 0 \\\\ \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\quad \\lambda_3 = 0, u_3 = \\begin{bmatrix} -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{bmatrix} \\] 对于矩阵\\(A^TA\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\lambda_1 = 6, v_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}}\\end{bmatrix} \\quad \\lambda_2 = 1, v_2 = \\begin{bmatrix} \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}}\\end{bmatrix} \\] 由上面的特征值信息，我们可以求出矩阵\\(\\Sigma\\)的对角线元素，即直接在特征值的基础上取根号即可，依次为： \\[ a_1 = \\sqrt{6}, a_2 = 1 \\] 由上面的所有计算结果，我们可以得到矩阵\\(U\\)，\\(V\\)和\\(\\Sigma\\)，如下： \\[ U = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\] \\[ V = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] \\[ \\Sigma = \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\] 于是，我们的原始矩阵\\(A\\)可以分解成如下： \\[ A = U \\Sigma V^T \\] 即： \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] 在前面的这个求解的例子中，我们很难发现SVD究竟有什么用处，毕竟求解矩阵乘积，矩阵的特征值和特征向量都是十分复杂是过程。实际上我们在求解奇异值的时候，可以利用矩阵的特征值进行求解，这样奇异值就和特征值具有相同的特点，如果按照从大到小的顺序排列奇异值，我们发现奇异值的数值下降很快，在有些情况下，奇异值前10%~15%的数值之和已经占据了全部奇异值数值之和的90%以上，而过于小的奇异值我们可以将其省略，因为奇异值太小，对最后的生成矩阵影响就会非常小，可以直接忽略不计。当我们采用这种策略的时候，我们可以适当的选取数值最大的几个奇异值，并选取对应的特征向量，这个时候我们就可以只是用这些数据来重构我们的原始矩阵。 下面是一个具体的使用SVD重构数据矩阵的python代码实例，数据来源为《机器学习实战》一书。在代码中，我们试图去利用SVD重构一个01矩阵组成的数字0。 数据如下： 12345678910111213141516171819202122232425262728293031320000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)# 调整特征向量，使得所有向量的分量之和为正数。def adjust(vectors): values = np.sum(vectors, axis=0) for i, v in enumerate(values): if v &lt; 0: vectors[:, i] *= -1 return vectors# 利用numpy的求解特征值和特征向量的函数进行求解。def eig(matrix): values, vectors = np.linalg.eig(matrix) # 有时候函数会返回复数值，这个时候我们只需要取其实数部分。 values = np.real(values) vectors = np.real(vectors) # 按照特征值的大小进行排序。 index = np.argsort(values)[::-1] values = values[index] vectors = adjust(vectors[:, index]) # 由于求解过程存在一定的误差，因此特征值会出现极小的负数，我们可以直接将其置为0。 values = np.maximum(values, 0) return values, vectorsdef svd(matrix): # 返回左侧矩阵的特征值和特征向量 left_values, left_vectors = eig(np.matmul(matrix, np.transpose(matrix))) # 返回右侧矩阵的特征值和特征向量 right_values, right_vectors = eig(np.matmul(np.transpose(matrix), matrix)) # Sigma矩阵 sigma = np.zeros_like(matrix, dtype=np.float64) for i in range(min(len(left_values), len(right_values))): sigma[i][i] = math.sqrt(left_values[i]) printMat(np.matmul(np.matmul(left_vectors, sigma), np.transpose(right_vectors))) pass# 输出，这里借鉴了《机器学习实战》一书的输出模板def printMat(inMat, thresh=0.6): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(\"1\", end=\"\") else: print(\"0\", end=\"\") print()if __name__ == '__main__': matrix = load_data() svd(matrix) 结果如下： 12345678910111213141516171819202122232425262728293031320000000000000000001000000000000000000000000011011110000000000000000000000001111111100000000000000000000000111111111100000000000000000000000111111110111010000000000000000011011111111111100000000000000001111111111111101000000000000000001111000001111110000000000000000011110000011111100000000000001111110000000011110000000000000011111100000000111100000000000000111111000000000111100000000000001111110000000001111000000000000000111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000011110000000000000011111000000011110100000000000000111111000001111110000000000000001111111111111111000000000000000011111111111110000000000000000000111111111111100000000000000000000000111111111000000000000000000000001111111111000000000000000000000000111111000000000000 可以看到，我们自己实现的SVD可以较好地重构出原始数据矩阵。由于求解过程中的误差，我们的重构结果和实际的数据矩阵还是有些不同的，所以，numpy已经将SVD进行了封装，我们可以直接使用其封装好的函数进行求解SVD，这样求出的结果会比自己手动求解的结果好。 使用numpy中的SVD的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)def printMat(inMat, thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(1, end=\"\") else: print(0, end=\"\") print()def imgCompress(numSV=5, thresh=0.8): matrix = load_data() print(\"****original matrix******\") printMat(matrix, thresh) U, Sigma, VT = np.linalg.svd(matrix) SigRecon = np.zeros((numSV, numSV)) for k in range(numSV): # construct diagonal matrix from vector SigRecon[k, k] = Sigma[k] reconMat = np.matmul(np.matmul(U[:, :numSV], SigRecon), VT[:numSV, :]) print(\"****reconstructed matrix using %d singular values******\" % numSV) printMat(reconMat, thresh)if __name__ == '__main__': imgCompress() 结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566****original matrix******0000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000****reconstructed matrix using 5 singular values******0000000000000000000000000000000000000000000000111100000000000000000000000000001111100000000000000000000000001111111000000000000000000000111111111111000000000000000000001111111111111000000000000000000011111111111111000000000000000000111111000001111000000000000000001111100000001110000000000000000111110000000001110000000000000011111100000000011110000000000000111111000000000111100000000000001111110000000001111000000000000000111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011110000000000000011111000000000111100000000000000111110000000001111000000000000001111100000000011110000000000000011111000000000111000000000000000111110000000111100000000000000000011111111111111100000000000000000011111111111111000000000000000000111111111111110000000000000000000111111111100000000000000000000001111111110000000000000000000000000111110000000000000 可以发现，当我们只使用5个奇异值以及其对应的特征向量的时候，我们就可以获得非常好的重构效果，这个时候，我们的数据量为： \\[ s = 32 * 5 + 5 + 5 * 32 = 325 \\\\ compress\\_rate = \\frac{325}{32 * 32} = 37.1\\% \\] 我们的数据量仅仅是原来的三分之一，因此可以SVD很好的进行数据压缩。 SVD和PCA 在求解SVD的过程中，我们求解了两个特殊的矩阵\\(U\\) 和\\(V\\) ，实际上，\\(U\\) 被称为是左奇异矩阵，\\(V\\) 被称为是右奇异矩阵。 回想起在求解PCA的过程中，我们也需要求解右奇异矩阵，并利用右奇异矩阵进行了数据的降维，这就是右奇异矩阵的作用之一。当我们考察左奇异矩阵的时候，发现合理的使用右奇异矩阵本质上是对数据量进行了一定的减小，当我们拥有一个矩阵\\(A\\)，大小为\\(m \\times n\\)，其中，\\(m\\) 表示的是数据量，即行数，\\(n\\) 表示的是数据的维度数目，即列数，当我们对数据矩阵右乘右奇异矩阵的时候，列数可以根据选择的特征向量的数目进行减小，当我们当数据矩阵左乘左奇异矩阵的时候，行数可以根据选择的特征向量的数目进行减小。因此，左奇异矩阵可以压缩行数，右奇异矩阵可以压缩列数，综合在一起就可以对数据进行较为全面的压缩。","link":"/2019/05/10/Article4SVD/"},{"title":"反向传播算法（二）之稍复杂的反向传播","text":"前言 前面介绍了单层全连接层，并且没有使用激活函数，这种情况比较简单，这一篇文章打算简单介绍一下多个输出，以及使用激活函数进行非线性激活的情况。还是请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 一、参数设置 和前面一样，这里使用的是长度为3的行向量，即 \\(x = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix}\\)，输出这里设置为长度为2的行向量，即 \\(\\hat{y} = \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix}\\)。权值参数我们记为 \\(\\omega\\)，偏置量我们记为 \\(b\\)，由于这里我们模拟的是进行分类操作，因此这里引入了一个非线性激活函数 \\(g\\)，为了方便我们进行求导，我们这里设置激活函数为sigmoid，即: \\[g(x) = \\frac{1}{1 + e^{-x}}, g\\prime(x) = g(x)(1 - g(x)) \\tag{1}\\] 有了上述的参数设置，我们可以有下面的式子： \\[ g(x \\omega + b) = \\hat{y} \\tag{2} \\] 继续将式子展开，我们有： \\[ g(\\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{21} &amp; \\omega_{22} \\\\ \\omega_{31} &amp; \\omega_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2\\end{bmatrix}) = \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix} \\tag{3} \\] 三、首先不考虑激活函数 我们首先不考虑激活函数，因此，我们可以暂时将结果记为 \\(a = \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}\\)。于是，我们可以得到下面的式子： \\[ \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix} = \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \\end{bmatrix} \\begin{bmatrix} \\omega_{11} &amp; \\omega_{12} \\\\ \\omega_{21} &amp; \\omega_{22} \\\\ \\omega_{31} &amp; \\omega_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b_1 &amp; b_2\\end{bmatrix} \\tag{4} \\] 将上面的公式(4)完全展开，可以得到下面的两个式子： \\[ a_1 = \\omega_{11} x_1 + \\omega_{21} x_2 + \\omega_{31} x_3 + b_1 \\tag{5} \\] \\[ a_2 = \\omega_{12} x_1 + \\omega_{22} x_2 + \\omega_{32} x_3 + b_2 \\tag{6} \\] 和前面的情况类似，我们可以对上面的两个式子中的参数求偏导数，于是，我们得到对于各个参数的偏导数计算公式如下： \\[ \\frac{\\partial a_1}{\\partial \\omega_{11}} = x_1, \\frac{\\partial a_1}{\\partial \\omega_{21}} = x_2, \\frac{\\partial a_1}{\\partial \\omega_{31}} = x_3, \\frac{\\partial a_1}{\\partial b_1} = 1 \\tag{7} \\] \\[ \\frac{\\partial a_2}{\\partial \\omega_{12}} = x_1, \\frac{\\partial a_2}{\\partial \\omega_{22}} = x_2, \\frac{\\partial a_2}{\\partial \\omega_{32}} = x_3, \\frac{\\partial a_2}{\\partial b_2} = 1 \\tag{8} \\] 以上就是现阶段的偏导数的计算公式。下一阶段我们将激活函数也考虑进来。 四、将激活函数也考虑进来 这一阶段我们考虑对 \\(a = \\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}\\) 使用非线性激活函数激活，即我们有： \\[ \\hat{y} = g(a) \\tag{9} \\] 展开之后就变成： \\[ \\begin{bmatrix} \\hat{y}_1 &amp; \\hat{y}_2 \\end{bmatrix} = g(\\begin{bmatrix} a_1 &amp; a_2 \\end{bmatrix}) \\tag{10} \\] 对应每一个元素，我们有： \\[ \\hat{y}_1 = g(a_1), \\hat{y}_2 = g(a_2) \\tag{11} \\] 所以我们求得每一个 \\(\\hat{y}_i\\) 对 \\(a_i\\) 的偏导数如下： \\[ \\frac{\\partial \\hat{y}_1}{\\partial a_1} = g\\prime(a_1), \\frac{\\partial \\hat{y}_2}{\\partial a_2} = g\\prime(a_2) \\tag{12} \\] 五、损失值定义 和前面的情况类似，我们使用输出与目标值之间的差值的平方和作为最后的cost，即： \\[ C = cost = \\sum(\\hat{y}_i - y_i)^2 = (\\hat{y}_1 - y_1)^2 + (\\hat{y}_2 - y_2)^2 \\tag{13} \\] 根据上式，我们可以得到 \\(C\\) 关于两个预测输出 \\(\\hat{y}_1\\)，\\(\\hat{y}_2\\)的偏导数： \\[ \\frac{\\partial C}{\\partial \\hat{y}_1} = 2 * (\\hat{y}_1 - y_1), \\frac{\\partial C}{\\partial \\hat{y}_2} = 2 * (\\hat{y}_2 - y_2) \\tag{14} \\] 六、综合 前面所做的工作实际上是在一步一步求解每一个环节的偏导数公式，根据求导公式的链式法则（chain rule），我们可以得到以下的每一个参数（\\(\\omega\\)，\\(b\\)）对于最后的cost的偏导数公式： \\[ \\frac{\\partial C}{\\partial \\omega_{11}} = \\frac{\\partial a_1}{\\partial \\omega_{11}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_1 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{15} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{21}} = \\frac{\\partial a_1}{\\partial \\omega_{21}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_2 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{16} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{31}} = \\frac{\\partial a_1}{\\partial \\omega_{31}} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = x_3 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{17} \\] \\[ \\frac{\\partial C}{\\partial b_1} = \\frac{\\partial a_1}{\\partial b_1} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial a_1} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_1} = g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\tag{18} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{12}} = \\frac{\\partial a_2}{\\partial \\omega_{12}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_1 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{19} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{22}} = \\frac{\\partial a_2}{\\partial \\omega_{22}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_2 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{20} \\] \\[ \\frac{\\partial C}{\\partial \\omega_{32}} = \\frac{\\partial a_2}{\\partial \\omega_{32}} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = x_3 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{21} \\] \\[ \\frac{\\partial C}{\\partial b_2} = \\frac{\\partial a_2}{\\partial b_2} \\cdot \\frac{\\partial \\hat{y}_2}{\\partial a_2} \\cdot \\frac{\\partial C}{\\partial \\hat{y}_2} = g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\tag{22} \\] 和前面一样，上面的公式已经可以用于进行梯度计算和反向传播了，但是上面的公式看上去不仅繁琐而且容易出错，因此，很有必要对上面的公式进行整理，以便我们用向量和矩阵进行表示和计算。 我们将每个变量的梯度按照次序排好，首先是 \\(\\omega\\) 参数的第一列，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{11}} \\\\ \\frac{\\partial C}{\\partial \\omega_{21}} \\\\ \\frac{\\partial C}{\\partial \\omega_{31}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\\\ x_2 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\\\ x_3 \\cdot g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) \\end{bmatrix} \\tag{23} \\] 接着是 \\(\\omega\\) 参数的第二列，如下： \\[ \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{12}} \\\\ \\frac{\\partial C}{\\partial \\omega_{22}} \\\\ \\frac{\\partial C}{\\partial \\omega_{32}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_1 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\\\ x_2 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\\\ x_3 \\cdot g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\tag{24} \\] 将两个矩阵结合在一起： \\[ \\begin {aligned} \\begin{bmatrix} \\frac{\\partial C}{\\partial \\omega_{11}} &amp; \\frac{\\partial C}{\\partial \\omega_{12}} \\\\ \\frac{\\partial C}{\\partial \\omega_{121}} &amp; \\frac{\\partial C}{\\partial \\omega_{22}} \\\\ \\frac{\\partial C}{\\partial \\omega_{31}} &amp; \\frac{\\partial C}{\\partial \\omega_{32}} \\end{bmatrix} &amp;= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= x^T \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\end{aligned} \\tag{25} \\] 注：上面公式中的 \\(\\cdot*\\) 为向量（矩阵）点乘，即表示向量（矩阵）对应位置的数值分别相乘，和矩阵的相乘不同。 最后化简如下: \\[ \\frac{\\partial C}{\\partial \\omega} = x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\tag{26} \\] 对于偏置量 \\(b\\)，我们计算梯度，然后进行整理，如下：（这里实际上进行了一定的简化，当我们设定偏置量为一个一维数组时，我们需要对下面的结果在列方向上取均值，以保证最后的结果可以和偏置量进行维度上的匹配。） \\[ \\begin{aligned} \\frac{\\partial C}{\\partial b} &amp;= \\begin{bmatrix} \\frac{\\partial C}{b_1} &amp; \\frac{\\partial C}{b_2} \\end{bmatrix} \\\\\\ &amp;= \\begin{bmatrix} g\\prime(a_1) \\cdot 2 \\cdot (\\hat{y}_1 - y_1) &amp; g\\prime(a_2) \\cdot 2 \\cdot (\\hat{y}_2 - y_2) \\end{bmatrix} \\\\\\ &amp;= \\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix} \\end{aligned} \\tag{27} \\] 以上就是单层全连接层，使用激活函数激活的梯度下降公式。 七、代码 这里我使用了两个训练样本，偏置量设定为一个1-D的数组，因此在更新参数时，需要对返回的结果取均值。详细请见backward函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npparam = {}nodes = {}learning_rate = 0.1def sigmoid(x): return 1.0 / (1. + np.exp(- x))def sigmoid_gradient(x): sig = sigmoid(x) return sig * (1. - sig)def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def forward(x): nodes['matmul'] = np.matmul(x, param['w']) nodes['bias'] = nodes['matmul'] + param['b'] nodes['sigmoid'] = sigmoid(nodes['bias']) return nodes['sigmoid']def backward(x, y_pred, y): matrix = np.multiply(sigmoid_gradient(nodes['bias']), cost_gradient(y_pred, y)) matrix2 = np.mean(matrix, 0, keepdims=False) param['w'] -= learning_rate * np.matmul(np.transpose(x), matrix) param['b'] -= learning_rate * matrix2def setup(): x = np.array([[1., 2., 3.], [3., 2., 1.]]) y = np.array([[1., 0.], [0., 1.]]) param['w'] = np.array([[.1, .2], [.3, .4], [.5, .6]]) param['b'] = np.array([0., 0.]) for i in range(1000): y_pred = forward(x) backward(x, y_pred, y) print(\"梯度下降前：\", y_pred, \"\\n梯度下降后：\", forward(x), \"\\ncost：\", cost(forward(x), y))if __name__ == '__main__': setup() 结果如下：可以看见，结果确实是在逐步想着目标结果靠近，cost值不断在减小。证明我们的算法是正确的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161梯度下降前： [[0.90024951 0.94267582] [0.80218389 0.88079708]]梯度下降后： [[0.87638772 0.93574933] [0.74070904 0.87317416]]cost： 1.4556414717601052梯度下降前： [[0.87638772 0.93574933] [0.74070904 0.87317416]]梯度下降后： [[0.84537106 0.92722992] [0.66043307 0.86435062]]cost： 1.3382380273209387梯度下降前： [[0.84537106 0.92722992] [0.66043307 0.86435062]]梯度下降后： [[0.80943371 0.91658752] [0.56909364 0.85403973]]cost： 1.2216201634346886梯度下降前： [[0.80943371 0.91658752] [0.56909364 0.85403973]]梯度下降后： [[0.77530479 0.90307287] [0.48379806 0.84187495]]cost： 1.1250926413642042梯度下降前： [[0.77530479 0.90307287] [0.48379806 0.84187495]]梯度下降后： [[0.74994151 0.88562481] [0.41750738 0.8273968 ]]cost： 1.050964830757349梯度下降前： [[0.74994151 0.88562481] [0.41750738 0.8273968 ]]梯度下降后： [[0.73518018 0.86276177] [0.37075203 0.81005788]]cost： 0.9880224900744513梯度下降前： [[0.73518018 0.86276177] [0.37075203 0.81005788]]梯度下降后： [[0.7288795 0.83251337] [0.33814879 0.78928408]]cost： 0.9253306342190869梯度下降前： [[0.7288795 0.83251337] [0.33814879 0.78928408]]梯度下降后： [[0.72817698 0.7925729 ] [0.31464068 0.76467358]]cost： 0.8564368364354394梯度下降前： [[0.72817698 0.7925729 ] [0.31464068 0.76467358]]梯度下降后： [[0.73084978 0.74107485] [0.29686131 0.73646017]]cost： 0.7792136510576879梯度下降前： [[0.73084978 0.74107485] [0.29686131 0.73646017]]梯度下降后： [[0.73542993 0.67843692] [0.28276129 0.70627592]]cost： 0.6965017597370333梯度下降前： [[0.73542993 0.67843692] [0.28276129 0.70627592]]梯度下降后： [[0.74100699 0.60952933] [0.27110861 0.67770711]]cost： 0.6159759746541653梯度下降前： [[0.74100699 0.60952933] [0.27110861 0.67770711]]梯度下降后： [[0.7470327 0.54300877] [0.2611523 0.65537568]]cost： 0.5458174231304158梯度下降前： [[0.7470327 0.54300877] [0.2611523 0.65537568]]梯度下降后： [[0.75318337 0.48629069] [0.25242337 0.64233397]]cost： 0.48903961977358096梯度下降前： [[0.75318337 0.48629069] [0.25242337 0.64233397]]梯度下降后： [[0.75927196 0.44162035] [0.24462032 0.63846146]]cost： 0.4435277424325401梯度下降前： [[0.75927196 0.44162035] [0.24462032 0.63846146]]梯度下降后： [[0.76519387 0.40729807] [0.23754304 0.64153151]]cost： 0.4059519984224861梯度下降前： [[0.76519387 0.40729807] [0.23754304 0.64153151]]梯度下降后： [[0.77089406 0.38056044] [0.23105412 0.64898998]]cost： 0.3739098246421919梯度下降前： [[0.77089406 0.38056044] [0.23105412 0.64898998]]梯度下降后： [[0.77634715 0.35906729] [0.22505587 0.65883242]]cost： 0.3459953751213052梯度下降前： [[0.77634715 0.35906729] [0.22505587 0.65883242]]梯度下降后： [[0.78154526 0.34118504] [0.21947641 0.66972652]]cost： 0.3213801718742878梯度下降前： [[0.78154526 0.34118504] [0.21947641 0.66972652]]梯度下降后： [[0.78649079 0.32585178] [0.21426107 0.68086606]]cost： 0.29951983756020983......梯度下降前： [[0.97352909 0.02666433] [0.02647091 0.97333567]]梯度下降后： [[0.97354315 0.02664997] [0.02645685 0.97335003]]cost： 0.002820371366327034梯度下降前： [[0.97354315 0.02664997] [0.02645685 0.97335003]]梯度下降后： [[0.97355719 0.02663563] [0.02644281 0.97336437]]cost： 0.002817357738697952梯度下降前： [[0.97355719 0.02663563] [0.02644281 0.97336437]]梯度下降后： [[0.97357121 0.02662131] [0.02642879 0.97337869]]cost： 0.002814350458880144梯度下降前： [[0.97357121 0.02662131] [0.02642879 0.97337869]]梯度下降后： [[0.9735852 0.02660701] [0.0264148 0.97339299]]cost： 0.0028113495069739336梯度下降前： [[0.9735852 0.02660701] [0.0264148 0.97339299]]梯度下降后： [[0.97359917 0.02659274] [0.02640083 0.97340726]]cost： 0.002808354863162452梯度下降前： [[0.97359917 0.02659274] [0.02640083 0.97340726]]梯度下降后： [[0.97361312 0.02657849] [0.02638688 0.97342151]]cost： 0.0028053665077110495梯度下降前： [[0.97361312 0.02657849] [0.02638688 0.97342151]]梯度下降后： [[0.97362705 0.02656426] [0.02637295 0.97343574]]cost： 0.002802384420967047梯度下降前： [[0.97362705 0.02656426] [0.02637295 0.97343574]]梯度下降后： [[0.97364096 0.02655005] [0.02635904 0.97344995]]cost： 0.002799408583359122梯度下降前： [[0.97364096 0.02655005] [0.02635904 0.97344995]]梯度下降后： [[0.97365484 0.02653587] [0.02634516 0.97346413]]cost： 0.002796438975397068梯度下降前： [[0.97365484 0.02653587] [0.02634516 0.97346413]]梯度下降后： [[0.97366871 0.02652171] [0.02633129 0.97347829]]cost： 0.002793475577671274梯度下降前： [[0.97366871 0.02652171] [0.02633129 0.97347829]]梯度下降后： [[0.97368255 0.02650757] [0.02631745 0.97349243]]cost： 0.0027905183708523346梯度下降前： [[0.97368255 0.02650757] [0.02631745 0.97349243]]梯度下降后： [[0.97369637 0.02649345] [0.02630363 0.97350655]]cost： 0.002787567335690624梯度下降前： [[0.97369637 0.02649345] [0.02630363 0.97350655]]梯度下降后： [[0.97371017 0.02647935] [0.02628983 0.97352065]]cost： 0.0027846224530159356","link":"/2019/05/12/Article7BackProp-2/"},{"title":"反向传播算法（三）之完整的反向传播算法","text":"前言 前面介绍了单层全连接层并使用激活函数激活的情况，尝试去进行了多样本的梯度下降计算，这一篇文章打算简单介绍一下多层全连接层的梯度下降的情况，重点在于如何进行梯度的向后传播。还是请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，但是可以很好的帮助理解反向传播算法。 一、模型定义 和前面的模型类似，我们使用的输入是一个长度为3的行向量，输出为长度为2的行向量，激活函数设置为 \\(g\\)，我们这里使用的是sigmoid激活函数，即： \\[ g(x) = \\frac{1}{1 + e^{-x}} \\tag{1} \\] 模型定义如图，首先定义一下字母记号（这里的字母表示是根据我自己的习惯来的，和其他的表示方法或许有点不同，不过没有关系。），\\(L\\)表示网络的层数，在我们上图中，可以靠打拼一共有三层(包括输入层)，所以\\(L = 3\\)。我们记网络的第\\(i\\)层为\\(a^i\\)，将输入层记作\\(a^0 = x\\)，很明显，输出层我们可以记作\\(a^{L-1} = \\hat{y}\\)，这里的 \\(\\hat{y}\\) 表示整个网络的输出。一般地，我们使用上标标记参数是属于网络的哪一层，下标表示该参数在参数矩阵（向量）中的位置。 我们在这里使用\\(z^i\\)表示还没有使用激活函数的网络第\\(i\\)层，很明显这里的\\(i\\)的范围是：\\(1 \\leq i \\leq L-1\\)，因为输入层不需要使用激活函数激活。 于是,我们可以得到下面的式子： \\[ \\begin{aligned} z^0 &amp;= a^0 \\omega^0 + b^0 \\\\ a^1 &amp;= g(z^0) \\\\ z^1 &amp;= a^1 \\omega^1 + b^1 \\\\ \\hat{y} &amp;= a^2 = g(z^1) \\end{aligned} \\] 其中的 \\(\\omega\\)表示的是每一层全连接的权重矩阵，\\(b\\)表示的是每一层的偏置量。不难看出，\\(\\omega^0\\)是一个3x3大小的矩阵，\\(b^0\\)是一个长度为3的行向量，\\(\\omega^1\\)是一个3x2大小的矩阵，\\(b^1\\)是一个长度为2的行向量。 和前面定义的模型类似，这里我们仍然使用差的平方之和作为最后的损失函数，即： \\[ C = cost(\\hat{y}, y) = \\sum(\\hat{y}_i - y_i)^2 = (\\hat{y}_1 - y_1)^2 + (\\hat{y}_2 - y_2)^2 = (a^2_1 - y_1)^2 + (a^2_2 - y_2)^2 \\tag{2} \\] 二、基本原理 首先要了解的是，所谓的反向传播，到底向后传播的是什么。简单来说，算法向后传播的是误差，即我们希望的目标值和真实值之间的差距，这里的目标值是网络每一层的输出，这里的真实值是理想中的那个完美的模型产生的数值。但是很显然，我们并不了解那个完美模型的每一层的输出是什么，我们只知道最后的标签（即 \\(y\\) ），所以我们需要根据最后一层的输出和 \\(y\\) 之间的误差，去调整每一层的输出，在这个调整的过程中，我们就是在调整每一层的权值和偏置量。 理解偏导数 对于偏导数 \\(\\frac{\\partial C}{\\partial a^{i}} ,(1 \\leq i \\leq L - 1)\\)，我们可以将这个偏导数理解成对于 \\(a^i\\)的一个小小的变化，\\(C\\)能有多敏感，我们这里说到的敏感度和前面说的误差本质上是一回事，因为每一层的 \\(a\\)都受到前面一层的输出的影响，所以当我们在向后传播误差到前面的全连接层的时候，我们必然会求出每一层的偏导数，即 \\(\\frac{\\partial C}{\\partial a^{i}} ,(1 \\leq i \\leq L - 1)\\)。此处 \\(i\\)不会取到0，这是因为在我们设定的模型结构中，\\(a^0\\) 表示的是输出层，而输入层本质上是不包含误差的，因此在我们这样的设置下，\\(i\\)的范围是\\(1，2，...,L- 1\\)。需要注意的是，有些网络会将输入层表示为 \\(a^1\\)，此时，\\(i\\)的最小取值就是2。不管如何设置，这些都是基于相同的原理。 求解偏导数 假设我们现在只关注最后的一层全连接层，会有： \\[ g(\\begin{bmatrix} a^1_1 &amp; a^1_2 &amp; a^1_3 \\end{bmatrix} \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{12} \\\\ \\omega^1_{21} &amp; \\omega^1_{22} \\\\ \\omega^1_{31} &amp; \\omega^1_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b^1_1 &amp; b^1_2\\end{bmatrix}) = \\begin{bmatrix} a^2_1 &amp; a^2_2 \\end{bmatrix} \\tag{3} \\] 我们沿用之前定义好的字母表示方法，用 \\(z^i\\)表示每一层未被激活时的矩阵，于是，我们可以有下面的式子： \\[ z^1 = a^1 \\omega^1 + b^1 \\\\ a^2 = g(z^1) \\\\ C = \\sum (a^2_i - y_i) ^2 \\] 将上面的第一个式子展开，我们有： \\[ \\begin{bmatrix} z^1_1 &amp; z^1_2\\end{bmatrix} = \\begin{bmatrix} a^1_1 &amp; a^1_2 &amp; a^1_3 \\end{bmatrix} \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{12} \\\\ \\omega^1_{21} &amp; \\omega^1_{22} \\\\ \\omega^1_{31} &amp; \\omega^1_{32} \\\\ \\end{bmatrix} + \\begin{bmatrix} b^1_1 &amp; b^1_2\\end{bmatrix} \\tag{4} \\] 继续将式子完全展开： \\[ z^1_1 = a^1_1 \\omega^1_{11} + a^1_2 \\omega^1_{21} + a^1_3 \\omega^1_{31} + b^1_1 \\tag{5} \\] \\[ z^1_2 = a^1_1 \\omega^1_{12} + a^1_2 \\omega^1_{22} + a^1_3 \\omega^1_{32} + b^1_2 \\tag{6} \\] 接着我们对前一层的输出求解偏导数，即，我们需要对 \\(a^1_1\\)，\\(a^1_2\\)，\\(a^1_3\\)求解偏导数。所以我们会有： \\[ \\frac{\\partial z^1_1}{\\partial a^1_1} = \\omega^1_{11},\\frac{\\partial z^1_1}{\\partial a^1_2} = \\omega^1_{21},\\frac{\\partial z^1_1}{\\partial a^1_3} = \\omega^1_{31} \\tag{7} \\] \\[ \\frac{\\partial z^1_2}{\\partial a^1_1} = \\omega^1_{12},\\frac{\\partial z^1_2}{\\partial a^1_2} = \\omega^1_{22},\\frac{\\partial z^1_2}{\\partial a^1_3} = \\omega^1_{32} \\tag{8} \\] 确实，这一步有些难以理解，实际上我们只是将后面一层的误差（敏感程度）通过求导的方式传递到前面一层而已。 对 \\(z^i\\)求偏导数 我们考虑对 \\(z^1 = \\begin{bmatrix} z^1_1 &amp; z^1_2 \\end{bmatrix}\\) 使用非线性激活函数激活，即我们有： \\[ a^2 = g(z^1) \\tag{9} \\] 展开之后就变成： \\[ \\begin{bmatrix} a^2_1 &amp; a^2_2 \\end{bmatrix} = g(\\begin{bmatrix} z^1_1 &amp; z^1_2 \\end{bmatrix}) \\tag{10} \\] 对应每一个元素，我们有： \\[ a^2_1 = g(z^1_1), a^2_2 = g(z^1_2) \\tag{11} \\] 所以我们求得每一个 \\(\\hat{y}_i\\) 对 \\(a_i\\) 的偏导数如下： \\[ \\frac{\\partial a^2_1}{\\partial z^1_1} = g\\prime(z^1_1), \\frac{\\partial a^2_2}{\\partial z^1_2} = g\\prime(z^1_2) \\tag{12} \\] cost值的相关偏导数 因为 \\(C = cost = (a^2_1 - y_1)^2 + (a^2_2 - y_2)^2\\)，所以我们可以求得： \\[ \\frac{\\partial C}{\\partial a^2_1} = 2 (a^2_1 - y_1),\\frac{\\partial C}{\\partial a^2_2} = 2 (a^2_2 - y_2) \\tag{13} \\] 整理总结 根据我们之前求出来的结果，我们可以将误差传递至 \\(a^1\\)层，于是，我们可以得到下面的几个式子： \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_1}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.1} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_1}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.2} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_1}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} \\tag{14.3} \\] \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_2}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.4} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_2}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.5} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_2}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{14.6} \\] 我们发现，上面的公式中，(14.1)和(14.4)，(14.2)和(14.5)，(14.3)和(14.6)计算的时同一个偏导数，那么究竟哪个偏导数的计算时正确的呢？实际上，每一个都不是正确的，但是每一个有都不是错误的，或者说每一个都只做了一半。这是因为每一个值都可以通过多条路径去影响最后的cost值。例如，以 \\(a^1_1\\)为例，它既可以和 \\(\\omega^1_{11}\\) 相乘来影响 \\(a^2_1\\) ，也可以通过和 \\(\\omega^1_{12}\\) 相乘来影响 \\(a^2_2\\) ，而这两条路最后都会影响cost值，因此，我们需要将所有的偏导数公式进行相加，得到我们最后真正的偏导数计算公式。 注意：实际上，如果对高等数学的链式法则求导有更深入的观察，可以一步就写出最后的偏导数公式，而我们上面这样做其实是不正确的，但是可以得出正确的结果。 \\[ \\frac{\\partial C}{\\partial a^1_1} = \\frac{\\partial z^1_1}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} + \\frac{\\partial z^1_2}{\\partial a^1_1} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.1} \\] \\[ \\frac{\\partial C}{\\partial a^1_2} = \\frac{\\partial z^1_1}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} + \\frac{\\partial z^1_2}{\\partial a^1_2} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.2} \\] \\[ \\frac{\\partial C}{\\partial a^1_3} = \\frac{\\partial z^1_1}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} +\\frac{\\partial z^1_2}{\\partial a^1_3} \\cdot \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\tag{15.3} \\] 同样，和前面一样，我们需要对上面的公式进行向量化的表示，这样代码编写会方便很多且不易出错。我们将(15.1)，(15.2)和(15.3)的结果整理成一个行向量（因为我们设定的模型的输入是一个行向量，所以，模型每一层的输出也都是一个行向量。行向量如下： \\[ \\begin{aligned} \\begin{bmatrix} \\frac{\\partial C}{\\partial a^1_1} &amp; \\frac{\\partial C}{\\partial a^1_2} &amp; \\frac{\\partial C}{\\partial a^1_3}\\end{bmatrix} &amp;= \\begin{bmatrix} \\frac{\\partial a^2_1}{\\partial z^1_1} \\cdot \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial a^2_2}{\\partial z^1_2} \\cdot \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial z^1_1}{\\partial a^1_1} &amp; \\frac{\\partial z^1_1}{\\partial a^1_2} &amp; \\frac{\\partial z^1_1}{\\partial a^1_3} \\\\ \\frac{\\partial z^1_2}{\\partial a^1_1} &amp; \\frac{\\partial z^1_2}{\\partial a^1_2} &amp; \\frac{\\partial z^1_2}{\\partial a^1_3} \\end{bmatrix} \\\\ &amp;= (\\begin{bmatrix} \\frac{\\partial a^2_1}{\\partial z^1_1} &amp; \\frac{\\partial a^2_2}{\\partial z^1_2}\\end{bmatrix} \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) \\begin{bmatrix} \\frac{\\partial z^1_1}{\\partial a^1_1} &amp; \\frac{\\partial z^1_1}{\\partial a^1_2} &amp; \\frac{\\partial z^1_1}{\\partial a^1_3} \\\\ \\frac{\\partial z^1_2}{\\partial a^1_1} &amp; \\frac{\\partial z^1_2}{\\partial a^1_2} &amp; \\frac{\\partial z^1_2}{\\partial a^1_3} \\end{bmatrix} \\\\ &amp;= (g\\prime(z^1) \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) \\begin{bmatrix} \\omega^1_{11} &amp; \\omega^1_{21} &amp; \\omega^1_{31} \\\\ \\omega^1_{12} &amp; \\omega^1_{22} &amp; \\omega^1_{32} \\end{bmatrix} \\\\ &amp;= (g\\prime(z^1) \\cdot * \\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}) (\\omega^1)^T \\end{aligned} \\tag{16} \\] 在上面的公式中，\\((\\omega^1)^T​\\) 表示的是 \\(\\omega^1​\\) 参数矩阵的转置，\\(\\cdot *​\\) 符号表示的是矩阵（向量）之间的点乘，即对应元素之间的相乘。 如果我们将 \\(\\begin{bmatrix} \\frac{\\partial C}{\\partial a^2_1} &amp; \\frac{\\partial C}{\\partial a^2_2} \\end{bmatrix}​\\) 记作 \\(\\delta^2​\\) ，将 \\(\\begin{bmatrix} \\frac{\\partial C}{\\partial a^1_1} &amp; \\frac{\\partial C}{\\partial a^1_2} &amp; \\frac{\\partial C}{\\partial a^1_3}\\end{bmatrix}​\\) 记作 \\(\\delta^1​\\)，那么我们就会得到更加简化的公式： \\[ \\delta^1 = (g\\prime(z^1) \\cdot * \\delta^2)(\\omega^1)^T \\tag{17} \\] 公式(17)就是我们需要找到的反向传播的核心公式。更一般的，如果我们有 \\(L\\) 层网络（包括输入层，那么，误差向后传递的核心公式就是如下： \\[ \\delta^i = (g\\prime(z^i) \\cdot * \\delta^{i + 1})(\\omega^i)^T \\quad (1 \\leq i \\leq L - 1) \\tag{18} \\] 其中，最后一层的 \\(\\delta^{L-1}\\) 就是根据输出值和真实值的计算公式，对每一个输出值进行求导操作。 根据传递来的误差进行参数的更新 现在，让我们重新审视一下我们之前在第二篇中求解出的参数更新公式。 \\[ \\frac{\\partial C}{\\partial \\omega} = x^T (\\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix}) \\tag{19} \\] \\[ \\frac{\\partial C}{\\partial b} = \\begin{bmatrix} g\\prime(a_1) &amp; g\\prime(a_2) \\end{bmatrix} \\cdot * \\begin{bmatrix} 2 \\cdot (\\hat{y}_1 - y_1) &amp; 2 \\cdot (\\hat{y}_2 - y_2)\\end{bmatrix} \\tag{20} \\] 按照我们在这一篇中使用的符号记法，重新写一下会有（因为这里有两层全连接层，之前只有一层，因此，我们这里的 \\(\\omega\\) 和 \\(b\\) 都使用的是最后一层的权值和偏置量，因此 \\(x\\) 就变成了倒数第二层的输出 \\(a^1\\)。）： \\[ \\frac{\\partial C}{\\partial \\omega^1} = (a^1)^T(g\\prime(z^1) \\cdot * \\delta^2) \\tag{21} \\] \\[ \\frac{\\partial C}{\\partial b^1} = g\\prime(z^1) \\cdot * \\delta^2 \\tag{22} \\] 于是根据上面的式子，我们就可以归纳出一般情况下的权重和偏置量的偏导数公式了。如果按照我们这篇文章中的字母记号的方法，那么，我们可以有：（\\(0 \\leq i \\leq L- 2\\)，\\(L\\)表示的是网络的层数，包括输入层。） \\[ \\frac{\\partial C}{\\partial \\omega^i} = (a^i)^T(g\\prime(z^i) \\cdot * \\delta^{i+1}) \\tag{23} \\] \\[ \\frac{\\partial C}{\\partial b^i} = g\\prime(z^i) \\cdot * \\delta^{i+1} \\tag{24} \\] 公式(18),(23),(24)就是反向传播算法的核心公式了，一般而言，我们首先会求出所有的 \\(\\delta\\) 参数，再根据 \\(\\delta\\)参数去求解所有的参数梯度，最后统一进行梯度的更新。 三、代码 和文中所使用的模型是一样的，由两个全连接层构成，使用sigmoid函数激活。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as npparam = {}nodes = {}learning_rate = 0.1def sigmoid(x): return 1.0 / (1. + np.exp(- x))def sigmoid_gradient(x): sig = sigmoid(x) return sig * (1. - sig)def cost(y_pred, y): return np.sum((y_pred - y) ** 2)def cost_gradient(y_pred, y): return 2 * (y_pred - y)def forward(x): nodes[\"a0\"] = x nodes['matmul0'] = np.matmul(x, param['w0']) nodes['z0'] = nodes['matmul0'] + param['b0'] nodes[\"a1\"] = sigmoid(nodes['z0']) nodes['matmul1'] = np.matmul(nodes['a1'], param['w1']) nodes['z1'] = nodes['matmul1'] + param['b1'] nodes['a2'] = sigmoid(nodes['z1']) return nodes['a2'] passdef backward(x, y_pred, y): \"\"\"compute delta\"\"\" delta2 = cost_gradient(y_pred, y) delta1 = np.matmul(np.multiply(sigmoid_gradient(nodes['z1']), delta2), np.transpose(param['w1'])) \"\"\"update\"\"\" gradient = {} gradient['w1'] = np.matmul(np.transpose(nodes['a1']), np.multiply(sigmoid_gradient(nodes[\"z1\"]), delta2)) gradient['b1'] = np.mean(np.multiply(sigmoid_gradient(nodes[\"z1\"]), delta2), axis=0) gradient[\"w0\"] = np.matmul(np.transpose(nodes['a0']), np.multiply(sigmoid_gradient(nodes[\"z0\"]), delta1)) gradient['b0'] = np.mean(np.multiply(sigmoid_gradient(nodes[\"z0\"]), delta1), axis=0) param['w1'] -= learning_rate * gradient['w1'] param['b1'] -= learning_rate * gradient['b1'] param[\"w0\"] -= learning_rate * gradient['w0'] param['b0'] -= learning_rate * gradient['b0'] passdef setup(): x = np.array([[1., 2., 3.], [3., 2., 1.]]) y = np.array([[1., 0.], [0., 1.]]) param['w0'] = np.random.random([3, 3]) param['b0'] = np.array([0., 0., 0.]) param['w1'] = np.random.random([3, 2]) param['b1'] = np.array([0., 0.]) for i in range(1000): y_pred = forward(x) backward(x, y_pred, y) print(\"梯度下降前：\", y_pred, \"\\n梯度下降后：\", forward(x), \"\\ncost：\", cost(forward(x), y))if __name__ == '__main__': setup() 结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176梯度下降前： [[0.79830536 0.83580604] [0.80449064 0.83875726]]梯度下降后： [[0.78872254 0.82729775] [0.79552187 0.83086468]]cost： 1.3905215341662558梯度下降前： [[0.78872254 0.82729775] [0.79552187 0.83086468]]梯度下降后： [[0.77882103 0.81832367] [0.78626614 0.82257321]]cost： 1.3682684724974281梯度下降前： [[0.77882103 0.81832367] [0.78626614 0.82257321]]梯度下降后： [[0.76863531 0.80888786] [0.77675443 0.81388922]]cost： 1.3458138579376486梯度下降前： [[0.76863531 0.80888786] [0.77675443 0.81388922]]梯度下降后： [[0.75820643 0.79900214] [0.76702339 0.8048258 ]]cost： 1.3232863979727467梯度下降前： [[0.75820643 0.79900214] [0.76702339 0.8048258 ]]梯度下降后： [[0.74758142 0.78868705] [0.75711474 0.79540344]]cost： 1.3008248813647023梯度下降前： [[0.74758142 0.78868705] [0.75711474 0.79540344]]梯度下降后： [[0.73681235 0.77797253] [0.74707448 0.7856506 ]]cost： 1.27857494201472梯度下降前： [[0.73681235 0.77797253] [0.74707448 0.7856506 ]]梯度下降后： [[0.72595508 0.76689824] [0.73695182 0.77560392]]cost： 1.2566851295390669梯度下降前： [[0.72595508 0.76689824] [0.73695182 0.77560392]]梯度下降后： [[0.71506782 0.75551347] [0.72679789 0.76530802]]cost： 1.2353024551577543梯度下降前： [[0.71506782 0.75551347] [0.72679789 0.76530802]]梯度下降后： [[0.70420955 0.74387642] [0.71666439 0.75481498]]cost： 1.214567655578547梯度下降前： [[0.70420955 0.74387642] [0.71666439 0.75481498]]梯度下降后： [[0.69343832 0.73205294] [0.70660222 0.74418325]]cost： 1.1946104774003092梯度下降前： [[0.69343832 0.73205294] [0.70660222 0.74418325]]梯度下降后： [[0.68280973 0.72011482] [0.69666013 0.73347616]]cost： 1.1755453177778676梯度下降前： [[0.68280973 0.72011482] [0.69666013 0.73347616]]梯度下降后： [[0.67237554 0.70813752] [0.68688359 0.72276011]]cost： 1.1574675529546317梯度下降前： [[0.67237554 0.70813752] [0.68688359 0.72276011]]梯度下降后： [[0.66218247 0.69619771] [0.67731377 0.71210253]]cost： 1.1404508391169166梯度下降前： [[0.66218247 0.69619771] [0.67731377 0.71210253]]梯度下降后： [[0.65227136 0.6843707 ] [0.66798687 0.70156968]]cost： 1.124545582249731梯度下降前： [[0.65227136 0.6843707 ] [0.66798687 0.70156968]]梯度下降后： [[0.64267666 0.67272797] [0.65893364 0.69122464]]cost： 1.1097786568052626梯度下降前： [[0.64267666 0.67272797] [0.65893364 0.69122464]]梯度下降后： [[0.63342615 0.66133502] [0.6501792 0.68112551]]cost： 1.0961543258622262梯度下降前： [[0.63342615 0.66133502] [0.6501792 0.68112551]]梯度下降后： [[0.62454101 0.65024966] [0.64174305 0.6713239 ]]cost： 1.0836561996688798梯度下降前： [[0.62454101 0.65024966] [0.64174305 0.6713239 ]]梯度下降后： [[0.61603613 0.63952088] [0.63363933 0.66186397]]cost： 1.0722499837437804梯度下降前： [[0.61603613 0.63952088] [0.63363933 0.66186397]]梯度下降后： [[0.60792055 0.62918816] [0.62587715 0.6527818 ]]cost： 1.0618867231940436梯度下降前： [[0.60792055 0.62918816] [0.62587715 0.6527818 ]]梯度下降后： [[0.60019807 0.61928143] [0.61846111 0.64410532]]cost： 1.0525062481304481梯度下降前： [[0.60019807 0.61928143] [0.61846111 0.64410532]]梯度下降后： [[0.59286793 0.60982139] [0.6113918 0.63585445]]cost： 1.0440405589832986梯度下降前： [[0.59286793 0.60982139] [0.6113918 0.63585445]]梯度下降后： [[0.5859255 0.60082016] [0.60466638 0.62804172]]cost： 1.036416947829888梯度下降前： [[0.5859255 0.60082016] [0.60466638 0.62804172]]梯度下降后： [[0.57936295 0.59228224] [0.59827914 0.62067296]]cost： 1.029560718871301梯度下降前： [[0.57936295 0.59228224] [0.59827914 0.62067296]]梯度下降后： [[0.57316992 0.58420554] [0.59222202 0.61374817]]cost： 1.0233974361978069梯度下降前： [[0.57316992 0.58420554] [0.59222202 0.61374817]]梯度下降后： [[0.5673341 0.57658245] [0.58648511 0.60726243]]cost： 1.0178546819469387梯度下降前： [[0.5673341 0.57658245] [0.58648511 0.60726243]]梯度下降后： [[0.56184178 0.56940091] [0.58105707 0.6012068 ]]cost： 1.0128633489602261......梯度下降前： [[0.94927668 0.05029516] [0.05573697 0.94479925]]梯度下降后： [[0.94931374 0.05025926] [0.05570013 0.94483464]]cost： 0.011240812046284514梯度下降前： [[0.94931374 0.05025926] [0.05570013 0.94483464]]梯度下降后： [[0.94935073 0.05022344] [0.05566335 0.94486995]]cost： 0.011225473896214073梯度下降前： [[0.94935073 0.05022344] [0.05566335 0.94486995]]梯度下降后： [[0.94938765 0.05018769] [0.05562665 0.9449052 ]]cost： 0.011210176001756078梯度下降前： [[0.94938765 0.05018769] [0.05562665 0.9449052 ]]梯度下降后： [[0.94942449 0.05015202] [0.05559002 0.94494039]]cost： 0.011194918207872375梯度下降前： [[0.94942449 0.05015202] [0.05559002 0.94494039]]梯度下降后： [[0.94946125 0.05011641] [0.05555345 0.94497551]]cost： 0.011179700360308792梯度下降前： [[0.94946125 0.05011641] [0.05555345 0.94497551]]梯度下降后： [[0.94949794 0.05008087] [0.05551696 0.94501057]]cost： 0.011164522305590443梯度下降前： [[0.94949794 0.05008087] [0.05551696 0.94501057]]梯度下降后： [[0.94953456 0.05004541] [0.05548053 0.94504556]]cost： 0.011149383891016414梯度下降前： [[0.94953456 0.05004541] [0.05548053 0.94504556]]梯度下降后： [[0.9495711 0.05001001] [0.05544418 0.94508049]]cost： 0.011134284964655492梯度下降前： [[0.9495711 0.05001001] [0.05544418 0.94508049]]梯度下降后： [[0.94960757 0.04997468] [0.05540789 0.94511535]]cost： 0.011119225375340889 可以看到，我们的算法是可以很好的进行反向传播，并且可以很好地减小cost值。","link":"/2019/05/12/Article8BackProp-3/"},{"title":"步长stride为1的二维卷积方法的反向传播算法","text":"前言 近年来，深度学习的快速发展带来了一系列喜人的成果，不管是在图像领域还是在NLP领域，深度学习都显示了其极其强大的能力。而深度学习之所以可以进行大规模的参数运算，反向传播算法功不可没，可以说，没有反向传播算法，深度学习就不可能得以快速发展，因此在此之前，有必要了解一下反向传播算法的具体原理和公式推导。请注意：这里的所有推导过程都只是针对当前设置的参数信息，并不具有一般性，但是所有的推导过程可以推导到一般的运算，因此以下给出的并不是反向传播算法的严格证明，不涉及十分复杂的公式推导，争取可以以一种简单的方式来理解卷积的反向传播。希望可以很好的帮助理解反向传播算法。 在之前曾经见到的说明过全连接层的反向传播，因此，这一次主要是专注于卷积层的反向传播。 需要注意的是，在本文中，所有的正向传播过程中，卷积的步长stride均固定为1。 一、参数设置 在前面的全连接层的反向传播算法的推导中，其实可以发现，反向传播算法的主要核心功能就是两个，一个是进行误差的向前传播，一个是进行参数的更新，当解决了这两个问题之后，某一个特定操作的反向传播算法就得到了解决。因此，我们先从一个简单具体的实例入手。 由于卷积往往可以看作一个二维平面上的操作，那么我们就先设定我们对一个二维数据矩阵进行卷积，卷积核则也是一个二维矩阵，步长参数我们首先设置为1，在以后的说明中，步长可以设定为其他的数值。按照TensorFlow定义的卷积格式，这里的padding我们默认均为VALID，事实上，如果设置为SAME，也可以通过填补0的方式改变成VALID。 这里我们设置我们的数据矩阵（记作\\(x\\)）大小为5x5，卷积核（记作\\(k\\)）大小为3x3，由于步长是1，因此，卷积之后获得的结果是一个3x3大小的数据矩阵（不妨我们记作\\(u\\)）。偏置项我们记为\\(b\\)，将和卷积之后的矩阵进行相加。 我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为5x5 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 始终为1 padding VALID 偏置项\\(b\\) 一个浮点数 我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} &amp; u_{1, 3}\\\\ u_{2, 1} &amp; u_{2, 2} &amp; u_{2, 3}\\\\ u_{3, 1} &amp; u_{3, 2} &amp; u_{3, 3}\\\\ \\end{bmatrix} \\] 我们将结果\\(u\\)继续展开，可以得到下面的庞大的矩阵： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} &amp; u_{1, 3}\\\\ u_{2, 1} &amp; u_{2, 2} &amp; u_{2, 3}\\\\ u_{3, 1} &amp; u_{3, 2} &amp; u_{3, 3}\\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 2}k_{1, 1} + x_{1, 3}k_{1, 2} +x_{1, 4}k_{1, 3} + \\\\ x_{2, 2}k_{2, 1} + x_{2, 3}k_{2, 2} +x_{2, 4}k_{2, 3} + \\\\ x_{3, 2}k_{3, 1} + x_{3, 3}k_{3, 2} +x_{3, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 3}k_{1, 1} + x_{1, 4}k_{1, 2} +x_{1, 5}k_{1, 3} + \\\\ x_{2, 3}k_{2, 1} + x_{2, 4}k_{2, 2} +x_{2, 5}k_{2, 3} + \\\\ x_{3, 3}k_{3, 1} + x_{3, 4}k_{3, 2} +x_{3, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{2, 1}k_{1, 1} + x_{2, 2}k_{1, 2} +x_{2, 3}k_{1, 3} + \\\\ x_{3, 1}k_{2, 1} + x_{3, 2}k_{2, 2} +x_{3, 3}k_{2, 3} + \\\\ x_{4, 1}k_{3, 1} + x_{4, 2}k_{3, 2} +x_{4, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{2, 2}k_{1, 1} + x_{2, 3}k_{1, 2} +x_{2, 4}k_{1, 3} + \\\\ x_{3, 2}k_{2, 1} + x_{3, 3}k_{2, 2} +x_{3, 4}k_{2, 3} + \\\\ x_{4, 2}k_{3, 1} + x_{4, 3}k_{3, 2} +x_{4, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{2, 3}k_{1, 1} + x_{2, 4}k_{1, 2} +x_{2, 5}k_{1, 3} + \\\\ x_{3, 3}k_{2, 1} + x_{3, 4}k_{2, 2} +x_{3, 5}k_{2, 3} + \\\\ x_{4, 3}k_{3, 1} + x_{4, 4}k_{3, 2} +x_{4, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{3, 1}k_{1, 1} + x_{3, 2}k_{1, 2} +x_{3, 3}k_{1, 3} + \\\\ x_{4, 1}k_{2, 1} + x_{4, 2}k_{2, 2} +x_{4, 3}k_{2, 3} + \\\\ x_{5, 1}k_{3, 1} + x_{5, 2}k_{3, 2} +x_{5, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 2}k_{1, 1} + x_{3, 3}k_{1, 2} +x_{3, 4}k_{1, 3} + \\\\ x_{4, 2}k_{2, 1} + x_{4, 3}k_{2, 2} +x_{4, 4}k_{2, 3} + \\\\ x_{5, 2}k_{3, 1} + x_{5, 3}k_{3, 2} +x_{5, 4}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{3, 3}k_{1, 1} + x_{3, 4}k_{1, 2} +x_{3, 5}k_{1, 3} + \\\\ x_{4, 3}k_{2, 1} + x_{4, 4}k_{2, 2} +x_{4, 5}k_{2, 3} + \\\\ x_{5, 3}k_{3, 1} + x_{5, 4}k_{3, 2} +x_{5, 5}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差前向传递 在前面已经完整的表示出了卷积的所有操作，下面我们来进行误差传递。 我们对上面的所有的输入进行求解偏导数的操作，我们可以得到下面的一张表格，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。于是，表格如下： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{1, 3}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\partial u_{2, 3}\\) \\(\\partial u_{3, 1}\\) \\(\\partial u_{3, 2}\\) \\(\\partial u_{3, 3}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 0 0 \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 0 \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 0 0 \\(\\partial x_{1, 4}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) 0 0 0 0 0 0 \\(\\partial x_{1, 5}\\) 0 0 \\(k_{1, 3}\\) 0 0 0 0 0 0 \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 \\(k_{1, 1}\\) 0 0 0 0 0 \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 0 \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\partial x_{2, 4}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\partial x_{2, 5}\\) 0 0 \\(k_{2, 3}\\) 0 0 \\(k_{1, 3}\\) 0 0 0 \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 0 \\(k_{2, 1}\\) 0 0 \\(k_{1, 1}\\) 0 0 \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(k_{1, 2}\\) \\(k_{1, 1}\\) 0 \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(k_{1, 1}\\) \\(\\partial x_{3, 4}\\) 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) 0 \\(k_{1, 3}\\) \\(k_{1, 2}\\) \\(\\partial x_{3, 5}\\) 0 0 \\(k_{3, 3}\\) 0 0 \\(k_{2, 3}\\) 0 0 \\(k_{1, 3}\\) \\(\\partial x_{4, 1}\\) 0 0 0 \\(k_{3, 1}\\) 0 0 \\(k_{2, 1}\\) 0 0 \\(\\partial x_{4, 2}\\) 0 0 0 \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(k_{2, 2}\\) \\(k_{2, 1}\\) 0 \\(\\partial x_{4, 3}\\) 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(k_{2, 1}\\) \\(\\partial x_{4, 4}\\) 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) 0 \\(k_{2, 3}\\) \\(k_{2, 2}\\) \\(\\partial x_{4, 5}\\) 0 0 0 0 0 \\(k_{3, 3}\\) 0 0 \\(k_{2, 3}\\) \\(\\partial x_{5, 1}\\) 0 0 0 0 0 0 \\(k_{3, 1}\\) 0 0 \\(\\partial x_{5, 2}\\) 0 0 0 0 0 0 \\(k_{3, 2}\\) \\(k_{3, 1}\\) 0 \\(\\partial x_{5, 3}\\) 0 0 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(k_{3, 1}\\) \\(\\partial x_{5, 4}\\) 0 0 0 0 0 0 0 \\(k_{3, 3}\\) \\(k_{3, 2}\\) \\(\\partial x_{5, 5}\\) 0 0 0 0 0 0 0 0 \\(k_{3, 3}\\) 可以看出，数据都是很规律的进行着重复。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。那么，根据求导的链式法则，我们有： \\[ \\frac{\\partial L}{\\partial x_{p, k}} = \\sum^{3}_{i = 1} \\sum^{3}_{j = 1} \\frac{\\partial L}{\\partial u_{i, j}} \\cdot \\frac{\\partial u_{i, j}}{\\partial x_{p, k}} = \\sum^{3}_{i = 1} \\sum^{3}_{j = 1} \\delta_{i, j} \\cdot \\frac{\\partial u_{i, j}}{\\partial x_{p, k}} \\] 根据这个公式，我们可以有： \\[ \\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} \\cdot k_{1, 2} + \\delta_{1, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} \\cdot k_{1, 3} + \\delta_{1, 2} \\cdot k_{1, 2} + \\delta_{1, 3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 4}} = \\delta_{1, 2} \\cdot k_{1, 3} + \\delta_{1, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{1, 5}} = \\delta_{1, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} \\cdot k_{2, 2} + \\delta_{1, 2} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1,2}+ \\delta_{2, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} \\cdot k_{2, 3} + \\delta_{1, 2} \\cdot k_{2, 2} + \\delta_{1, 3} \\cdot k_{2, 1} + \\delta_{2, 1} \\cdot k_{1,3}+ \\delta_{2,2} \\cdot k_{1, 2} +\\delta_{2,3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 4}} = \\delta_{1, 2} \\cdot k_{2, 3} + \\delta_{1, 3} \\cdot k_{2, 2} + \\delta_{2, 2} \\cdot k_{1,3}+ \\delta_{2, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{2, 5}} = \\delta_{1, 3} \\cdot k_{2, 3} + \\delta_{2, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1} \\cdot k_{3, 2} + \\delta_{1, 2} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 2} + \\delta_{2, 2} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 2} + \\delta_{3, 2} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1} \\cdot k_{3, 3} + \\delta_{1, 2} \\cdot k_{3, 2} + \\delta_{1, 3} \\cdot k_{3, 1} + \\delta_{2, 1} \\cdot k_{2, 3} + \\delta_{2, 2} \\cdot k_{2, 2} + \\delta_{2, 3} \\cdot k_{2, 1} + \\delta_{3, 1} \\cdot k_{1, 3} + \\delta_{3, 2} \\cdot k_{1, 2} + \\delta_{3, 3} \\cdot k_{1, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 4}} = \\delta_{1, 2} \\cdot k_{3, 3} + \\delta_{1, 3} \\cdot k_{3, 2} + \\delta_{2, 2} \\cdot k_{2, 3} + \\delta_{2, 3} \\cdot k_{2, 2} + \\delta_{3, 2} \\cdot k_{1, 3} + \\delta_{3, 3} \\cdot k_{1, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{3, 5}} = \\delta_{1, 3} \\cdot k_{3, 3} + \\delta_{2, 3} \\cdot k_{2, 3} + \\delta_{3, 3} \\cdot k_{1, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 1}} = \\delta_{2, 1} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 2}} = \\delta_{2, 1} \\cdot k_{3, 2} + \\delta_{2, 2} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 2} + \\delta_{3, 2} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 3}} = \\delta_{2, 1} \\cdot k_{3, 3} + \\delta_{2, 2} \\cdot k_{3, 2} + \\delta_{2, 3} \\cdot k_{3, 1} + \\delta_{3, 1} \\cdot k_{2, 3} + \\delta_{3, 2} \\cdot k_{2, 2} + \\delta_{3, 3} \\cdot k_{2, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 4}} = \\delta_{2, 2} \\cdot k_{3, 3} + \\delta_{2, 3} \\cdot k_{3, 2} + \\delta_{3, 2} \\cdot k_{2, 3} + \\delta_{3, 3} \\cdot k_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{4, 5}} = \\delta_{2, 3} \\cdot k_{3, 3} + \\delta_{3, 3} \\cdot k_{2, 3} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 1}} = \\delta_{3, 1} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 2}} = \\delta_{3, 1} \\cdot k_{3, 2} + \\delta_{3, 2} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 3}} = \\delta_{3, 1} \\cdot k_{3, 3} +\\delta_{3, 2} \\cdot k_{3, 2} + \\delta_{3, 3} \\cdot k_{3, 1} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 4}} = \\delta_{3, 2} \\cdot k_{3, 3} + \\delta_{3, 3} \\cdot k_{3, 2} \\] \\[ \\frac{\\partial L}{\\partial x_{5, 5}} = \\delta_{3, 3} \\cdot k_{3, 3} \\] 以上的式子虽然多，烦，一不小心就容易出错，但是每一个式子都是很简单的相乘相加，因此，我们考虑使用向量化或者矩阵化的表达方式。 为了更好的进行矩阵化表达，我们将\\(\\frac{\\partial L}{\\partial x_{3, 3}}\\)单独拿出来看，我们发现，这个式子可以变成两个相同的矩阵进行卷积（由于使用的是padding为VALID的模式，因此，在这种情况下，步长stride信息可有可无。），即： \\[ \\frac{\\partial L}{\\partial x_{3, 3}} = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 进一步，我们发现，以上所有的式子的构成元素都包含在上面的两个矩阵中。 我们记右侧的全部由卷积核元素构成的矩阵为\\(k'\\) 下面的一个步骤需要一点观察技巧了，如果在\\(\\delta\\)矩阵的上下左右同时填上两层0，变成如下的形式： \\[ \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;\\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ \\end{bmatrix} \\] 在此基础上，我们利用矩阵\\(k'\\)对上式进行卷积，该卷积的步长stride为1，可以得到一个和原始的输入矩阵相同大小的矩阵，不妨记该矩阵作\\(x’\\)。 接着，我们对之前求得的25个式子按照对应的顺序进行排列，记作\\(x''\\)，于是有： \\[ x'' = \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} \\] 经过计算，我们可以发现，\\(x'\\)和\\(x''\\)正好相等。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}}&amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}}&amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}}&amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}}&amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}}&amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} \\\\ \\end{bmatrix} = \\begin{bmatrix} 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;\\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;\\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3}&amp;0 &amp;0 \\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ 0 &amp;0 &amp;0&amp;0&amp;0&amp;0&amp;0\\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 在这个卷积操作中，步长stride为1。 所以我们发现，在卷积操作中误差的传递主要是利用该卷积的卷积核（经过一定的变换）对传递而来的误差进行卷积来完成的。所以，我们要解决的问题又两个，一个是卷积核的变换是什么样子的，另一个就是需要在传递来的误差上下左右填补多少0。 卷积核的变换 在前面，我们发现误差传递的时候使用的卷积核和正向传播时使用的略有不同，事实上，在误差传递的时候，我们使用的卷积核是正向传播时使用的卷积核的中心对称矩阵，抑或是将正向传播使用的矩阵旋转180°之后就得到了误差传递时使用的矩阵。在这里，并不需要严格证明这一结果，只需要知道需要这么做即可。 填补0 另一个问题是我们需要在传递过来的误差矩阵周围填补多少0。我们在这里假设输入矩阵是一个正方形矩阵，卷积核也是一个正方形矩阵，输入矩阵的长宽均为\\(n\\)，卷积核的长宽均为\\(k\\)，步长为1，则输出的矩阵长宽为\\(m = n - ( k - 1)\\)。假设经过填补0之后的误差矩阵长宽均为\\(x\\)，因为我们需要对卷积核进行旋转180°，所以卷积核长宽保持不变，所以有： \\[ x - (k - 1) = n \\] 又有： \\[ \\because m = n - (k - 1) \\\\ \\therefore n = m + (k - 1) \\\\ \\therefore x - (k - 1) = m + (k - 1) \\\\ \\therefore x = m + 2 * (k - 1) \\] 因此，上下左右需要填补\\(k - 1\\)层0。 在这里只讨论了正方形的输入矩阵和正方形的卷积核，但是这一结论很容易推广到任意尺寸的输入矩阵和卷积核，这里就不再赘述。 至此，我们就解决了在步长stride为1的卷积过程中的误差传递的问题。下面就是解决参数更新的问题了。 三、参数更新 和误差传递类似，我们需要对每一个可以更新的参数求解偏导数，和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， 即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以有下面的式子：这里以求解\\(\\frac{\\partial L}{\\partial k_{1, 1}}\\) 为例： \\[ \\begin{aligned} \\frac{\\partial L}{\\partial k_{1, 1}} =&amp; \\frac{\\partial L}{\\partial u_{1, 1}} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 2}} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{1, 3}} \\frac{\\partial u_{1, 3}}{k_{1, 1}} + \\\\ &amp;\\frac{\\partial L}{\\partial u_{2, 1}} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 2}} \\frac{\\partial u_{2, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{2, 3}} \\frac{\\partial u_{2, 3}}{k_{1, 1}} + \\\\ &amp;\\frac{\\partial L}{\\partial u_{3, 1}} \\frac{\\partial u_{3, 1}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{3, 2}} \\frac{\\partial u_{3, 2}}{k_{1, 1}} + \\frac{\\partial L}{\\partial u_{3, 3}} \\frac{\\partial u_{3, 3}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} \\frac{\\partial u_{1, 1}}{k_{1, 1}} + \\delta_{1, 2} \\frac{\\partial u_{1, 2}}{k_{1, 1}} + \\delta_{1, 3} \\frac{\\partial u_{1, 3}}{k_{1, 1}} + \\\\ &amp;\\delta_{2, 1} \\frac{\\partial u_{2, 1}}{k_{1, 1}} + \\delta_{2, 2} \\frac{\\partial u_{2, 2}}{k_{1, 1}} + \\delta_{2, 3} \\frac{\\partial u_{2, 3}}{k_{1, 1}} + \\\\ &amp;\\delta_{3, 1} \\frac{\\partial u_{3, 1}}{k_{1, 1}} + \\delta_{3, 2} \\frac{\\partial u_{3, 2}}{k_{1, 1}} + \\delta_{3, 3} \\frac{\\partial u_{3, 3}}{k_{1, 1}} \\\\ =&amp; \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 2} + \\delta_{1, 3} x_{1, 3} + \\delta_{2, 1} x_{2, 1} + \\delta_{2, 2} x_{2, 2} + \\delta_{2, 3} x_{2, 3} + \\delta_{3, 1} x_{3, 1} + \\delta_{3, 2} x_{3, 2} + \\delta_{3, 3} x_{3, 3} \\end{aligned} \\] 类似地，我们可以求出剩下的所有的偏导数，这里我们汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = \\delta_{1, 1} x_{1, 1} + \\delta_{1, 2} x_{1, 2} + \\delta_{1, 3} x_{1, 3} + \\delta_{2, 1} x_{2, 1} + \\delta_{2, 2} x_{2, 2} + \\delta_{2, 3} x_{2, 3} + \\delta_{3, 1} x_{3, 1} + \\delta_{3, 2} x_{3, 2} + \\delta_{3, 3} x_{3, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = \\delta_{1, 1} x_{1, 2} + \\delta_{1, 2} x_{1, 3} + \\delta_{1, 3} x_{1, 4} + \\delta_{2, 1} x_{2, 2} + \\delta_{2, 2} x_{2, 3} + \\delta_{2, 3} x_{2, 4} + \\delta_{3, 1} x_{3, 2} + \\delta_{3, 2} x_{3, 3} + \\delta_{3, 3} x_{3, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = \\delta_{1, 1} x_{1, 3} + \\delta_{1, 2} x_{1, 4} + \\delta_{1, 3} x_{1, 5} + \\delta_{2, 1} x_{2, 3} + \\delta_{2, 2} x_{2, 4} + \\delta_{2, 3} x_{2, 5} + \\delta_{3, 1} x_{3, 3} + \\delta_{3, 2} x_{3, 4} + \\delta_{3, 3} x_{3, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = \\delta_{1, 1} x_{2, 1} + \\delta_{1, 2} x_{2, 2} + \\delta_{1, 3} x_{2, 3} + \\delta_{2, 1} x_{3, 1} + \\delta_{2, 2} x_{3, 2} + \\delta_{2, 3} x_{3, 3} + \\delta_{3, 1} x_{4, 1} + \\delta_{3, 2} x_{4, 2} + \\delta_{3, 3} x_{4, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = \\delta_{1, 1} x_{2, 2} + \\delta_{1, 2} x_{2, 3} + \\delta_{1, 3} x_{2, 4} + \\delta_{2, 1} x_{3, 2} + \\delta_{2, 2} x_{3, 3} + \\delta_{2, 3} x_{3, 4} + \\delta_{3, 1} x_{4, 2} + \\delta_{3, 2} x_{4, 3} + \\delta_{3, 3} x_{4, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = \\delta_{1, 1} x_{2, 3} + \\delta_{1, 2} x_{2, 4} + \\delta_{1, 3} x_{2, 5} + \\delta_{2, 1} x_{3, 3} + \\delta_{2, 2} x_{3, 4} + \\delta_{2, 3} x_{3, 5} + \\delta_{3, 1} x_{4, 3} + \\delta_{3, 2} x_{4, 4} + \\delta_{3, 3} x_{4, 5} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = \\delta_{1, 1} x_{3, 1} + \\delta_{1, 2} x_{3, 2} + \\delta_{1, 3} x_{3, 3} + \\delta_{2, 1} x_{4, 1} + \\delta_{2, 2} x_{4, 2} + \\delta_{2, 3} x_{4, 3} + \\delta_{3, 1} x_{5, 1} + \\delta_{3, 2} x_{5, 2} + \\delta_{3, 3} x_{5, 3} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = \\delta_{1, 1} x_{3, 2} + \\delta_{1, 2} x_{3, 3} + \\delta_{1, 3} x_{3, 4} + \\delta_{2, 1} x_{4, 2} + \\delta_{2, 2} x_{4, 3} + \\delta_{2, 3} x_{4, 4} + \\delta_{3, 1} x_{5, 2} + \\delta_{3, 2} x_{5, 3} + \\delta_{3, 3} x_{5, 4} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = \\delta_{1, 1} x_{3, 3} + \\delta_{1, 2} x_{3, 4} + \\delta_{1, 3} x_{3, 5} + \\delta_{2, 1} x_{4, 3} + \\delta_{2, 2} x_{4, 4} + \\delta_{2, 3} x_{4, 5} + \\delta_{3, 1} x_{5, 3} + \\delta_{3, 2} x_{5, 4} + \\delta_{3, 3} x_{5, 5} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1}+ \\delta_{1, 2}+ \\delta_{1, 3}+ \\delta_{2, 1}+ \\delta_{2, 2}+ \\delta_{2, 3}+ \\delta_{3, 1}+ \\delta_{3, 2}+ \\delta_{3, 3} \\] 同样，我们将上面的偏导数信息整理一下，按照每个元素对应的位置进行排列，于是，我们有： \\[ \\frac{\\partial L}{\\partial k} = [\\frac{\\partial L}{\\partial k_{i, j}}] = \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} \\] 当我们这么整理之后，可以发现，这个矩阵可以拆解成两个矩阵的步长为1的卷积，即有： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} &amp; \\delta_{1, 3} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} &amp; \\delta_{2, 3} \\\\ \\delta_{3, 1} &amp; \\delta_{3, 2} &amp; \\delta_{3, 3} \\\\ \\end{bmatrix} \\] 因此，我们可以总结出，权重的梯度就是输入矩阵和误差矩阵进行步长为1卷积产生的结果矩阵。 对于偏置项的梯度\\(\\frac{\\partial L}{\\partial b}\\)则是全部的误差矩阵的元素的和。 四、总结 我们将上面的求解过程总结如下有： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵 输入卷积核\\(k\\) 一个二维矩阵 步长\\(stride\\) 始终为1 padding VALID 偏置项\\(b\\) 一个浮点数 正向传播： 1conv(x, kernel, bias, &quot;VALID&quot;) 反向传播： 1234567891011121314conv_backward(error, x, kernel, bias): # 计算传递给下一层的误差 1.在error周围填补上合适数目的0 2.将kernel旋转180° 3.将填补上0的误差和旋转之后的kernel进行步长为1的卷积，从而得到传递给下一层的误差new_error。 # 更新参数 1.将输入矩阵x和上一层传递来的误差矩阵error进行步长为1的卷积，得到kernel的更新梯度 2.将上一层传递来的误差矩阵error所有元素求和，得到bias的更新梯度 3.kernel := kernel - 学习率 * kernel的更新梯度 4.bias := bias - 学习率 * bias的更新梯度 # 返回误差，用以传递到下一层 return new_error","link":"/2019/05/24/Article14ConvBackProp-part1/"},{"title":"步长stride为s的二维卷积方法的反向传播算法：一个十分极端的例子","text":"前言 在前面的文章中，介绍了二维平面上的卷积及其反向传播的算法，但是，步长为1和2毕竟都是两个比较小的数字，如果换成更大的数字，反向传播的方式是不是还适合呢？所以，我们考虑下面这个十分极端的例子，来验证反向传播算法的有效性。 一、参数设置 在之前的参数设置中，我们使用的输入矩阵都是5x5，在这篇文章中，我们使用10x10大小的矩阵，在卷积核方面，我们依然使用3x3大小的卷积核，步长stride方面，我们使用一个很大的数字7，padding方式依然设置为VALID。 因此，我们的参数汇总如下： 参数 设置 输入矩阵\\(x\\) 一个二维矩阵，大小为10x10 输入卷积核\\(k\\) 一个二维矩阵，大小为3x3 步长\\(stride\\) 设置为7 padding VALID 偏置项\\(b\\) 一个浮点数 和前面一样，我们定义卷积操作的符号为\\(conv\\)，我们可以将卷积表示为（需要注意的是这里步长选取为7）： \\[ x \\; conv \\; k + b = u \\] 展开之后，我们可以得到： \\[ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} &amp; x_{1, 6} &amp; x_{1, 7} &amp; x_{1, 8} &amp;x_{1, 9} &amp;x_{1, 10} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} &amp; x_{2, 6} &amp; x_{2, 7} &amp; x_{2, 8} &amp;x_{2, 9} &amp;x_{2, 10} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} &amp; x_{3, 6} &amp; x_{3, 7} &amp; x_{3, 8} &amp;x_{3, 9} &amp;x_{3, 10} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} &amp; x_{4, 6} &amp; x_{4, 7} &amp; x_{4, 8} &amp;x_{4, 9} &amp;x_{4, 10} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} &amp; x_{5, 6} &amp; x_{5, 7} &amp; x_{5, 8} &amp;x_{5, 9} &amp;x_{5, 10} \\\\ x_{6, 1} &amp; x_{6, 2} &amp; x_{6, 3} &amp;x_{6, 4} &amp;x_{6, 5} &amp; x_{6, 6} &amp; x_{6, 7} &amp; x_{6, 8} &amp;x_{6, 9} &amp;x_{6, 10} \\\\ x_{7, 1} &amp; x_{7, 2} &amp; x_{7, 3} &amp;x_{7, 4} &amp;x_{7, 5} &amp; x_{7, 6} &amp; x_{7, 7} &amp; x_{7, 8} &amp;x_{7, 9} &amp;x_{7, 10} \\\\ x_{8, 1} &amp; x_{8, 2} &amp; x_{8, 3} &amp;x_{8, 4} &amp;x_{8, 5} &amp; x_{8, 6} &amp; x_{8, 7} &amp; x_{8, 8} &amp;x_{8, 9} &amp;x_{8, 10} \\\\ x_{9, 1} &amp; x_{9, 2} &amp; x_{9, 3} &amp;x_{9, 4} &amp;x_{9, 5} &amp; x_{9, 6} &amp; x_{9, 7} &amp; x_{9, 8} &amp;x_{9, 9} &amp;x_{9, 10} \\\\ x_{10, 1} &amp; x_{10, 2} &amp; x_{10, 3} &amp;x_{10, 4} &amp;x_{10, 5} &amp; x_{10, 6} &amp; x_{10, 7} &amp; x_{10, 8} &amp;x_{10, 9} &amp;x_{10, 10} \\\\ \\end{bmatrix} \\; conv \\; \\begin{bmatrix} k_{1, 1} &amp; k_{1, 2} &amp; k_{1, 3}\\\\ k_{2, 1} &amp; k_{2, 2} &amp; k_{2, 3}\\\\ k_{3, 1} &amp; k_{3, 2} &amp; k_{3, 3}\\\\ \\end{bmatrix} + b = \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} \\] 将矩阵\\(u\\)进一步展开，我们有： \\[ \\begin{bmatrix} u_{1, 1} &amp; u_{1, 2} \\\\ u_{2, 1} &amp; u_{2, 2} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\begin{matrix} x_{1, 1}k_{1, 1} + x_{1, 2}k_{1, 2} +x_{1, 3}k_{1, 3} + \\\\ x_{2, 1}k_{2, 1} + x_{2, 2}k_{2, 2} +x_{2, 3}k_{2, 3} + \\\\ x_{3, 1}k_{3, 1} + x_{3, 2}k_{3, 2} +x_{3, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{1, 8}k_{1, 1} + x_{1, 9}k_{1, 2} +x_{1, 10}k_{1, 3} + \\\\ x_{2, 8}k_{2, 1} + x_{2, 9}k_{2, 2} +x_{2, 10}k_{2, 3} + \\\\ x_{3, 8}k_{3, 1} + x_{3, 9}k_{3, 2} +x_{3, 10}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\\\ \\begin{matrix} x_{8, 1}k_{1, 1} + x_{8, 2}k_{1, 2} +x_{8, 3}k_{1, 3} + \\\\ x_{9, 1}k_{2, 1} + x_{9, 2}k_{2, 2} +x_{9, 3}k_{2, 3} + \\\\ x_{10, 1}k_{3, 1} + x_{10, 2}k_{3, 2} +x_{10, 3}k_{3, 3} + b \\\\ \\end{matrix} &amp; \\begin{matrix} x_{8, 8}k_{1, 1} + x_{8, 9}k_{1, 2} +x_{8, 10}k_{1, 3} + \\\\ x_{9, 8}k_{2, 1} + x_{9, 9}k_{2, 2} +x_{9, 10}k_{2, 3} + \\\\ x_{10, 8}k_{3, 1} + x_{10, 9}k_{3, 2} +x_{10, 10}k_{3, 3} + b \\\\ \\end{matrix} \\\\ \\end{bmatrix} \\] 二、误差传递 和之前一样，为了方便计算，也为了方便观察，我们计算如下的表格，每一列表示的是一个特定的输出 \\(\\partial u_{i, j}\\)，每一行表示的是一个特定的输入值\\(\\partial x_{p, k}\\)，行与列相交的地方表示的就是二者相除的结果，表示的是输出对于输入的偏导数，即\\(\\frac{\\partial u_{i, j}}{\\partial x_{p, k}}\\)。最后一列显示的是计算出的需要传递的误差的偏导数，具体计算方法和前面一样，在这里不再赘述： \\(\\partial u_{1, 1}\\) \\(\\partial u_{1, 2}\\) \\(\\partial u_{2, 1}\\) \\(\\partial u_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{i, j}}\\) \\(\\partial x_{1, 1}\\) \\(k_{1, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 1}} = \\delta_{1, 1} k_{1, 1}\\) \\(\\partial x_{1, 2}\\) \\(k_{1, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 2}} = \\delta_{1, 1} k_{1, 2}\\) \\(\\partial x_{1, 3}\\) \\(k_{1, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 3}} = \\delta_{1, 1} k_{1, 3}\\) \\(\\partial x_{1, 8}\\) 0 \\(k_{1, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 8}} = \\delta_{1, 2}k_{1, 1}\\) \\(\\partial x_{1, 9}\\) 0 \\(k_{1, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 9}} = \\delta_{1, 2}k_{1, 2}\\) \\(\\partial x_{1, 10}\\) 0 \\(k_{1, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{1, 10}} = \\delta_{1, 2}k_{1, 3}\\) \\(\\partial x_{2, 1}\\) \\(k_{2, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 1}} = \\delta_{1, 1} k_{2, 1}\\) \\(\\partial x_{2, 2}\\) \\(k_{2, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 2}} = \\delta_{1, 1} k_{2, 2}\\) \\(\\partial x_{2, 3}\\) \\(k_{2, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 3}} = \\delta_{1, 1} k_{2, 3}\\) \\(\\partial x_{2, 8}\\) 0 \\(k_{2, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 8}} = \\delta_{1, 2}k_{2, 1}\\) \\(\\partial x_{2, 9}\\) 0 \\(k_{2, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 9}} = \\delta_{1, 2}k_{2, 2}\\) \\(\\partial x_{2, 10}\\) 0 \\(k_{2, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{2, 10}} = \\delta_{1, 2}k_{2, 3}\\) \\(\\partial x_{3, 1}\\) \\(k_{3, 1}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 1}} = \\delta_{1, 1} k_{3, 1}\\) \\(\\partial x_{3, 2}\\) \\(k_{3, 2}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 2}} = \\delta_{1, 1} k_{3, 2}\\) \\(\\partial x_{3, 3}\\) \\(k_{3, 3}\\) 0 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 3}} = \\delta_{1, 1} k_{3, 3}\\) \\(\\partial x_{3, 8}\\) 0 \\(k_{3, 1}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 8}} = \\delta_{1, 2}k_{3, 1}\\) \\(\\partial x_{3, 9}\\) 0 \\(k_{3, 2}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 9}} = \\delta_{1, 2}k_{3, 2}\\) \\(\\partial x_{3, 10}\\) 0 \\(k_{3, 3}\\) 0 0 \\(\\frac{\\partial L}{\\partial x_{3, 10}} = \\delta_{1, 2}k_{3, 3}\\) \\(\\partial x_{8, 1}\\) 0 0 \\(k_{1, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 1}} = \\delta_{2, 1} k_{1, 1}\\) \\(\\partial x_{8, 2}\\) 0 0 \\(k_{1, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 2}} = \\delta_{2, 1} k_{1, 2}\\) \\(\\partial x_{8, 3}\\) 0 0 \\(k_{1, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{8, 3}} = \\delta_{2, 1} k_{1, 3}\\) \\(\\partial x_{8, 8}\\) 0 0 0 \\(k_{1, 1}\\) \\(\\frac{\\partial L}{\\partial x_{8, 8}} = \\delta_{2, 2}k_{1, 1}\\) \\(\\partial x_{8, 9}\\) 0 0 0 \\(k_{1, 2}\\) \\(\\frac{\\partial L}{\\partial x_{8, 9}} = \\delta_{2, 2}k_{1, 2}\\) \\(\\partial x_{8, 10}\\) 0 0 0 \\(k_{1, 3}\\) \\(\\frac{\\partial L}{\\partial x_{8, 10}} = \\delta_{2, 2}k_{1, 3}\\) \\(\\partial x_{9, 1}\\) 0 0 \\(k_{2, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 1}} = \\delta_{2, 1} k_{2, 1}\\) \\(\\partial x_{9, 2}\\) 0 0 \\(k_{2, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 2}} = \\delta_{2, 1} k_{2, 2}\\) \\(\\partial x_{9, 3}\\) 0 0 \\(k_{2, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{9, 3}} = \\delta_{2, 1} k_{2, 3}\\) \\(\\partial x_{9, 8}\\) 0 0 0 \\(k_{2, 1}\\) \\(\\frac{\\partial L}{\\partial x_{9, 8}} = \\delta_{2, 2}k_{2, 1}\\) \\(\\partial x_{9, 9}\\) 0 0 0 \\(k_{2, 2}\\) \\(\\frac{\\partial L}{\\partial x_{9, 9}} = \\delta_{2, 2}k_{2, 2}\\) \\(\\partial x_{9, 10}\\) 0 0 0 \\(k_{2, 3}\\) \\(\\frac{\\partial L}{\\partial x_{9, 10}} = \\delta_{2, 2}k_{2, 3}\\) \\(\\partial x_{10, 1}\\) 0 0 \\(k_{3, 1}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 1}} = \\delta_{2, 1} k_{3, 1}\\) \\(\\partial x_{10, 2}\\) 0 0 \\(k_{3, 2}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 2}} = \\delta_{2, 1} k_{3, 2}\\) \\(\\partial x_{10, 3}\\) 0 0 \\(k_{3, 3}\\) 0 \\(\\frac{\\partial L}{\\partial x_{10, 3}} = \\delta_{2, 1} k_{3, 3}\\) \\(\\partial x_{10, 8}\\) 0 0 0 \\(k_{3, 1}\\) \\(\\frac{\\partial L}{\\partial x_{10, 8}} = \\delta_{2, 2}k_{3, 1}\\) \\(\\partial x_{10, 9}\\) 0 0 0 \\(k_{3, 2}\\) \\(\\frac{\\partial L}{\\partial x_{10, 9}} = \\delta_{2, 2}k_{3, 2}\\) \\(\\partial x_{10, 10}\\) 0 0 0 \\(k_{3, 3}\\) \\(\\frac{\\partial L}{\\partial x_{10, 10}} = \\delta_{2, 2}k_{3, 3}\\) \\(else\\) 0 0 0 0 0 可以看出，无论是何种卷积方式，数据都是十分有规律地进行分布。 我们假设后面传递过来的误差是 \\(\\delta\\) ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 其中，\\(\\delta_{i, j} = \\frac{\\partial L}{\\partial u_{i, j}}\\)，误差分别对应于每一个输出项。这里的\\(L\\)表示的是最后的Loss损失。我们的目的就是希望这个损失尽可能小。 根据前面的方法，我们先要求应该传递给下一层的误差。所以第一步，我们先在接受来的误差矩阵中插入合适数目的0，由于这里前向卷积采用的步长stride是7，所以接收到误差矩阵中的每个元素之间应该插入（7 - 1 = 6）个0，即： \\[ \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 接着，由于我们采用的卷积核的大小是3x3，所有，我们依然需要在上面矩阵的外围补上（3 - 1 = 2）层0，即： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] 下一步就是将正向卷积的卷积核旋转180°，即： \\[ \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} \\] 最后一步就是将上面的误差矩阵和旋转后的卷积核进行步长为1的卷积，即： \\[ \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\; conv(stride = 1)\\; \\begin{bmatrix} k_{3, 3} &amp; k_{3, 2} &amp; k_{3, 1} \\\\ k_{2, 3} &amp; k_{2, 2} &amp; k_{2, 1} \\\\ k_{1, 3} &amp; k_{1, 2} &amp; k_{1, 1} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\delta_{1, 1} k_{1, 1} &amp; \\delta_{1, 1} k_{1, 2} &amp; \\delta_{1, 1} k_{1, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{1, 1} &amp; \\delta_{1, 2}k_{1, 2} &amp; \\delta_{1, 2}k_{1, 3} \\\\ \\delta_{1, 1} k_{2, 1} &amp; \\delta_{1, 1} k_{2, 2} &amp; \\delta_{1, 1} k_{2, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{2, 1} &amp; \\delta_{1, 2}k_{2, 2} &amp; \\delta_{1, 2}k_{2, 3} \\\\ \\delta_{1, 1} k_{3, 1} &amp; \\delta_{1, 1} k_{3, 2} &amp; \\delta_{1, 1} k_{3, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2}k_{3, 1} &amp; \\delta_{1, 2}k_{3, 2} &amp; \\delta_{1, 2}k_{3, 3} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{2, 1} k_{1, 1} &amp; \\delta_{2, 1} k_{1, 2} &amp; \\delta_{2, 1} k_{1, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{1, 1} &amp; \\delta_{2, 2}k_{1, 2} &amp; \\delta_{2, 2}k_{1, 3} \\\\ \\delta_{2, 1} k_{2, 1} &amp; \\delta_{2, 1} k_{2, 2} &amp; \\delta_{2, 1} k_{2, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{2, 1} &amp; \\delta_{2, 2}k_{2, 2} &amp; \\delta_{2, 2}k_{2, 3} \\\\ \\delta_{2, 1} k_{3, 1} &amp; \\delta_{2, 1} k_{3, 2} &amp; \\delta_{2, 1} k_{3, 3} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2}k_{3, 1} &amp; \\delta_{2, 2}k_{3, 2} &amp; \\delta_{2, 2}k_{3, 3} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} \\frac{\\partial L}{\\partial x_{1, 1}} &amp; \\frac{\\partial L}{\\partial x_{1, 2}} &amp; \\frac{\\partial L}{\\partial x_{1, 3}} &amp; \\frac{\\partial L}{\\partial x_{1, 4}} &amp; \\frac{\\partial L}{\\partial x_{1, 5}} &amp; \\frac{\\partial L}{\\partial x_{1, 6}} &amp; \\frac{\\partial L}{\\partial x_{1, 7}} &amp; \\frac{\\partial L}{\\partial x_{1, 8}} &amp; \\frac{\\partial L}{\\partial x_{1, 9}} &amp; \\frac{\\partial L}{\\partial x_{1, 10}} \\\\ \\frac{\\partial L}{\\partial x_{2, 1}} &amp; \\frac{\\partial L}{\\partial x_{2, 2}} &amp; \\frac{\\partial L}{\\partial x_{2, 3}} &amp; \\frac{\\partial L}{\\partial x_{2, 4}} &amp; \\frac{\\partial L}{\\partial x_{2, 5}} &amp; \\frac{\\partial L}{\\partial x_{2, 6}} &amp; \\frac{\\partial L}{\\partial x_{2, 7}} &amp; \\frac{\\partial L}{\\partial x_{2, 8}} &amp; \\frac{\\partial L}{\\partial x_{2, 9}} &amp; \\frac{\\partial L}{\\partial x_{2, 10}} \\\\ \\frac{\\partial L}{\\partial x_{3, 1}} &amp; \\frac{\\partial L}{\\partial x_{3, 2}} &amp; \\frac{\\partial L}{\\partial x_{3, 3}} &amp; \\frac{\\partial L}{\\partial x_{3, 4}} &amp; \\frac{\\partial L}{\\partial x_{3, 5}} &amp; \\frac{\\partial L}{\\partial x_{3, 6}} &amp; \\frac{\\partial L}{\\partial x_{3, 7}} &amp; \\frac{\\partial L}{\\partial x_{3, 8}} &amp; \\frac{\\partial L}{\\partial x_{3, 9}} &amp; \\frac{\\partial L}{\\partial x_{3, 10}} \\\\ \\frac{\\partial L}{\\partial x_{4, 1}} &amp; \\frac{\\partial L}{\\partial x_{4, 2}} &amp; \\frac{\\partial L}{\\partial x_{4, 3}} &amp; \\frac{\\partial L}{\\partial x_{4, 4}} &amp; \\frac{\\partial L}{\\partial x_{4, 5}} &amp; \\frac{\\partial L}{\\partial x_{4, 6}} &amp; \\frac{\\partial L}{\\partial x_{4, 7}} &amp; \\frac{\\partial L}{\\partial x_{4, 8}} &amp; \\frac{\\partial L}{\\partial x_{4, 9}} &amp; \\frac{\\partial L}{\\partial x_{4, 10}} \\\\ \\frac{\\partial L}{\\partial x_{5, 1}} &amp; \\frac{\\partial L}{\\partial x_{5, 2}} &amp; \\frac{\\partial L}{\\partial x_{5, 3}} &amp; \\frac{\\partial L}{\\partial x_{5, 4}} &amp; \\frac{\\partial L}{\\partial x_{5, 5}} &amp; \\frac{\\partial L}{\\partial x_{5, 6}} &amp; \\frac{\\partial L}{\\partial x_{5, 7}} &amp; \\frac{\\partial L}{\\partial x_{5, 8}} &amp; \\frac{\\partial L}{\\partial x_{5, 9}} &amp; \\frac{\\partial L}{\\partial x_{5, 10}} \\\\ \\frac{\\partial L}{\\partial x_{6, 1}} &amp; \\frac{\\partial L}{\\partial x_{6, 2}} &amp; \\frac{\\partial L}{\\partial x_{6, 3}} &amp; \\frac{\\partial L}{\\partial x_{6, 4}} &amp; \\frac{\\partial L}{\\partial x_{6, 5}} &amp; \\frac{\\partial L}{\\partial x_{6, 6}} &amp; \\frac{\\partial L}{\\partial x_{6, 7}} &amp; \\frac{\\partial L}{\\partial x_{6, 8}} &amp; \\frac{\\partial L}{\\partial x_{6, 9}} &amp; \\frac{\\partial L}{\\partial x_{6, 10}} \\\\ \\frac{\\partial L}{\\partial x_{7, 1}} &amp; \\frac{\\partial L}{\\partial x_{7, 2}} &amp; \\frac{\\partial L}{\\partial x_{7, 3}} &amp; \\frac{\\partial L}{\\partial x_{7, 4}} &amp; \\frac{\\partial L}{\\partial x_{7, 5}} &amp; \\frac{\\partial L}{\\partial x_{7, 6}} &amp; \\frac{\\partial L}{\\partial x_{7, 7}} &amp; \\frac{\\partial L}{\\partial x_{7, 8}} &amp; \\frac{\\partial L}{\\partial x_{7, 9}} &amp; \\frac{\\partial L}{\\partial x_{7, 10}} \\\\ \\frac{\\partial L}{\\partial x_{8, 1}} &amp; \\frac{\\partial L}{\\partial x_{8, 2}} &amp; \\frac{\\partial L}{\\partial x_{8, 3}} &amp; \\frac{\\partial L}{\\partial x_{8, 4}} &amp; \\frac{\\partial L}{\\partial x_{8, 5}} &amp; \\frac{\\partial L}{\\partial x_{8, 6}} &amp; \\frac{\\partial L}{\\partial x_{8, 7}} &amp; \\frac{\\partial L}{\\partial x_{8, 8}} &amp; \\frac{\\partial L}{\\partial x_{8, 9}} &amp; \\frac{\\partial L}{\\partial x_{8, 10}} \\\\ \\frac{\\partial L}{\\partial x_{9, 1}} &amp; \\frac{\\partial L}{\\partial x_{9, 2}} &amp; \\frac{\\partial L}{\\partial x_{9, 3}} &amp; \\frac{\\partial L}{\\partial x_{9, 4}} &amp; \\frac{\\partial L}{\\partial x_{9, 5}} &amp; \\frac{\\partial L}{\\partial x_{9, 6}} &amp; \\frac{\\partial L}{\\partial x_{9, 7}} &amp; \\frac{\\partial L}{\\partial x_{9, 8}} &amp; \\frac{\\partial L}{\\partial x_{9, 9}} &amp; \\frac{\\partial L}{\\partial x_{9, 10}} \\\\ \\frac{\\partial L}{\\partial x_{10, 1}} &amp; \\frac{\\partial L}{\\partial x_{10, 2}} &amp; \\frac{\\partial L}{\\partial x_{10, 3}} &amp; \\frac{\\partial L}{\\partial x_{10, 4}} &amp; \\frac{\\partial L}{\\partial x_{10, 5}} &amp; \\frac{\\partial L}{\\partial x_{10, 6}} &amp; \\frac{\\partial L}{\\partial x_{10, 7}} &amp; \\frac{\\partial L}{\\partial x_{10, 8}} &amp; \\frac{\\partial L}{\\partial x_{10, 9}} &amp; \\frac{\\partial L}{\\partial x_{10, 10}} \\\\ \\end{bmatrix} \\] 经过上面的计算，在误差传递上，我们的算法可以正确运行，即使步长stride是一个任意的数字。接下来我们来验证更新梯度的计算。 三、更新梯度 和前面的定义一样，假设我们在这一阶段接收到的后方传递过来的误差为\\(\\delta\\)， ，即： \\[ \\delta = \\begin{bmatrix} \\delta_{1, 1} &amp; \\delta_{1, 2} \\\\ \\delta_{2, 1} &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 那么根据偏导数求解的链式法则，我们可以计算出所有的需要的偏导数，这里的计算过程和前面的计算过程是一样的，这里不再赘述。汇总如下： \\[ \\frac{\\partial L}{\\partial k_{1, 1}} = x_{1, 1}\\delta_{1, 1} + x_{1, 8}\\delta_{1, 2} + x_{8, 1}\\delta_{2, 1} + x_{8, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 2}} = x_{1, 2}\\delta_{1, 1} + x_{1, 9}\\delta_{1, 2} + x_{8, 2}\\delta_{2, 1} + x_{8, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{1, 3}} = x_{1, 3}\\delta_{1, 1} + x_{1, 10}\\delta_{1, 2} + x_{8, 3}\\delta_{2, 1} + x_{8, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 1}} = x_{2, 1}\\delta_{1, 1} + x_{2, 8}\\delta_{1, 2} + x_{9, 1}\\delta_{2, 1} + x_{9, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 2}} = x_{2, 2}\\delta_{1, 1} + x_{2, 9}\\delta_{1, 2} + x_{9, 2}\\delta_{2, 1} + x_{9, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{2, 3}} = x_{2, 3}\\delta_{1, 1} + x_{2, 10}\\delta_{1, 2} + x_{9, 3}\\delta_{2, 1} + x_{9, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 1}} = x_{3, 1}\\delta_{1, 1} + x_{3, 8}\\delta_{1, 2} + x_{10, 1}\\delta_{2, 1} + x_{10, 8}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 2}} = x_{3, 2}\\delta_{1, 1} + x_{3, 9}\\delta_{1, 2} + x_{10, 2}\\delta_{2, 1} + x_{10, 9}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial k_{3, 3}} = x_{3, 3}\\delta_{1, 1} + x_{3, 10}\\delta_{1, 2} + x_{10, 3}\\delta_{2, 1} + x_{10, 10}\\delta_{2, 2} \\] \\[ \\frac{\\partial L}{\\partial b} = \\delta_{1, 1} + \\delta_{1, 2} + \\delta_{2, 1} + \\delta_{2, 2} \\] 按照之前的算法，由于正向卷积中的步长stride为7，因此，在计算更新梯度的过程中，我们依然需要在接收到的误差矩阵的每两个相邻的元素之间插入（7 - 1 = 6）个0，即： \\[ \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 接着我们拿输入矩阵\\(x\\)和上面的矩阵进行步长为1的卷积，则可以得到卷积核参数的更新梯度。即： \\[ \\begin{bmatrix} \\frac{\\partial L}{\\partial k_{1, 1}} &amp; \\frac{\\partial L}{\\partial k_{1, 2}} &amp; \\frac{\\partial L}{\\partial k_{1, 3}} \\\\ \\frac{\\partial L}{\\partial k_{2, 1}} &amp; \\frac{\\partial L}{\\partial k_{2, 2}} &amp; \\frac{\\partial L}{\\partial k_{2, 3}} \\\\ \\frac{\\partial L}{\\partial k_{3, 1}} &amp; \\frac{\\partial L}{\\partial k_{3, 2}} &amp; \\frac{\\partial L}{\\partial k_{3, 3}} \\\\ \\end{bmatrix} = \\\\ \\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; x_{1, 3} &amp;x_{1, 4} &amp;x_{1, 5} &amp; x_{1, 6} &amp; x_{1, 7} &amp; x_{1, 8} &amp;x_{1, 9} &amp;x_{1, 10} \\\\ x_{2, 1} &amp; x_{2, 2} &amp; x_{2, 3} &amp;x_{2, 4} &amp;x_{2, 5} &amp; x_{2, 6} &amp; x_{2, 7} &amp; x_{2, 8} &amp;x_{2, 9} &amp;x_{2, 10} \\\\ x_{3, 1} &amp; x_{3, 2} &amp; x_{3, 3} &amp;x_{3, 4} &amp;x_{3, 5} &amp; x_{3, 6} &amp; x_{3, 7} &amp; x_{3, 8} &amp;x_{3, 9} &amp;x_{3, 10} \\\\ x_{4, 1} &amp; x_{4, 2} &amp; x_{4, 3} &amp;x_{4, 4} &amp;x_{4, 5} &amp; x_{4, 6} &amp; x_{4, 7} &amp; x_{4, 8} &amp;x_{4, 9} &amp;x_{4, 10} \\\\ x_{5, 1} &amp; x_{5, 2} &amp; x_{5, 3} &amp;x_{5, 4} &amp;x_{5, 5} &amp; x_{5, 6} &amp; x_{5, 7} &amp; x_{5, 8} &amp;x_{5, 9} &amp;x_{5, 10} \\\\ x_{6, 1} &amp; x_{6, 2} &amp; x_{6, 3} &amp;x_{6, 4} &amp;x_{6, 5} &amp; x_{6, 6} &amp; x_{6, 7} &amp; x_{6, 8} &amp;x_{6, 9} &amp;x_{6, 10} \\\\ x_{7, 1} &amp; x_{7, 2} &amp; x_{7, 3} &amp;x_{7, 4} &amp;x_{7, 5} &amp; x_{7, 6} &amp; x_{7, 7} &amp; x_{7, 8} &amp;x_{7, 9} &amp;x_{7, 10} \\\\ x_{8, 1} &amp; x_{8, 2} &amp; x_{8, 3} &amp;x_{8, 4} &amp;x_{8, 5} &amp; x_{8, 6} &amp; x_{8, 7} &amp; x_{8, 8} &amp;x_{8, 9} &amp;x_{8, 10} \\\\ x_{9, 1} &amp; x_{9, 2} &amp; x_{9, 3} &amp;x_{9, 4} &amp;x_{9, 5} &amp; x_{9, 6} &amp; x_{9, 7} &amp; x_{9, 8} &amp;x_{9, 9} &amp;x_{9, 10} \\\\ x_{10, 1} &amp; x_{10, 2} &amp; x_{10, 3} &amp;x_{10, 4} &amp;x_{10, 5} &amp; x_{10, 6} &amp; x_{10, 7} &amp; x_{10, 8} &amp;x_{10, 9} &amp;x_{10, 10} \\\\ \\end{bmatrix} \\; conv(stride = 1)\\; \\begin{bmatrix} \\delta_{1, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{1, 2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\delta_{2, 1} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\delta_{2, 2} \\\\ \\end{bmatrix} \\] 经过计算，两者的结果是相同的，这也就验证了我们的算法在一些比较极端的情况下也是正确的。 四、总结 经过一个比较极端的卷积实例的讲解，我们验证了我们算法的正确性，而下一步就是用代码实现二维平面上的卷积及其反向传播算法。","link":"/2019/05/24/Article16ConvBackProp-part3/"},{"title":"EM算法","text":"前言 EM算法是一种可以求解含有隐变量的迭代算法，当我们在实际过程中收集数据的时候，并不一定会收集全部的有效信息。比如，当我们想统计全校学生的身高分布的时候，可以将全校学生的身高看作是一个正态分布，但是毕竟男生和女生之间身高的分布还是有些不同的，但是万一我们没有对性别信息进行统计，而只是统计了身高信息的话，求得的概率分布的参数肯定会有较大的误差，这个时候，我们就需要将每一个样本的性别分布也考虑进去，从而希望获得更准确的概率分布估计。 一、准备工作 1、极大似然估计 极大似然估计我们并不陌生，在逻辑回归的求解过程中，我们就是用了极大似然估计，现在还是简单说明一下。 假设我们现在有一个概率分布，不妨记作\\(P(x;\\theta)​\\)，其中，\\(\\theta​\\)是未知参数，有可能是一个数值，也有可能是多个数值组成的参数向量，\\(x​\\)表示输入样本。现在我们想通过抽样的方式对参数\\(\\theta​\\)进行估计。假设我们一共采集了\\(N​\\)组数据，为\\(\\{x_1, x_2, \\cdots, x_N\\}​\\)。那么样本的联合概率函数可以表示为关于\\(\\theta​\\)的函数，即： \\[ L(\\theta) = L(x_1, x_2, \\cdots, x_N;\\theta) = \\prod_i^N P(x_i;\\theta) \\] \\(L(\\theta)​\\)是参数 \\(θ​\\) 的函数，随着 \\(θ​\\) 在参数变化，\\(L​\\)函数也在变化。而极大似然估计目的就是在样本\\(\\{x_1,...,x_N\\}​\\)固定的情况下，寻找最优的 \\(θ​\\) 来极大化似然函数： \\[ \\theta^{*} = \\mathop{\\arg\\max}_{\\theta}{L(\\theta)} \\] 上式在数学领域，可以看作是对 \\(θ^{*}\\)求解，求\\(L(θ)\\) 函数的极值点，导数为0处，即为 \\(θ*\\) 的点。 又因为\\(L(θ)​\\) 和 \\(ln(θ)​\\) 在同一个 \\(θ​\\) 处取得极值，我们可以对 \\(L(θ)​\\) 取对数，将连乘转化为连加(方便求导)，得到对数化似然函数： \\[ \\theta^*= \\mathop{\\arg\\max}_{\\theta}{ln\\;L(\\theta)} = \\mathop{\\arg\\max}_{\\theta} \\sum_i ln\\; P(x_i;\\theta) \\] 2、Jensen不等式 下图是一张描述Jensen不等式十分经典的图。 如果一个函数\\(f(x)\\)在其定义域内是一个连续函数，且其二阶导数恒小于等于0，我们称该函数在其定义域上是凹函数，反之，如果二阶导数恒大于等于0，我们称该函数在其定义域上是凸函数。 如果\\(f(x)\\)是一个凸函数，那么在其定义域上我们有: \\[ E(f(X)) \\geq f(E(X)) \\] 反之，如果\\(f(x)\\)是一个凹函数，在其定义域上我们有： \\[ E(f(X)) \\leq f(E(X)) \\] 其中，\\(E\\)表示对变量取期望。上面两个不等式当且仅当\\(X\\)是一个常量时可以取等号。 3、边缘分布 假设我们有两个随机变量，那么我们通过抽样，就会获得一个二维的联合概率分布\\(P(X=x_i,Y=y_j)\\)。 对每一个\\(X=x_i\\)，对其所有的\\(Y\\)进行求和操作，我们有： \\[ \\sum_{y_j}P(X=x_i, Y=y_j) = P(X=x_i) \\] 将上面的式子称之为\\(X=x_i\\)的边际分布（边缘分布）。 ​ 有了以上的一些基础准备，我们就可以来推导EM算法了。 二、EM算法 假设我们的数据集为： \\[ D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\] 其中 \\(x^{(i)}​\\) 是每一个具体的输出实例，表示每一次独立实验的结果，\\(N​\\)表示独立实验的次数。 我们设样本的概率分布函数为\\(P(x^{(i)};\\theta)​\\)，其中\\(\\theta​\\)是模型中的待估参数，可以是一个变量，也可以是多个变量所组成的参数向量。 根据极大似然估计，我们有： \\[ L(\\theta) = \\prod_{i}P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{1} \\] 两边同时取对数： \\[ ln\\;L(\\theta) = \\sum_{i} ln \\; P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{2} \\] 此时，我们可以将 \\(P(x^{(i)}; \\theta)\\)看作是关于隐变量的一个边缘分布，即（我们假设隐变量为\\(Z\\)）： \\[ ln \\; L(\\theta) = \\sum_i ln \\; \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{3} \\] 这里我们利用了边缘分布的相关等式，即： \\[ P(x^{(i)}; \\theta) = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] 在上面的式子中，\\(z​\\)是一个隐藏变量，必然也会满足一个特定的概率分布，我们不妨把这个分布记作\\(Q_{i}(z^{(i)})​\\)，显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1​\\)。这里的下标和上标\\(i​\\)表示的是第\\(i​\\)个样本。故我们将上式改写成： \\[ \\begin{aligned} ln \\; L(\\theta) &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\\\ &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\end{aligned} \\tag{4} \\] 现在，我们把如下的部分单独拿出来： \\[ Q_i(z^{(i)}) = p(z^{(i)}) \\\\ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = f(z^{(i)}) \\] 很显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = \\sum_i p(z^{(i)}) = 1\\)，所以，我们可以将上式写成： \\[ ln \\; L(\\theta) = \\sum_i ln \\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)}) \\tag{5} \\] 可以看出，我们的\\(\\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)})\\)实际上实在对\\(f(z^{(i)})\\)计算期望，其中\\(p(z^{(i)})\\)是函数\\(f(z^{(i)})\\)的概率分布函数，于是，我们可以把上面的式子记作： \\[ E[f(z^{(i)})] = \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\tag{6} \\] 于是，我们的似然函数就变成了： \\[ ln \\; L(\\theta) = \\sum_i ln \\;(E[f(z^{(i)})]) = \\sum_i ln \\; (E[\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}]) \\tag{7} \\] 这个时候就是Jensen不等式出场的时候了。 我们观察到函数\\(g(x)=ln(x)\\)，它的一阶导数是\\(g'(x) = \\frac{1}{x}\\)，二阶导数是\\(g''(x) = - \\frac{1}{x^2}\\)恒小于0，因此\\(g(x) = ln(x)\\)是一个凹函数，此时，我们利用Jensen不等式处理\\(ln \\;L(\\theta)\\)，有： \\[ \\begin{aligned} H(Y|X)&amp; =\\sum_{x\\in X} p(x)H(Y|X) \\\\ &amp; =-\\sum_{x\\in X} p(x)\\sum_{y\\in Y}p(y|x)\\log p(y|x)\\\\ &amp; =-\\sum_{x\\in X} \\sum_{y\\in Y}p(y,x)\\log p(y|x) \\end{aligned} \\] 故：我们根据Jensen不等式，有了以下的一个重要的不等式关系： \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{9} \\] 需要注意的是，我们使用Jensen不等式的时候，是对\\(z^{(i)}\\)的分布使用的，而\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}))\\)是函数\\(ln \\; L(\\theta)\\)的一个下界，所以实际上，\\(ln \\; L(\\theta)\\)包含了两个参数变量，一个是\\(\\theta\\)， 一个是隐藏变量\\(z^{(i)}\\)，所以我们需要弄清楚调整\\(\\theta\\)和\\(z^{(i)}\\)的区别。 由于Jensen不等式处理的是\\(z^{(i)}\\)，因此当我们调整\\(z^{(i)}\\)的时候，我们实际上实在调整似然函数\\(ln \\; L(\\theta)\\)的下界，使得似然函数\\(ln \\; L(\\theta)\\)的下界一点一点上升，最终于此时的似然函数\\(ln \\; L(\\theta)\\)的值相等。 然后，固定\\(z^{(i)}\\)的数值，调整\\(\\theta\\)的时候，就可以将\\(z^{(i)}\\)看作是一个已知变量，这个时候就可以利用极大似然估计的方法对\\(\\theta\\)参数的值进行计算，此时会得到一个新的\\(\\theta\\)的值，不妨记作\\(\\theta'\\)。我们这个时候再根据这个新的\\(\\theta'\\)的值，重新调整\\(z^{(i)}\\)，使得函数的下界一点一点上升，达到和\\(ln \\; L(\\theta)\\)相同之后，再固定\\(z^{(i)}\\)，更新\\(\\theta\\)的值。一直重复以上过程，直到似然函数收敛到某一个极大值\\(\\theta^*\\)处。 下图是一个很经典的关于EM算法的迭代过程示意图。（图片来自网络） 在上面的求解过程中，只有当此时的函数下界等于当前\\(\\theta\\)的对数似然函数时，才能保证当我们优化了这个下界的时候，才真正优化了目标似然函数。 \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{10} \\] 在优化迭代的过程中，我们通过固定\\(\\theta​\\)并调整\\(z^{(i)}​\\)的可能分布，使得等式成立，即达到\\(ln \\; L(\\theta)​\\)的下界。根据Jensen不等式的条件，当\\(f(x)​\\)是一个凹函数的时候，有\\(f(E[X]) \\geq E[f(X)]​\\)，欲使等号成立，\\(X​\\)需要是一个常量。那么，在上面的式子中，我们有\\(X = \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}​\\)，故此时我们需要将\\(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})​\\)看作一个常数，不妨我们设这个常数为\\(C​\\)，于是我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}) = C \\tag{11} \\] \\[ P(x^{(i)}, z^{(i)}; \\theta) = C Q_i(z^{(i)}) \\] \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\sum_{z^{(i)}} Q_i(z^{(i)}) \\tag{12} \\] 考虑到\\(Q_i(z^{(i)})\\)实际上是隐变量\\(z^{(i)}\\)的一个概率分布，满足： \\[ \\sum_{z^{(i)}} Q_i(z^{(i)}) = 1 ,\\quad Q_i(z^{(i)}) \\geq 0 \\] 于是，我们将\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1\\)代入到上面的式子(12)中，有： \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\tag{11} \\] 再将\\(C\\)带入到公式(11)中，我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = C = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] \\[ \\begin{aligned} Q_i(z^{(i)}) &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{\\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta)} \\\\ &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(x^{(i)};\\theta)} \\\\ &amp;= P(z^{(i)}|x^{(i)};\\theta) \\end{aligned} \\tag{12} \\] 即我们可以得到\\(Q_i(z^{(i)})\\)的值，也即我们得到\\(P(z^{(i)}|x^{(i)};\\theta)\\)的值，表示在当前的模型参数\\(\\theta\\)为定值时，在给定的\\(x^{(i)}\\)的条件下，得到\\(z^{(i)}\\)的概率大小。 至此，我们的EM算法的大部分情况进行了说明。首先，我们会对模型的参数\\(\\theta\\) 进行随机初始化，不妨记作\\(\\theta^0\\)。然后会在每一次的迭代循环中计算\\(z^{(i)}\\)的条件概率期望，这就是EM算法中的”E步“。最后再根据计算得到的概率分布，根据极大似然的方法计算在当前隐藏变量分布下的使得似然函数取得极大的\\(\\theta\\)的值，并进行更新，这就是EM算法中的”M步“。 观察到在”M步“中，我们有： \\[ ln \\; L(\\theta) = \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\] \\[ \\theta^{(j + 1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) - \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)})) \\] 观察到在上面的式子中，\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)}))\\)对于整个优化的过程来说相当于是一个常数项，因此可以省略，于是，上式可以简写成： \\[ \\begin{aligned} \\theta^{(j + 1)} &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\end{aligned} \\tag{13} \\] 公式(13)内部的\\(\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta))\\)就是EM算法的核心，我们一般将其称之为Q函数，通常记为：\\(Q(\\theta, \\theta^{(j)})\\)。 所以，我们的EM算法可以总结如下： 数据集为\\(D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\}\\) ，随机初始化模型参数\\(\\theta\\)，记作\\(\\theta^{(0)}\\)。 对每一次迭代循环，\\(j = 0, 1, 2, 3, \\cdots, M​\\)，我们有： E步（E-Step）：在当前的模型参数\\(\\theta^{(j)}​\\)的条件下，计算联合分布的条件概率期望： \\[ Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\\theta^{(j)}) \\] M步（M-Step）：在计算出条件概率分布的期望的基础上，极大化似然函数，得到新的模型参数的值\\(\\theta^{(j+1)}​\\): \\[ \\theta^{(j+1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 如果\\(\\theta^{(j+1)}​\\)已经收敛，则跳出循环结束： 输出最后模型参数\\(\\theta​\\)的值。 三、EM算法解决三硬币模型 三硬币模型是EM算法的一个简单使用，问题请参考《统计学习方法》一书。 假设有三枚质量分布不均匀的硬币A、B、C，这些硬币正面出现的概率分别是\\(\\pi\\)、\\(p\\)、\\(q\\)。进行如下掷硬币试验： 先掷A，如果A是正面则再掷B，如果A是反面则再掷C。对于B或C的结果，如果是正面则记为1，如果是反面则记为0。进行N次独立重复实验，得到结果。现在只能观测到结果，不能观测到掷硬币的过程，估计模型参数\\(\\theta=(\\pi,p,q)​\\)。 由于实验一共进行了N次，每一次都是独立重复实验，那么我们可以将实验结果记录如下，其中每一次的实验结果是已知的： \\[ X = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\quad x^{(i)} \\in \\{0, 1\\} \\] 每次独立实验都会产生一个隐藏变量\\(z^{(i)}\\)，这个隐藏变量是无法被观测到的，我们可以将其记录如下，这个隐藏变量的记录结果是未知的： \\[ Z = \\{z^{(1)}, z^{(2)}, \\cdots, z^{(N)}\\} \\quad z^{(i)} \\in \\{0, 1\\} \\] 对于第\\(i\\)次的独立重复实验，我们有： \\[ P(x^{(i)} = 0;\\theta) = \\pi(1-p)^{1-x^{(i)}} + (1-\\pi)(1-q)^{1-x^{(i)}} \\] \\[ P(x^{(i)}=1;\\theta) = \\pi p^{x^{(i)}} + (1-\\pi)q^{1-x^{(i)}} \\] 故，综合起来看，我们有： \\[ P(x^{(i)};\\theta) = \\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}} \\] 构造极大似然函数 我们可以构造我们的极大似然函数如下： \\[ \\begin{aligned} L(\\theta) &amp;= \\prod_i P(x^{(i)};\\theta) \\\\ &amp;= \\prod_i [\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\end{aligned} \\] 两边同时取对数，有： \\[ ln \\; L(\\theta) = \\sum_i ln\\;[\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\] 构造我们的Q函数 在没有说明的情况下，我们使用下标表示第几次迭代过程，用上标表示第几个样本，\\(\\theta^{(j)}\\)的上标表示第\\(j\\)次迭代。 对于三硬币问题，我们的Q函数可以构造如下： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ \\end{aligned} \\] 故，我们需要求解\\(P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)这四个概率值。 求解极大值 \\[ \\begin{aligned} P(z^{(i)}=1|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=1;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= \\mu_j^{(i)} \\end{aligned} \\] 上式对于迭代过程来说是一个定值，我们使用符号\\(\\mu_j^{(i)}\\)来表示，上标\\((i)\\)表示的是第\\(i\\)个样本，下标\\(j\\)表示的是第\\(j\\)次迭代过程。 那么很明显，我们有： \\[ \\begin{aligned} P(z^{(i)}=0|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=0;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{(1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(j)}}}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= 1 - \\mu_j^{(i)} \\end{aligned} \\] 接着，我们计算\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)： \\[ P(x^{(i)}, z^{(i)}=1;\\theta) = \\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}} \\] \\[ P(x^{(i)}, z^{(i)}=0;\\theta)=(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}} \\] 我们将上面的计算结果都带入到Q函数中，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot ln\\;[\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}] + (1-\\mu_j^{(i)})\\cdot ln\\; [(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}]\\} \\end{aligned} \\] 下一步就是对我们需要求解的变量进行求偏导数的操作，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial \\pi} &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}}{\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}} + (1-\\mu_j^{(i)})\\cdot \\frac{-1 \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}{(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi} + (\\mu_j^{(i)}-1)\\cdot \\frac{1}{1-\\pi}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)}(\\frac{1}{\\pi} + \\frac{1}{1-\\pi}) - \\frac{1}{1-\\pi} \\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi(1-\\pi)}\\} - \\frac{N\\pi}{\\pi(1-\\pi)} \\\\ &amp;= \\frac{1}{\\pi(1-\\pi)}\\{\\sum_i \\mu_j^{(i)} - N\\pi\\} \\end{aligned} \\] 令上式为0，我们有： \\[ \\sum_i \\mu_j^{(i)} - N\\pi = 0 \\] 即： \\[ \\pi = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] 同样的道理，我们可以计算出Q函数相对于\\(p\\)的偏导数，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial p} &amp;= \\sum_i \\mu_j^{(i)} \\frac{x^{(i)} \\cdot p^{x^{(i)}-1} \\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-x^{(i)})\\cdot (1-p)^{-x^{(i)}}\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} + 0 \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\frac{\\frac{x^{(i)}}{p}\\cdot p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} \\cdot \\frac{1}{1-p}\\cdot (1-x^{(i)})\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\{\\frac{x^{(i)}}{p} + \\frac{1-x^{(i)}}{p-1}\\} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\cdot \\frac{(p-1)\\cdot x^{(i)} + p(1-x^{(i)})}{p(p-1)} \\\\ &amp;= \\frac{1}{p(p-1)} \\sum_i \\mu_j^{(i)} \\{p-x^{(i)}\\} \\\\ &amp;= \\frac{1}{p(p-1)}\\{p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)}\\} \\end{aligned} \\] 令上式等于0，我们可以得到： \\[ p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)} = 0 \\] 即： \\[ p = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] 同理，我们对\\(q\\)求偏导数，有： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial q} &amp;= \\sum_i (1-\\mu_j^{(i)})(\\frac{x^{(i)}}{q}+\\frac{1-x^{(i)}}{p-1}) \\\\ &amp;= \\sum_i (1-\\mu_j^{(i)})\\frac{q-x^{(i)}}{q(q-1)} \\\\ &amp;= \\frac{1}{q(q-1)}\\{q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} \\} \\end{aligned} \\] 令上式等于0，我们有： \\[ q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} =0 \\] 即： \\[ q = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 所以，以上就是我们解决三硬币模型的迭代公式的求解过程，公式汇总如下，这里加入了下标，表示新的一轮迭代变量： \\[ \\mu_j^{(i)} = \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\] \\[ \\pi_{j+1} = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] \\[ p_{j+1} = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] \\[ q_{j+1} = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 四、EM算法的收敛性 在之前的过程中，我们都是默认EM算法可以收敛到某一极大值附近，但是并没有给出十分严格的证明，所以，我们需要对EM的收敛性进行一定的验证。 由于我们是利用极大似然估计来估计参数的值，那么，我们只需要保证在每一次的迭代过程中，似然函数的数值都在上升即可，即下面的不等式成立： \\[ ln \\; L(\\theta^{(j+1)}) \\geq ln\\;L(\\theta^{(j)}) \\] 由于： \\[ P(x^{(i)};\\theta) = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}|x^{(i)};\\theta)} \\] 因此，两边取对数，我们有： \\[ ln \\; P(x^{(i)};\\theta) = ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta) \\] 对每一个样本进行累加，我们有： \\[ \\sum_i ln \\; P(x^{(i)};\\theta) = \\sum_i (ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta)) \\] 根据我们构造的Q函数，我们有： \\[ Q(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 另，我们可以构造如下的一个函数，记作\\(H(\\theta, \\theta^{(j)})\\)，如下： \\[ H(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\] 我们将上面的两个函数相减，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) - H(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\&amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) (ln\\;P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;\\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}| x^{(i)};\\theta)} \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;P(x^{(i)};\\theta) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) (\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) ) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) \\\\ &amp;= ln \\;L(\\theta) \\end{aligned} \\] 在上面的式子中，第三行是利用了条件概率的公式，第五行则是利用了\\(\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) = 1\\)的条件。 所以，我们构造出的两个式子，相减之后正好是我们的极大似然函数的对数。 于是，我们将\\(ln\\; L(\\theta^{(j+1)})\\)和\\(ln\\;L(\\theta^{(j)})\\)相减，我们有： \\[ \\begin{aligned} ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j+1)}, \\theta^{(j)})) - (Q(\\theta^{(j)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\\\ &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\end{aligned} \\] 对于第一个括号内部的\\(Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})\\)，由于我们是通过极大化Q函数来更新参数的数值，所以\\(Q(\\theta^{(j+1)}, \\theta^{(j)}) \\geq Q(\\theta^{(j)}, \\theta^{(j)})\\)，故这一部分一定会大于等于0，即： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] 对于第二个括号内部的数值，我们有： \\[ \\begin{aligned} H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln\\; \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} \\\\ &amp;\\leq \\sum_i ln(\\sum_{z^{(i)}} \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} P(z^{(i)}|x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i ln (\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;= \\sum_i ln(1) \\\\ &amp;= 0 \\end{aligned} \\] 在上面的第三步中，我们使用了Jensen不等式，在第五步中，我们使用了\\(\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)}) = 1\\)这一条件。 于是，我们可以有： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] \\[ H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) \\leq 0 \\] 故： \\[ (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\geq 0 \\] 即： \\[ ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) \\geq 0 \\] 所以EM算法是可以逐步收敛到某一极大值附近的。证毕。 五、EM算法的缺陷 EM算法是处理含有隐藏变量模型的重要算法，但是EM算法也有其缺陷，首先，EM算法对初始值敏感，不同的初始值可能会导致不同的结果，这是由于似然函数的性质决定的，如果一个似然函数是凹函数，那么最后会收敛到极大值附近，也就是最大值附近，但是如果函数存在多个极大值，那么算法的初始值就会影响最后的结果。","link":"/2019/05/10/Article2EM Algorithm/"}],"tags":[{"name":"深度学习","slug":"深度学习","link":"/tags/深度学习/"},{"name":"目标检测","slug":"目标检测","link":"/tags/目标检测/"},{"name":"YOLOv3","slug":"YOLOv3","link":"/tags/YOLOv3/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"AdaBoost","slug":"AdaBoost","link":"/tags/AdaBoost/"},{"name":"Welcome","slug":"Welcome","link":"/tags/Welcome/"},{"name":"卷积","slug":"卷积","link":"/tags/卷积/"},{"name":"反向传播","slug":"反向传播","link":"/tags/反向传播/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/逻辑回归/"},{"name":"主成分分析","slug":"主成分分析","link":"/tags/主成分分析/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"反向传播算法","slug":"反向传播算法","link":"/tags/反向传播算法/"},{"name":"奇异值分解","slug":"奇异值分解","link":"/tags/奇异值分解/"},{"name":"SVD","slug":"SVD","link":"/tags/SVD/"},{"name":"EM算法","slug":"EM算法","link":"/tags/EM算法/"}],"categories":[{"name":"目标检测","slug":"目标检测","link":"/categories/目标检测/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"},{"name":"深度学习","slug":"深度学习","link":"/categories/深度学习/"}]}