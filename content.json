{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Adaboost算法","text":"前言 提升方法（boosting）是一种常用的机器学习方法，应用十分广泛，而且效果非常好，近几年的很多比赛的优胜选手都或多或少使用了提升方法用以提高自己的成绩。 提升方法的本质是通过对每一个训练样本赋予一个权重，并通过改变这些样本的权重，来学习多个分类器，并按照一定的算法将这些分类器组合在一起，通常是线性组合，因为单个分类器往往效果有限，因此组合多个分类器往往会提高模型的性能。 一、提升方法简介 提升方法（boosting）实际上是集成学习方法的一种，其基于这样的一种朴素思想：对于一个复杂的任务来说，将多个“专家”（这里的“专家”本意是指各种机器学习模型，可以较好地满足实际问题的需要）的意见进行适当的整合，进而得出最后的综合的判断，比其中任何一个单一的“专家”给出的判断要好。实际上，也就是“三个臭皮匠顶过诸葛亮”的意思。因此，boousting的本意就是寻找到合适的“臭皮匠”。 在实际的数据处理的过程中，我们往往可以很容易地发现各种各样的弱机器学习模型，这些模型仅仅比随机猜测好一些，但是还远远不能满足实际作业的精度要求。但是要寻找到一个单一的十分强大的机器学习模型往往会十分困难，虽然可以很好的满足要求，但是寻找这样的模型并不容易。不过好在，我们可以通过整合之前发现的弱机器学习模型，来进行综合考虑，从而形成一个可以媲美单一的强大的机器学习模型。这些弱机器学习模型往往被称之为“弱学习方法”，强机器学习模型往往被称之为“强学习方法”。 二、AdaBoost算法 1、AdaBoost算法的过程 2、AdaBoost的使用","link":"/2019/05/10/Article1AdaBoost/"},{"title":"逻辑回归算法","text":"前言 前面主要是讲反向传播和梯度下降的方法，那么其实涉及梯度的机器学习方法并不是只有深度学习一种，逻辑回归也是可以利用梯度的信息进行参数的更新，使得模型逐步满足我们的数据要求。注意，逻辑回归输出的是属于某一种类别的概率，利用阈值的控制来进行判别，因此逻辑回归本质上是一种分类方法。 一、逻辑斯蒂回归 逻辑斯蒂回归（logistic regression，下面简称逻辑回归），是一种十分经典的分类方法。我们首先介绍一下逻辑回归的定义。 假设我们有一个数据集 \\(S\\)，一共包含\\(m\\)个样本数据，即： \\(S = \\{(x_1,y_1),(x_2,y_2),...,(x_m，y_m)\\}\\)，其中，\\(y_i \\in \\{0, 1\\}\\)。为了表示的方便，我们不妨设当\\(y_i = 1\\)时为正样本，当\\(y_i = 0\\)时为负样本，当然，反过来也是可以的，这个并不重要，只不过一般习惯这样表达。 在SVM中，我们根据数据集的分布，求解出了一个二分的超平面 \\(f(x) = \\omega \\cdot x+b​\\)，现在我们要对一个新的样本点\\(x_0​\\)进行分类预测，需要将这个样本点带入上面的超平面公式。当\\(f(x_0) = \\omega \\cdot x_0 + b &gt; 0​\\)时，我们将这个样本点标记为1，当\\(f(x_0) = \\omega \\cdot x_0 + b \\leq 0​\\)时，我们将这个样本点标记为-1。观察到SVM只能对新样本输出 \\(\\pm1​\\)，无法较为准确的输出样本属于每一个类别的概率究竟是多少。SVM结果的正确性在于它保证了找到的是样本间隔最大的那个超平面，这样就可以保证以最高的精确度区分新的样本。然而SVM却无法对一个新样本的概率进行求解。而这就是逻辑回归主要做的事情，它输出的是一个概率值，当这个概率值大于一定的阈值时，样本标记为1，反之则标记为0。 二、sigmoid函数 熟悉深度学习的人肯定对这个函数非常了解，它是早期深度学习网络经常使用的激活函数之一。它的定义公式如下： \\[ sigmoid(x) = \\frac{1}{1 + e^{-x}} \\quad x \\in R \\] 它的函数图像是一个典型的S型曲线，定义域是全体实数。我们根据公式可以发现，这个函数将全体实数映射到了 \\((0, 1)\\) 区间上，并在这个区间上单调递增，\\(x\\)越大，函数值越大。而这正好符合我们需要的概率分布的规律。 三、逻辑回归模型 二项逻辑回归模型本质上是一个类似下面公式的条件分布： \\[ P(Y = 1 | x) = \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{1} \\] \\[ P(Y = 0 | x) = 1 - \\frac{1}{1 + e^{-(\\omega \\cdot x + b)}} \\tag{2} \\] 其中，第一个式子是我们需要重点关注的。我们对第一个式子右边的分数，上下同时乘以 \\(e^{\\omega \\cdot x + b}\\)，得到： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x + b}}{1 + e^{\\omega \\cdot x + b}} \\tag{3} \\] 以上就是我们经常可以看到的逻辑回归的公式了。 现在我们将偏置量也放进参数 \\(\\omega\\)中，所以变量\\(x\\)的尾部会增加一个多余的维度1来和偏置量进行匹配，于是，我们有如下的表示方式： \\[ \\omega = \\begin{bmatrix} \\omega_1 &amp; \\omega_2 &amp; \\cdots &amp; \\omega_n &amp; b\\end{bmatrix} \\] \\[ x = \\begin{bmatrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n &amp; 1\\end{bmatrix} \\] 于是，原逻辑回归公式可以有以下的表达： \\[ P(Y = 1 | x) = \\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} \\tag{4} \\] 四、损失函数 很容易想到，损失函数我们仍然可以使用前面介绍的差方和的方法计算。当距离目标越近时，差方和越小，损失就会越小。事实上并不能这样进行处理。 \\[ J(\\omega, b) = \\sum_i( \\frac{1}{1 + e^{-(\\omega \\cdot x_i)}} - y_i)^2 = \\sum_i (\\frac{e^{\\omega \\cdot x}}{1 + e^{\\omega \\cdot x}} - y_i) ^2\\tag{4} \\] 原因是如果使用差方和作为最后的损失函数，那么我们得到的最后的损失函数并不是一个简单的凹函数（或者凸函数），这个函数存在许多的局部极小值，因此很难通过梯度下降的方法收敛到全局最小值附近，这样导致的结果就是训练出来的模型并不能很好的满足我们的需要，误差较大。 所以我们必须要重新定义一个满足我们条件的损失函数，或者称为目标函数。我们考虑使用极大似然估计的方法进行参数估计。 对于其中的某一个样本，如果该样本的标签为1，那么我们需要极大化\\(P(Y = 1|x)\\)，如果该样本的标签为0，那么我们需要极大化\\(1 - P(Y = 1|x)\\)，于是对于每一个样本数据，综合来看，我们只需要极大化以下的式子即可： \\[ P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\] 上面的式子看起来很吓人，其实本质很简单。于是我们的似然函数可以表示为 \\[ L(\\omega) = \\prod_i P(Y = 1|x_i)^{y_i} (1 - P(Y= 1|x_i))^{1 - y_i} \\tag{5} \\] 由于这里涉及指数，而且是连乘运算，计算不方便，于是我们可以用取对数的方式进行处理，这里可以这样处理的原因是上式取最大的时候，其对数也一定是取最大值，因为对数函数是一个单调函数。于是有： \\[ log L(\\omega) = \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\tag{6} \\] 现在我们可以将之前的计算结果带入到公式(6)中进行化简： \\[ \\begin{aligned} logL(\\omega) &amp;= \\sum_i y_i log(P(Y = 1|x_i)) + (1 - y_i) log(1 - P(Y = 1|x_i)) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log(1 - \\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + (1 - y_i) log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) + log( \\frac{1}{1 + e^{\\omega \\cdot x_i}}) - y_i log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i (log(\\frac{e^{\\omega \\cdot x_i}}{1 + e^{\\omega \\cdot x_i}}) - log(\\frac{1}{1 + e^{\\omega \\cdot x_i}})) + log(\\frac{1}{1 + e^{\\omega \\cdot x_i}}) \\\\ &amp;= \\sum_i y_i log(e^{\\omega \\cdot x_i}) - log(1 + e^{\\omega \\cdot x_i}) \\\\ &amp;= \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\end{aligned} \\tag{7} \\] 于是我们需要的似然函数就变成了： \\[ logL(\\omega) = \\sum_i y_i \\omega \\cdot x_i - log(1 + e^{\\omega \\cdot x_i}) \\tag{8} \\] 上面的似然函数并不能直接进行最优化求解，于是我们常常利用梯度下降的方法进行逐步迭代求解，这就需要对上面的公式进行求导的操作，我们对参数\\(\\omega\\)求导如下： \\[ \\begin{aligned} \\frac{\\partial logL(\\omega)}{\\partial \\omega} &amp;= \\sum_i y_i x_i - \\frac{1}{1+e^{\\omega \\cdot x_i}} e^{\\omega \\cdot x_i} x_i \\\\ &amp;= \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\end{aligned} \\tag{9} \\] 以上就是利用梯度下降算法进行逻辑回归问题的求解的（偏）导数计算公式，在每一轮的迭代过程中，我们对所有的样本进行梯度计算，最后累加梯度，然后按照计算出的梯度信息更新需要的参数。 由于我们这里需要将似然函数极大化，因此和反向传播的梯度下降不同，这里使用的是类似的梯度上升的方法，于是我们有如下的迭代公式：这里的\\(\\alpha\\)指的是学习率（或者说是步长信息） \\[ \\omega := \\omega + \\alpha \\frac{\\partial logL(\\omega)}{\\partial \\omega} \\tag{10} \\] 五、关于逻辑回归的似然函数 在利用不同的方式计算损失函数时，我们总是希望得到的损失函数时一个凸函数（或者凹函数），这样我们就可以保证只有一个全局的最优解，而且不存在局部极小值或者极大值，而这些条件都对我们使用梯度下降方法来求解最优化问题十分有利。如果一个函数的二阶导数总是恒大于0，我们称这个函数为凸函数，如果一个函数的二阶导数总是恒小于0，我们称这个函数为凹函数，凸函数一定存在一个全局最小值，凹函数一定存在一个全局最大值，并且不管是凹函数还是凸函数都不存在局部极小值或者局部最大值。 我们以\\(y = x^2\\)为例，经过计算我们得到它的二阶导数为\\(y\\prime\\prime = 2\\)，是一个大于0的常数，因此该函数是一个凸函数，其不存在各种局部最小值或者局部最大值，只有一个全局最小值0（当然这个全局最小值也可以看作是一个某一个区域内的局部最小值）。 现在我们重新审视一下我们的似然函数，之前我们已经求解出了似然函数的梯度信息（一阶导数），即： \\[ \\frac{\\partial logL(\\omega)}{\\partial \\omega} = \\sum_i y_i x_i - \\frac{e^{\\omega \\cdot x_i} x_i}{1+e^{\\omega \\cdot x_i}} \\tag{11} \\] 我们继续对上式进行求导的操作，有： \\[ \\begin{aligned} \\frac{\\partial^2 logL(\\omega)}{\\partial \\omega^2} &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i (1 + e^{e^{\\omega \\cdot x_i}}) - e^{\\omega \\cdot x_i}(e^{\\omega \\cdot x_i} x_i)}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - x_i \\frac{e^{\\omega \\cdot x_i} x_i}{(1 + e^{\\omega \\cdot x_i})^2} \\\\ &amp;= \\sum_i - \\frac{e^{\\omega \\cdot x_i}}{(1 + e^{\\omega \\cdot x_i})^2} x_i \\cdot x_i \\end{aligned} \\tag{12} \\] 可以发现的是，上式对于任何的一个数据集合来说都是恒小于0的，因此可以说当我们使用极大似然估计作为损失函数时，该函数是一个凹函数，有一个最大值，可以利用梯度下降（严格来说这个应该是梯度上升的方法）进行求解。 通常，当我们求解出最终的参数时，可以获得一个超平面。往往我们将逻辑回归的阈值设置为0.5，将输出值高于0.5的样本标记为正样本，将输出值低于0.5的样本标记为负样本，于是，我们可以得到分类的超平面为： \\[ \\frac{1}{1 + e^{\\omega \\cdot x}} = \\frac{1}{2} \\tag{13} \\] 化简之后，我们可以得到最终的超平面的表达式为： \\[ \\omega \\cdot x = 0 \\tag{14} \\] 六，代码 这里的数据使用的是《机器学习实战》的数据，一共有100组数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100-0.017612 14.053064 0-1.395634 4.662541 1-0.752157 6.538620 0-1.322371 7.152853 00.423363 11.054677 00.406704 7.067335 10.667394 12.741452 0-2.460150 6.866805 10.569411 9.548755 0-0.026632 10.427743 00.850433 6.920334 11.347183 13.175500 01.176813 3.167020 1-1.781871 9.097953 0-0.566606 5.749003 10.931635 1.589505 1-0.024205 6.151823 1-0.036453 2.690988 1-0.196949 0.444165 11.014459 5.754399 11.985298 3.230619 1-1.693453 -0.557540 1-0.576525 11.778922 0-0.346811 -1.678730 1-2.124484 2.672471 11.217916 9.597015 0-0.733928 9.098687 0-3.642001 -1.618087 10.315985 3.523953 11.416614 9.619232 0-0.386323 3.989286 10.556921 8.294984 11.224863 11.587360 0-1.347803 -2.406051 11.196604 4.951851 10.275221 9.543647 00.470575 9.332488 0-1.889567 9.542662 0-1.527893 12.150579 0-1.185247 11.309318 0-0.445678 3.297303 11.042222 6.105155 1-0.618787 10.320986 01.152083 0.548467 10.828534 2.676045 1-1.237728 10.549033 0-0.683565 -2.166125 10.229456 5.921938 1-0.959885 11.555336 00.492911 10.993324 00.184992 8.721488 0-0.355715 10.325976 0-0.397822 8.058397 00.824839 13.730343 01.507278 5.027866 10.099671 6.835839 1-0.344008 10.717485 01.785928 7.718645 1-0.918801 11.560217 0-0.364009 4.747300 1-0.841722 4.119083 10.490426 1.960539 1-0.007194 9.075792 00.356107 12.447863 00.342578 12.281162 0-0.810823 -1.466018 12.530777 6.476801 11.296683 11.607559 00.475487 12.040035 0-0.783277 11.009725 00.074798 11.023650 0-1.337472 0.468339 1-0.102781 13.763651 0-0.147324 2.874846 10.518389 9.887035 01.015399 7.571882 0-1.658086 -0.027255 11.319944 2.171228 12.056216 5.019981 1-0.851633 4.375691 1-1.510047 6.061992 0-1.076637 -3.181888 11.821096 10.283990 03.010150 8.401766 1-1.099458 1.688274 1-0.834872 -1.733869 1-0.846637 3.849075 11.400102 12.628781 01.752842 5.468166 10.078557 0.059736 10.089392 -0.715300 11.825662 12.693808 00.197445 9.744638 00.126117 0.922311 1-0.679797 1.220530 10.677983 2.556666 10.761349 10.693862 0-2.168791 0.143632 11.388610 9.341997 00.317029 14.739025 0 前两列为数据，最后一列为对应数据的标签。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import matplotlib.pyplot as pltimport numpy as npdef read_file(): positive = [] negative = [] f = open(\"testSet.txt\", \"r\") for line in f: l = line.strip() l = l.split(\"\\t\") l = [float(i) for i in l] if l[-1] == 1.0: positive.append([l[0], l[1], 1]) else: negative.append([l[0], l[1], 1]) positive = np.array(positive, dtype=np.float) negative = np.array(negative, dtype=np.float) return positive, negativedef cost(positive, negative, w): c = 0 for i in positive: c += 1.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: c += 0.0 * np.sum(np.multiply(w, i)) - np.log(1 + np.exp(np.sum(np.multiply(w, i)))) return cdef loop(): positive, negative = read_file() w = np.array([1, 1, 1]) w_copy = w.copy() alpha = 0.001 for i in range(2000): print(cost(positive, negative, w)) gradient = np.array([0., 0., 0.]) for i in positive: gradient += 1.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) for i in negative: gradient += 0.0 * i - (np.exp(np.sum(np.multiply(w, i))) * i) / (1 + np.exp(np.sum(np.multiply(w, i)))) w = w + alpha * gradient return positive, negative, w, w_copyif __name__ == '__main__': positive, negative, w, w_origin = loop() plt.scatter(positive[:, 0], positive[:, 1], c=\"red\") plt.scatter(negative[:, 0], negative[:, 1], c='blue') xs = np.linspace(-4, 3.5, 300) ys1 = (w[0] * xs + w[2]) / (- w[1]) ys2 = (w_origin[0] * xs + w_origin[2]) / (- w_origin[1]) plt.plot(xs, ys1, c=\"green\") plt.plot(xs, ys2, c=\"yellow\") plt.show() 代码运行结果如下，其中黄色直线表示的是初始的分隔超平面，绿色的直线表示为分类超平面，可以看出，样本数据可以较好地被分隔开，第二幅图表示的是梯度上升的情况，可以看到，算法可以较好地收敛于全局最优解附近：","link":"/2019/05/10/Article3逻辑回归/"},{"title":"奇异值分解SVD","text":"前言 奇异值分解(Singular Value Decomposition，以下简称SVD)是在一种十分经典的无监督的机器学习算法，它可以用于处理降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。 特征值和特征向量 再了解SVD之前，对于矩阵的特征值和特征向量还是有必要做一个简单的回顾。 假设矩阵\\(A\\)是一个实对称矩阵，大小为\\(n \\times n\\)，如果有一个实数\\(\\lambda\\)和一个长度为\\(n\\)的向量\\(\\alpha\\)，满足下面的等式关系： \\[ A \\alpha = \\lambda \\alpha \\tag{1} \\] 我们就将\\(\\lambda\\)称为矩阵\\(A\\)的特征值，将\\(\\alpha\\)称为矩阵\\(A\\)的特征向量，是一个列向量。 由线性代数的相关知识我们可以知道，对于一个\\(n\\)阶的实对称矩阵，一定存在\\(n\\)个相互正交的特征向量。对于每一个特征向量，我们可以将其规范化，使其模长\\(|\\alpha|\\)为1。于是，我们有： \\[ \\alpha_i^T \\alpha_i = 1 \\\\ \\alpha_i^T \\alpha_j = 0, \\quad i \\ne j \\] 我们将所有的特征值按照从大到小的顺序进行排列，并将对应的特征的特征向量也按照特征值的大小进行排列，于是，我们会有以下的个排列： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_n \\\\ \\alpha_1, \\alpha_2, \\cdots, \\alpha_n \\] 上面的所有的特征值可以依次被放入一个$ n n\\(的对角矩阵的对角线元素中，我们不妨称这个对角矩阵为\\)​$，于是我们有： \\[ \\Sigma = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\ddots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ddots &amp; \\lambda_n \\\\ \\end{bmatrix} \\] 同样，我们也将上述的特征向量按照特征值的顺序进行排列，也组成了一个\\(n \\times n\\)的矩阵，不妨记作\\(W\\)，于是有： \\[ W = [\\alpha_1, \\alpha_2, \\cdots, \\alpha_n] \\] 综上，我们可以得到如下的关系式： \\[ A W = W \\Sigma \\tag{2} \\] 在等式的两边同时乘以\\(W^{-1}\\)，我们可以得到： \\[ A = W \\Sigma W^{-1} \\tag{3} \\] 考虑到特征向量彼此两两正交，模长为1，我们就有： \\[ W W^T = E \\tag{4} \\] 即： \\[ W^{-1} = W^T \\tag{5} \\] 将式(5)代入式(3)中，我们可以得到： \\[ A = W \\Sigma W^{-1} = W \\Sigma W^T \\tag{6} \\] 这是一个非常重要的关系，在前面我们就是利用这个关系来计算主成分分析PCA的。 奇异值分解SVD 在前面已经找到了一个重要的关系，在我们之前的讨论过程中，我们已经知道满足这个条件的矩阵的要求是该矩阵是一个实对称矩阵，如果只是一个普通的矩阵\\(A_{m \\times n}\\)，我们其实也可以通过类似的方法进行矩阵分解，这时候我们需要构造的矩阵等式如下： \\[ A = U \\Sigma V^T \\tag{7} \\] 其中，\\(U\\)是一个大小为$m m \\(的矩阵，\\)\\(是一个大小为\\)m n\\(的矩阵，\\)V^T\\(是一个大小为\\)n n$的矩阵。 矩阵\\(U\\) 的求解 定义了相关的矩阵等式关系之后，我们既需要求解其中的每一个矩阵的具体数值。首先来求解矩阵\\(U​\\)的数值。 首先构造矩阵\\(AA^T\\)，该矩阵是一个大小为\\(m \\times m\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\lambda_1, \\lambda_2, \\cdots, \\lambda_m \\\\ u_1, u_2, \\cdots, u_m \\] 其中每一对特征值和特征向量都满足： \\[ (AA^T) u_i = \\lambda_i u_i \\tag{7} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(U\\)， 即： \\[ U = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\] 矩阵\\(V\\) 的求解 和求解矩阵\\(U\\)类似，首先构造矩阵\\(A^T A\\)，该矩阵是一个大小为\\(n \\times n\\)的矩阵，然后我们对这个矩阵进行特征值和特征向量的求解，并按照对应特征值的大小对特征向量进行排序。我们这里设特征值序列和特征向量序列如下： \\[ \\sigma_1, \\sigma_2, \\cdots, \\sigma_n \\\\ v_1, v_2, \\cdots, v_n \\] 其中每一对特征值和特征向量都满足： \\[ (A^TA) v_i = \\sigma_i v_i \\tag{8} \\] 我们将这些特征向量组合成一个矩阵，就是我们需要的矩阵\\(V​\\)， 即： \\[ V = \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} \\] 矩阵\\(\\Sigma\\) 的求解 当求出矩阵\\(U​\\)和矩阵\\(V​\\)时，我们就可以求出矩阵\\(\\Sigma​\\)了： \\[ U \\Sigma V^T = A\\\\ \\Sigma = U^{-1} A (V^T)^{-1} \\] 考虑到矩阵\\(U\\)和矩阵\\(V\\)的特殊性质，即\\(U^{-1} = U^T\\)，\\(V^{-1} = V^T\\)，代入上面的式子中，我们有： \\[ \\Sigma = U^T A V \\tag{9} \\] 原理 其实，以上的步骤并不是随便得来的，是有严格证明的，下面以证明矩阵\\(V​\\)为例进行说明。 首先，我们等式\\(A = U \\Sigma V^T\\)左右两边分别求矩阵转置，得到： \\[ A^T = V \\Sigma^T U^T \\] 接着有： \\[ \\begin{aligned} A^T A &amp;= (V \\Sigma^T U^T)(U \\Sigma V^T) \\\\ &amp;= V \\Sigma^T U^T U \\Sigma V^T \\\\ &amp;= V \\Sigma^T \\Sigma V^T \\\\ &amp;= V (U^T A V)^T (U^T A V) V^T \\\\ &amp;= V (V^T A^T U)(U^T A V) V^T \\\\ &amp;= V V^T A^T U U^T A V V^T \\\\ &amp;= A^T A \\end{aligned} \\] 注意到上面证明过程的第三行，我们有$A^T A = V ^T V^T \\(，而矩阵\\)V\\(是矩阵\\)A^T A\\(的特征向量组成的矩阵，因此，矩阵\\)^T \\(就是矩阵\\)A^T A\\(的特征值组成的对角矩阵，这又给了我们一个新的求解\\)$矩阵的方法。 当我们求出所有的 \\(A^T A\\) 矩阵的特征值之后，我们在这些特征值的基础上进行开方操作，并将结果组成一个对角矩阵，这个对角矩阵就是我们需要的 \\(\\Sigma\\) 矩阵。 所以我们发现，实际上\\(\\Sigma\\)是一个对角矩阵，只在对角线上存在着有效元素，其他部位的元素都为0。我们将矩阵\\(\\Sigma\\)的对角线上的元素称为奇异值。 同样的道理，如果我们计算\\(A A^T\\)，我们也能得到相同的结果，而计算过程就是证明矩阵\\(U\\)的正确性的过程。 接下来，我们考虑这样一件事，我们已经求解出了所有的矩阵信息，有\\(A = U \\Sigma V^T\\)，我们同时在等式的左右两边乘以\\(V\\)，有： \\[ AV = U \\Sigma V^T V = U \\Sigma \\] 即： \\[ A \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n\\end{bmatrix} = \\begin{bmatrix} u_1 &amp; u_2 &amp; \\cdots &amp; u_m\\end{bmatrix} \\begin{bmatrix} a_1 &amp; \\; &amp; \\; \\\\ \\; &amp; a_2 \\; &amp; \\\\ \\; &amp; \\; &amp; \\ddots \\end{bmatrix}_{m \\times n} \\] 其中，\\(a_i\\) 表示的是 \\(\\Sigma\\) 矩阵的对角线元素，所以有： \\[ A v_i = u_i a_i \\] 故： \\[ a_i = \\frac{A v_i}{u_i} \\tag{9} \\] 上述也是一种求解\\(\\Sigma\\)矩阵的方法。 SVD的使用 接下来，我们通过一个简单的例子来实际使用以下SVD。假设我们的数据矩阵\\(A\\)如下： \\[ A = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] 接下来我们计算\\(A^T A\\)和\\(A A^T\\)，有： \\[ AA^T = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\\\ A^TA = \\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 2 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 2 \\\\2 &amp; 5 \\end{bmatrix} \\] 对于矩阵\\(AA^T\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\quad \\lambda_1 = 6, u_1 = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} \\\\ \\frac{1}{\\sqrt{30}} \\\\ \\frac{2}{\\sqrt{30}} \\end{bmatrix} \\quad \\lambda_2 = 1, u_2 = \\begin{bmatrix} 0 \\\\ \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\quad \\lambda_3 = 0, u_3 = \\begin{bmatrix} -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{bmatrix} \\] 对于矩阵\\(A^TA\\)，我们可以求解出它的所有的特征值和特征向量，如下： \\[ \\lambda_1 = 6, v_1 = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}}\\end{bmatrix} \\quad \\lambda_2 = 1, v_2 = \\begin{bmatrix} \\frac{2}{\\sqrt{5}} \\\\ -\\frac{1}{\\sqrt{5}}\\end{bmatrix} \\] 由上面的特征值信息，我们可以求出矩阵\\(\\Sigma\\)的对角线元素，即直接在特征值的基础上取根号即可，依次为： \\[ a_1 = \\sqrt{6}, a_2 = 1 \\] 由上面的所有计算结果，我们可以得到矩阵\\(U\\)，\\(V\\)和\\(\\Sigma\\)，如下： \\[ U = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\] \\[ V = \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] \\[ \\Sigma = \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\] 于是，我们的原始矩阵\\(A\\)可以分解成如下： \\[ A = U \\Sigma V^T \\] 即： \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{\\sqrt{30}} &amp; 0 &amp; -\\frac{1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{30}} &amp; \\frac{2}{\\sqrt{5}} &amp; \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{30}} &amp; -\\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{6}}\\end{bmatrix} \\begin{bmatrix} \\sqrt{6} &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{5}} &amp; \\frac{2}{\\sqrt{5}} \\\\ \\frac{2}{\\sqrt{5}} &amp; -\\frac{1}{\\sqrt{5}} \\end{bmatrix} \\] 在前面的这个求解的例子中，我们很难发现SVD究竟有什么用处，毕竟求解矩阵乘积，矩阵的特征值和特征向量都是十分复杂是过程。实际上我们在求解奇异值的时候，可以利用矩阵的特征值进行求解，这样奇异值就和特征值具有相同的特点，如果按照从大到小的顺序排列奇异值，我们发现奇异值的数值下降很快，在有些情况下，奇异值前10%~15%的数值之和已经占据了全部奇异值数值之和的90%以上，而过于小的奇异值我们可以将其省略，因为奇异值太小，对最后的生成矩阵影响就会非常小，可以直接忽略不计。当我们采用这种策略的时候，我们可以适当的选取数值最大的几个奇异值，并选取对应的特征向量，这个时候我们就可以只是用这些数据来重构我们的原始矩阵。 下面是一个具体的使用SVD重构数据矩阵的python代码实例，数据来源为《机器学习实战》一书。在代码中，我们试图去利用SVD重构一个01矩阵组成的数字0。 数据如下： 12345678910111213141516171819202122232425262728293031320000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)# 调整特征向量，使得所有向量的分量之和为正数。def adjust(vectors): values = np.sum(vectors, axis=0) for i, v in enumerate(values): if v &lt; 0: vectors[:, i] *= -1 return vectors# 利用numpy的求解特征值和特征向量的函数进行求解。def eig(matrix): values, vectors = np.linalg.eig(matrix) # 有时候函数会返回复数值，这个时候我们只需要取其实数部分。 values = np.real(values) vectors = np.real(vectors) # 按照特征值的大小进行排序。 index = np.argsort(values)[::-1] values = values[index] vectors = adjust(vectors[:, index]) # 由于求解过程存在一定的误差，因此特征值会出现极小的负数，我们可以直接将其置为0。 values = np.maximum(values, 0) return values, vectorsdef svd(matrix): # 返回左侧矩阵的特征值和特征向量 left_values, left_vectors = eig(np.matmul(matrix, np.transpose(matrix))) # 返回右侧矩阵的特征值和特征向量 right_values, right_vectors = eig(np.matmul(np.transpose(matrix), matrix)) # Sigma矩阵 sigma = np.zeros_like(matrix, dtype=np.float64) for i in range(min(len(left_values), len(right_values))): sigma[i][i] = math.sqrt(left_values[i]) printMat(np.matmul(np.matmul(left_vectors, sigma), np.transpose(right_vectors))) pass# 输出，这里借鉴了《机器学习实战》一书的输出模板def printMat(inMat, thresh=0.6): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(\"1\", end=\"\") else: print(\"0\", end=\"\") print()if __name__ == '__main__': matrix = load_data() svd(matrix) 结果如下： 12345678910111213141516171819202122232425262728293031320000000000000000001000000000000000000000000011011110000000000000000000000001111111100000000000000000000000111111111100000000000000000000000111111110111010000000000000000011011111111111100000000000000001111111111111101000000000000000001111000001111110000000000000000011110000011111100000000000001111110000000011110000000000000011111100000000111100000000000000111111000000000111100000000000001111110000000001111000000000000000111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000011110000000000000011111000000011110100000000000000111111000001111110000000000000001111111111111111000000000000000011111111111110000000000000000000111111111111100000000000000000000000111111111000000000000000000000001111111111000000000000000000000000111111000000000000 可以看到，我们自己实现的SVD可以较好地重构出原始数据矩阵。由于求解过程中的误差，我们的重构结果和实际的数据矩阵还是有些不同的，所以，numpy已经将SVD进行了封装，我们可以直接使用其封装好的函数进行求解SVD，这样求出的结果会比自己手动求解的结果好。 使用numpy中的SVD的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathdef load_data(): matrix = [] f = open(\"zero.txt\") for i in f: line = i.strip(\"\\n\") line = list(line) matrix.append([int(i) for i in line]) return np.array(matrix)def printMat(inMat, thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i, k]) &gt; thresh: print(1, end=\"\") else: print(0, end=\"\") print()def imgCompress(numSV=5, thresh=0.8): matrix = load_data() print(\"****original matrix******\") printMat(matrix, thresh) U, Sigma, VT = np.linalg.svd(matrix) SigRecon = np.zeros((numSV, numSV)) for k in range(numSV): # construct diagonal matrix from vector SigRecon[k, k] = Sigma[k] reconMat = np.matmul(np.matmul(U[:, :numSV], SigRecon), VT[:numSV, :]) print(\"****reconstructed matrix using %d singular values******\" % numSV) printMat(reconMat, thresh)if __name__ == '__main__': imgCompress() 结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566****original matrix******0000000000000011000000000000000000000000000011111100000000000000000000000001111111100000000000000000000000111111111100000000000000000000111111111111100000000000000000011111111111111100000000000000000011111111111111100000000000000000111111100001111100000000000000011111110000011111000000000000001111110000000011110000000000000011111100000000111110000000000000111111000000000111100000000000001111110000000001111000000000000001111110000000001111000000000000111111100000000011110000000000001111110000000000111100000000000001111100000000001111000000000000111111000000000011110000000000000111110000000000111100000000000001111100000000011111000000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000001111100000000111110000000000000011111000000011111100000000000000111111000001111110000000000000000111111111111111100000000000000000111111111111111000000000000000001111111111111110000000000000000001111111111110000000000000000000001111111111000000000000000000000000111111000000000000****reconstructed matrix using 5 singular values******0000000000000000000000000000000000000000000000111100000000000000000000000000001111100000000000000000000000001111111000000000000000000000111111111111000000000000000000001111111111111000000000000000000011111111111111000000000000000000111111000001111000000000000000001111100000001110000000000000000111110000000001110000000000000011111100000000011110000000000000111111000000000111100000000000001111110000000001111000000000000000111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001111000000000000011111000000000011110000000000000111110000000000111100000000000001111100000000001110000000000000001111100000000011110000000000000011111000000000111100000000000000111110000000001111000000000000001111100000000011110000000000000011111000000000111000000000000000111110000000111100000000000000000011111111111111100000000000000000011111111111111000000000000000000111111111111110000000000000000000111111111100000000000000000000001111111110000000000000000000000000111110000000000000 可以发现，当我们只使用5个奇异值以及其对应的特征向量的时候，我们就可以获得非常好的重构效果，这个时候，我们的数据量为： \\[ s = 32 * 5 + 5 + 5 * 32 = 325 \\\\ compress\\_rate = \\frac{325}{32 * 32} = 37.1\\% \\] 我们的数据量仅仅是原来的三分之一，因此可以SVD很好的进行数据压缩。 SVD和PCA 在求解SVD的过程中，我们求解了两个特殊的矩阵\\(U\\) 和\\(V\\) ，实际上，\\(U\\) 被称为是左奇异矩阵，\\(V\\) 被称为是右奇异矩阵。 回想起在求解PCA的过程中，我们也需要求解右奇异矩阵，并利用右奇异矩阵进行了数据的降维，这就是右奇异矩阵的作用之一。当我们考察左奇异矩阵的时候，发现合理的使用右奇异矩阵本质上是对数据量进行了一定的减小，当我们拥有一个矩阵\\(A\\)，大小为\\(m \\times n\\)，其中，\\(m\\) 表示的是数据量，即行数，\\(n\\) 表示的是数据的维度数目，即列数，当我们对数据矩阵右乘右奇异矩阵的时候，列数可以根据选择的特征向量的数目进行减小，当我们当数据矩阵左乘左奇异矩阵的时候，行数可以根据选择的特征向量的数目进行减小。因此，左奇异矩阵可以压缩行数，右奇异矩阵可以压缩列数，综合在一起就可以对数据进行较为全面的压缩。","link":"/2019/05/10/Article4SVD/"},{"title":"EM算法","text":"前言 EM算法是一种可以求解含有隐变量的迭代算法，当我们在实际过程中收集数据的时候，并不一定会收集全部的有效信息。比如，当我们想统计全校学生的身高分布的时候，可以将全校学生的身高看作是一个正态分布，但是毕竟男生和女生之间身高的分布还是有些不同的，但是万一我们没有对性别信息进行统计，而只是统计了身高信息的话，求得的概率分布的参数肯定会有较大的误差，这个时候，我们就需要将每一个样本的性别分布也考虑进去，从而希望获得更准确的概率分布估计。 一、准备工作 1、极大似然估计 极大似然估计我们并不陌生，在逻辑回归的求解过程中，我们就是用了极大似然估计，现在还是简单说明一下。 假设我们现在有一个概率分布，不妨记作\\(P(x;\\theta)​\\)，其中，\\(\\theta​\\)是未知参数，有可能是一个数值，也有可能是多个数值组成的参数向量，\\(x​\\)表示输入样本。现在我们想通过抽样的方式对参数\\(\\theta​\\)进行估计。假设我们一共采集了\\(N​\\)组数据，为\\(\\{x_1, x_2, \\cdots, x_N\\}​\\)。那么样本的联合概率函数可以表示为关于\\(\\theta​\\)的函数，即： \\[ L(\\theta) = L(x_1, x_2, \\cdots, x_N;\\theta) = \\prod_i^N P(x_i;\\theta) \\] \\(L(\\theta)​\\)是参数 \\(θ​\\) 的函数，随着 \\(θ​\\) 在参数变化，\\(L​\\)函数也在变化。而极大似然估计目的就是在样本\\(\\{x_1,...,x_N\\}​\\)固定的情况下，寻找最优的 \\(θ​\\) 来极大化似然函数： \\[ \\theta^{*} = \\mathop{\\arg\\max}_{\\theta}{L(\\theta)} \\] 上式在数学领域，可以看作是对 \\(θ^{*}\\)求解，求\\(L(θ)\\) 函数的极值点，导数为0处，即为 \\(θ*\\) 的点。 又因为\\(L(θ)​\\) 和 \\(ln(θ)​\\) 在同一个 \\(θ​\\) 处取得极值，我们可以对 \\(L(θ)​\\) 取对数，将连乘转化为连加(方便求导)，得到对数化似然函数： \\[ \\theta^*= \\mathop{\\arg\\max}_{\\theta}{ln\\;L(\\theta)} = \\mathop{\\arg\\max}_{\\theta} \\sum_i ln\\; P(x_i;\\theta) \\] 2、Jensen不等式 下图是一张描述Jensen不等式十分经典的图。 如果一个函数\\(f(x)\\)在其定义域内是一个连续函数，且其二阶导数恒小于等于0，我们称该函数在其定义域上是凹函数，反之，如果二阶导数恒大于等于0，我们称该函数在其定义域上是凸函数。 如果\\(f(x)\\)是一个凸函数，那么在其定义域上我们有: \\[ E(f(X)) \\geq f(E(X)) \\] 反之，如果\\(f(x)\\)是一个凹函数，在其定义域上我们有： \\[ E(f(X)) \\leq f(E(X)) \\] 其中，\\(E\\)表示对变量取期望。上面两个不等式当且仅当\\(X\\)是一个常量时可以取等号。 3、边缘分布 假设我们有两个随机变量，那么我们通过抽样，就会获得一个二维的联合概率分布\\(P(X=x_i,Y=y_j)\\)。 对每一个\\(X=x_i\\)，对其所有的\\(Y\\)进行求和操作，我们有： \\[ \\sum_{y_j}P(X=x_i, Y=y_j) = P(X=x_i) \\] 将上面的式子称之为\\(X=x_i\\)的边际分布（边缘分布）。 ​ 有了以上的一些基础准备，我们就可以来推导EM算法了。 二、EM算法 假设我们的数据集为： \\[ D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\] 其中 \\(x^{(i)}​\\) 是每一个具体的输出实例，表示每一次独立实验的结果，\\(N​\\)表示独立实验的次数。 我们设样本的概率分布函数为\\(P(x^{(i)};\\theta)​\\)，其中\\(\\theta​\\)是模型中的待估参数，可以是一个变量，也可以是多个变量所组成的参数向量。 根据极大似然估计，我们有： \\[ L(\\theta) = \\prod_{i}P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{1} \\] 两边同时取对数： \\[ ln\\;L(\\theta) = \\sum_{i} ln \\; P(x^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{2} \\] 此时，我们可以将 \\(P(x^{(i)}; \\theta)\\)看作是关于隐变量的一个边缘分布，即（我们假设隐变量为\\(Z\\)）： \\[ ln \\; L(\\theta) = \\sum_i ln \\; \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\quad 1 \\leq i \\leq N \\tag{3} \\] 这里我们利用了边缘分布的相关等式，即： \\[ P(x^{(i)}; \\theta) = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] 在上面的式子中，\\(z​\\)是一个隐藏变量，必然也会满足一个特定的概率分布，我们不妨把这个分布记作\\(Q_{i}(z^{(i)})​\\)，显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1​\\)。这里的下标和上标\\(i​\\)表示的是第\\(i​\\)个样本。故我们将上式改写成： \\[ \\begin{aligned} ln \\; L(\\theta) &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\\\ &amp;=&amp; \\sum_i ln \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\end{aligned} \\tag{4} \\] 现在，我们把如下的部分单独拿出来： \\[ Q_i(z^{(i)}) = p(z^{(i)}) \\\\ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = f(z^{(i)}) \\] 很显然，我们有\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = \\sum_i p(z^{(i)}) = 1\\)，所以，我们可以将上式写成： \\[ ln \\; L(\\theta) = \\sum_i ln \\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)}) \\tag{5} \\] 可以看出，我们的\\(\\sum_{z^{(i)}} p(z^{(i)}) f(z^{(i)})\\)实际上实在对\\(f(z^{(i)})\\)计算期望，其中\\(p(z^{(i)})\\)是函数\\(f(z^{(i)})\\)的概率分布函数，于是，我们可以把上面的式子记作： \\[ E[f(z^{(i)})] = \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} \\tag{6} \\] 于是，我们的似然函数就变成了： \\[ ln \\; L(\\theta) = \\sum_i ln \\;(E[f(z^{(i)})]) = \\sum_i ln \\; (E[\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}]) \\tag{7} \\] 这个时候就是Jensen不等式出场的时候了。 我们观察到函数\\(g(x)=ln(x)\\)，它的一阶导数是\\(g'(x) = \\frac{1}{x}\\)，二阶导数是\\(g''(x) = - \\frac{1}{x^2}\\)恒小于0，因此\\(g(x) = ln(x)\\)是一个凹函数，此时，我们利用Jensen不等式处理\\(ln \\;L(\\theta)\\)，有： \\[ \\begin{aligned} H(Y|X)&amp; =\\sum_{x\\in X} p(x)H(Y|X) \\\\ &amp; =-\\sum_{x\\in X} p(x)\\sum_{y\\in Y}p(y|x)\\log p(y|x)\\\\ &amp; =-\\sum_{x\\in X} \\sum_{y\\in Y}p(y,x)\\log p(y|x) \\end{aligned} \\] 故：我们根据Jensen不等式，有了以下的一个重要的不等式关系： \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{9} \\] 需要注意的是，我们使用Jensen不等式的时候，是对\\(z^{(i)}\\)的分布使用的，而\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}))\\)是函数\\(ln \\; L(\\theta)\\)的一个下界，所以实际上，\\(ln \\; L(\\theta)\\)包含了两个参数变量，一个是\\(\\theta\\)， 一个是隐藏变量\\(z^{(i)}\\)，所以我们需要弄清楚调整\\(\\theta\\)和\\(z^{(i)}\\)的区别。 由于Jensen不等式处理的是\\(z^{(i)}\\)，因此当我们调整\\(z^{(i)}\\)的时候，我们实际上实在调整似然函数\\(ln \\; L(\\theta)\\)的下界，使得似然函数\\(ln \\; L(\\theta)\\)的下界一点一点上升，最终于此时的似然函数\\(ln \\; L(\\theta)\\)的值相等。 然后，固定\\(z^{(i)}\\)的数值，调整\\(\\theta\\)的时候，就可以将\\(z^{(i)}\\)看作是一个已知变量，这个时候就可以利用极大似然估计的方法对\\(\\theta\\)参数的值进行计算，此时会得到一个新的\\(\\theta\\)的值，不妨记作\\(\\theta'\\)。我们这个时候再根据这个新的\\(\\theta'\\)的值，重新调整\\(z^{(i)}\\)，使得函数的下界一点一点上升，达到和\\(ln \\; L(\\theta)\\)相同之后，再固定\\(z^{(i)}\\)，更新\\(\\theta\\)的值。一直重复以上过程，直到似然函数收敛到某一个极大值\\(\\theta^*\\)处。 下图是一个很经典的关于EM算法的迭代过程示意图。（图片来自网络） 在上面的求解过程中，只有当此时的函数下界等于当前\\(\\theta\\)的对数似然函数时，才能保证当我们优化了这个下界的时候，才真正优化了目标似然函数。 \\[ ln \\; L(\\theta) \\geq \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\tag{10} \\] 在优化迭代的过程中，我们通过固定\\(\\theta​\\)并调整\\(z^{(i)}​\\)的可能分布，使得等式成立，即达到\\(ln \\; L(\\theta)​\\)的下界。根据Jensen不等式的条件，当\\(f(x)​\\)是一个凹函数的时候，有\\(f(E[X]) \\geq E[f(X)]​\\)，欲使等号成立，\\(X​\\)需要是一个常量。那么，在上面的式子中，我们有\\(X = \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}​\\)，故此时我们需要将\\(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})​\\)看作一个常数，不妨我们设这个常数为\\(C​\\)，于是我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})}) = C \\tag{11} \\] \\[ P(x^{(i)}, z^{(i)}; \\theta) = C Q_i(z^{(i)}) \\] \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\sum_{z^{(i)}} Q_i(z^{(i)}) \\tag{12} \\] 考虑到\\(Q_i(z^{(i)})\\)实际上是隐变量\\(z^{(i)}\\)的一个概率分布，满足： \\[ \\sum_{z^{(i)}} Q_i(z^{(i)}) = 1 ,\\quad Q_i(z^{(i)}) \\geq 0 \\] 于是，我们将\\(\\sum_{z^{(i)}} Q_i(z^{(i)}) = 1\\)代入到上面的式子(12)中，有： \\[ \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) = C \\tag{11} \\] 再将\\(C\\)带入到公式(11)中，我们有： \\[ \\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})} = C = \\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta) \\] \\[ \\begin{aligned} Q_i(z^{(i)}) &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{\\sum_{z^{(i)}} P(x^{(i)}, z^{(i)}; \\theta)} \\\\ &amp;= \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(x^{(i)};\\theta)} \\\\ &amp;= P(z^{(i)}|x^{(i)};\\theta) \\end{aligned} \\tag{12} \\] 即我们可以得到\\(Q_i(z^{(i)})\\)的值，也即我们得到\\(P(z^{(i)}|x^{(i)};\\theta)\\)的值，表示在当前的模型参数\\(\\theta\\)为定值时，在给定的\\(x^{(i)}\\)的条件下，得到\\(z^{(i)}\\)的概率大小。 至此，我们的EM算法的大部分情况进行了说明。首先，我们会对模型的参数\\(\\theta\\) 进行随机初始化，不妨记作\\(\\theta^0\\)。然后会在每一次的迭代循环中计算\\(z^{(i)}\\)的条件概率期望，这就是EM算法中的”E步“。最后再根据计算得到的概率分布，根据极大似然的方法计算在当前隐藏变量分布下的使得似然函数取得极大的\\(\\theta\\)的值，并进行更新，这就是EM算法中的”M步“。 观察到在”M步“中，我们有： \\[ ln \\; L(\\theta) = \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) \\cdot ln(\\frac{P(x^{(i)}, z^{(i)}; \\theta)}{Q_i(z^{(i)})})) \\] \\[ \\theta^{(j + 1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) - \\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)})) \\] 观察到在上面的式子中，\\(\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)})ln(Q_i(z^{(i)}))\\)对于整个优化的过程来说相当于是一个常数项，因此可以省略，于是，上式可以简写成： \\[ \\begin{aligned} \\theta^{(j + 1)} &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} Q_i(z^{(i)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\end{aligned} \\tag{13} \\] 公式(13)内部的\\(\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta))\\)就是EM算法的核心，我们一般将其称之为Q函数，通常记为：\\(Q(\\theta, \\theta^{(j)})\\)。 所以，我们的EM算法可以总结如下： 数据集为\\(D = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\}\\) ，随机初始化模型参数\\(\\theta\\)，记作\\(\\theta^{(0)}\\)。 对每一次迭代循环，\\(j = 0, 1, 2, 3, \\cdots, M​\\)，我们有： E步（E-Step）：在当前的模型参数\\(\\theta^{(j)}​\\)的条件下，计算联合分布的条件概率期望： \\[ Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\\theta^{(j)}) \\] M步（M-Step）：在计算出条件概率分布的期望的基础上，极大化似然函数，得到新的模型参数的值\\(\\theta^{(j+1)}​\\): \\[ \\theta^{(j+1)} = \\mathop{\\arg\\max}_{\\theta}\\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 如果\\(\\theta^{(j+1)}​\\)已经收敛，则跳出循环结束： 输出最后模型参数\\(\\theta​\\)的值。 三、EM算法解决三硬币模型 三硬币模型是EM算法的一个简单使用，问题请参考《统计学习方法》一书。 假设有三枚质量分布不均匀的硬币A、B、C，这些硬币正面出现的概率分别是\\(\\pi\\)、\\(p\\)、\\(q\\)。进行如下掷硬币试验： 先掷A，如果A是正面则再掷B，如果A是反面则再掷C。对于B或C的结果，如果是正面则记为1，如果是反面则记为0。进行N次独立重复实验，得到结果。现在只能观测到结果，不能观测到掷硬币的过程，估计模型参数\\(\\theta=(\\pi,p,q)​\\)。 由于实验一共进行了N次，每一次都是独立重复实验，那么我们可以将实验结果记录如下，其中每一次的实验结果是已知的： \\[ X = \\{x^{(1)}, x^{(2)}, \\cdots, x^{(N)}\\} \\quad x^{(i)} \\in \\{0, 1\\} \\] 每次独立实验都会产生一个隐藏变量\\(z^{(i)}\\)，这个隐藏变量是无法被观测到的，我们可以将其记录如下，这个隐藏变量的记录结果是未知的： \\[ Z = \\{z^{(1)}, z^{(2)}, \\cdots, z^{(N)}\\} \\quad z^{(i)} \\in \\{0, 1\\} \\] 对于第\\(i\\)次的独立重复实验，我们有： \\[ P(x^{(i)} = 0;\\theta) = \\pi(1-p)^{1-x^{(i)}} + (1-\\pi)(1-q)^{1-x^{(i)}} \\] \\[ P(x^{(i)}=1;\\theta) = \\pi p^{x^{(i)}} + (1-\\pi)q^{1-x^{(i)}} \\] 故，综合起来看，我们有： \\[ P(x^{(i)};\\theta) = \\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}} \\] 构造极大似然函数 我们可以构造我们的极大似然函数如下： \\[ \\begin{aligned} L(\\theta) &amp;= \\prod_i P(x^{(i)};\\theta) \\\\ &amp;= \\prod_i [\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\end{aligned} \\] 两边同时取对数，有： \\[ ln \\; L(\\theta) = \\sum_i ln\\;[\\pi p^{x^{(i)}} (1-p)^{1-x^{(i)}} + (1-\\pi)q^{x^{(i)}}(1-q)^{1-x^{(i)}}] \\] 构造我们的Q函数 在没有说明的情况下，我们使用下标表示第几次迭代过程，用上标表示第几个样本，\\(\\theta^{(j)}\\)的上标表示第\\(j\\)次迭代。 对于三硬币问题，我们的Q函数可以构造如下： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ \\end{aligned} \\] 故，我们需要求解\\(P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)这四个概率值。 求解极大值 \\[ \\begin{aligned} P(z^{(i)}=1|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=1;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= \\mu_j^{(i)} \\end{aligned} \\] 上式对于迭代过程来说是一个定值，我们使用符号\\(\\mu_j^{(i)}\\)来表示，上标\\((i)\\)表示的是第\\(i\\)个样本，下标\\(j\\)表示的是第\\(j\\)次迭代过程。 那么很明显，我们有： \\[ \\begin{aligned} P(z^{(i)}=0|x^{(i)};\\theta^{(j)}) &amp;= \\frac{P(x^{(i)}, z^{(i)}=0;\\theta^{(j)})}{P(x^{(i)});\\theta^{(j)})} \\\\ &amp;= \\frac{(1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(j)}}}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\\\ &amp;= 1 - \\mu_j^{(i)} \\end{aligned} \\] 接着，我们计算\\(P(x^{(i)}, z^{(i)}=1;\\theta)\\)，\\(P(x^{(i)}, z^{(i)}=0;\\theta)\\)： \\[ P(x^{(i)}, z^{(i)}=1;\\theta) = \\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}} \\] \\[ P(x^{(i)}, z^{(i)}=0;\\theta)=(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}} \\] 我们将上面的计算结果都带入到Q函数中，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\{P(z^{(i)} =1|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=1;\\theta) + P(z^{(i)} =0|x^{(i)};\\theta^{(j)})\\cdot ln\\;P(x^{(i)}, z^{(i)}=0;\\theta)\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot ln\\;[\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}] + (1-\\mu_j^{(i)})\\cdot ln\\; [(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}]\\} \\end{aligned} \\] 下一步就是对我们需要求解的变量进行求偏导数的操作，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial \\pi} &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}}{\\pi \\cdot p^{x^{(i)}} \\cdot (1-p)^{1-x^{(i)}}} + (1-\\mu_j^{(i)})\\cdot \\frac{-1 \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}{(1-\\pi) \\cdot q^{(i)}\\cdot (1-q)^{1-x^{(i)}}}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi} + (\\mu_j^{(i)}-1)\\cdot \\frac{1}{1-\\pi}\\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)}(\\frac{1}{\\pi} + \\frac{1}{1-\\pi}) - \\frac{1}{1-\\pi} \\} \\\\ &amp;= \\sum_i \\{\\mu_j^{(i)} \\cdot \\frac{1}{\\pi(1-\\pi)}\\} - \\frac{N\\pi}{\\pi(1-\\pi)} \\\\ &amp;= \\frac{1}{\\pi(1-\\pi)}\\{\\sum_i \\mu_j^{(i)} - N\\pi\\} \\end{aligned} \\] 令上式为0，我们有： \\[ \\sum_i \\mu_j^{(i)} - N\\pi = 0 \\] 即： \\[ \\pi = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] 同样的道理，我们可以计算出Q函数相对于\\(p\\)的偏导数，如下： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial p} &amp;= \\sum_i \\mu_j^{(i)} \\frac{x^{(i)} \\cdot p^{x^{(i)}-1} \\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-x^{(i)})\\cdot (1-p)^{-x^{(i)}}\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} + 0 \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\frac{\\frac{x^{(i)}}{p}\\cdot p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} + p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}} \\cdot \\frac{1}{1-p}\\cdot (1-x^{(i)})\\cdot (-1)}{p^{x^{(i)}}\\cdot (1-p)^{1-x^{(i)}}} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\{\\frac{x^{(i)}}{p} + \\frac{1-x^{(i)}}{p-1}\\} \\\\ &amp;= \\sum_i \\mu_j^{(i)} \\cdot \\frac{(p-1)\\cdot x^{(i)} + p(1-x^{(i)})}{p(p-1)} \\\\ &amp;= \\frac{1}{p(p-1)} \\sum_i \\mu_j^{(i)} \\{p-x^{(i)}\\} \\\\ &amp;= \\frac{1}{p(p-1)}\\{p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)}\\} \\end{aligned} \\] 令上式等于0，我们可以得到： \\[ p \\cdot \\sum_i \\mu_j^{(i)} - \\sum_i \\mu_j^{(i)} \\cdot x^{(i)} = 0 \\] 即： \\[ p = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] 同理，我们对\\(q\\)求偏导数，有： \\[ \\begin{aligned} \\frac{\\partial Q}{\\partial q} &amp;= \\sum_i (1-\\mu_j^{(i)})(\\frac{x^{(i)}}{q}+\\frac{1-x^{(i)}}{p-1}) \\\\ &amp;= \\sum_i (1-\\mu_j^{(i)})\\frac{q-x^{(i)}}{q(q-1)} \\\\ &amp;= \\frac{1}{q(q-1)}\\{q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} \\} \\end{aligned} \\] 令上式等于0，我们有： \\[ q\\cdot \\sum_i (1-\\mu_j^{(i)}) - \\sum_i (1-\\mu_j^{(i)})x^{(i)} =0 \\] 即： \\[ q = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 所以，以上就是我们解决三硬币模型的迭代公式的求解过程，公式汇总如下，这里加入了下标，表示新的一轮迭代变量： \\[ \\mu_j^{(i)} = \\frac{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})})}{\\pi_j \\cdot p_j^{x^{(i)}} \\cdot (1 - p_j^{(1-x^{(i)})}) + (1-\\pi_j) \\cdot q_j^{x^{(i)}} \\cdot (1-q_j)^{1-x^{(i)}}} \\] \\[ \\pi_{j+1} = \\frac{1}{N} \\sum_i \\mu_j^{(i)} \\] \\[ p_{j+1} = \\frac{\\sum_i \\mu_j^{(i)\\cdot x^{(i)}}}{\\sum_i \\mu_j^{(i)}} \\] \\[ q_{j+1} = \\frac{\\sum_i (1-\\mu_j^{(i)})x^{(i)}}{\\sum_i (1-\\mu_j^{(i)})} \\] 四、EM算法的收敛性 在之前的过程中，我们都是默认EM算法可以收敛到某一极大值附近，但是并没有给出十分严格的证明，所以，我们需要对EM的收敛性进行一定的验证。 由于我们是利用极大似然估计来估计参数的值，那么，我们只需要保证在每一次的迭代过程中，似然函数的数值都在上升即可，即下面的不等式成立： \\[ ln \\; L(\\theta^{(j+1)}) \\geq ln\\;L(\\theta^{(j)}) \\] 由于： \\[ P(x^{(i)};\\theta) = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}|x^{(i)};\\theta)} \\] 因此，两边取对数，我们有： \\[ ln \\; P(x^{(i)};\\theta) = ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta) \\] 对每一个样本进行累加，我们有： \\[ \\sum_i ln \\; P(x^{(i)};\\theta) = \\sum_i (ln\\; P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}|x^{(i)};\\theta)) \\] 根据我们构造的Q函数，我们有： \\[ Q(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\] 另，我们可以构造如下的一个函数，记作\\(H(\\theta, \\theta^{(j)})\\)，如下： \\[ H(\\theta, \\theta^{(j)}) = \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\] 我们将上面的两个函数相减，有： \\[ \\begin{aligned} Q(\\theta, \\theta^{(j)}) - H(\\theta, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(x^{(i)}, z^{(i)};\\theta)) \\\\&amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) (ln\\;P(x^{(i)}, z^{(i)};\\theta) - ln\\;P(z^{(i)}| x^{(i)};\\theta)) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;\\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(z^{(i)}| x^{(i)};\\theta)} \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)})ln\\;P(x^{(i)};\\theta) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) (\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) ) \\\\ &amp;= \\sum_i ln\\;P(x^{(i)};\\theta) \\\\ &amp;= ln \\;L(\\theta) \\end{aligned} \\] 在上面的式子中，第三行是利用了条件概率的公式，第五行则是利用了\\(\\sum_{z^{(i)}}P(z^{(i)}|x^{(i)};\\theta^{(j)}) = 1\\)的条件。 所以，我们构造出的两个式子，相减之后正好是我们的极大似然函数的对数。 于是，我们将\\(ln\\; L(\\theta^{(j+1)})\\)和\\(ln\\;L(\\theta^{(j)})\\)相减，我们有： \\[ \\begin{aligned} ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j+1)}, \\theta^{(j)})) - (Q(\\theta^{(j)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\\\ &amp;= (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\end{aligned} \\] 对于第一个括号内部的\\(Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})\\)，由于我们是通过极大化Q函数来更新参数的数值，所以\\(Q(\\theta^{(j+1)}, \\theta^{(j)}) \\geq Q(\\theta^{(j)}, \\theta^{(j)})\\)，故这一部分一定会大于等于0，即： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] 对于第二个括号内部的数值，我们有： \\[ \\begin{aligned} H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;\\quad- \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln (P(z^{(i)}| x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i \\sum_{z^{(i)}} P(z^{(i)}|x^{(i)};\\theta^{(j)}) ln\\; \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} \\\\ &amp;\\leq \\sum_i ln(\\sum_{z^{(i)}} \\frac{P(z^{(i)}| x^{(i)};\\theta^{(j+1)})}{P(z^{(i)}| x^{(i)};\\theta^{(j)})} P(z^{(i)}|x^{(i)};\\theta^{(j)})) \\\\ &amp;= \\sum_i ln (\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)})) \\\\ &amp;= \\sum_i ln(1) \\\\ &amp;= 0 \\end{aligned} \\] 在上面的第三步中，我们使用了Jensen不等式，在第五步中，我们使用了\\(\\sum_{z^{(i)}} P(z^{(i)}| x^{(i)};\\theta^{(j+1)}) = 1\\)这一条件。 于是，我们可以有： \\[ Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)}) \\geq 0 \\] \\[ H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)}) \\leq 0 \\] 故： \\[ (Q(\\theta^{(j+1)}, \\theta^{(j)})-Q(\\theta^{(j)}, \\theta^{(j)})) - (H(\\theta^{(j+1)}, \\theta^{(j)}) - H(\\theta^{(j)}, \\theta^{(j)})) \\geq 0 \\] 即： \\[ ln \\; L(\\theta^{(j+1)}) - ln\\;L(\\theta^{(j)}) \\geq 0 \\] 所以EM算法是可以逐步收敛到某一极大值附近的。证毕。 五、EM算法的缺陷 EM算法是处理含有隐藏变量模型的重要算法，但是EM算法也有其缺陷，首先，EM算法对初始值敏感，不同的初始值可能会导致不同的结果，这是由于似然函数的性质决定的，如果一个似然函数是凹函数，那么最后会收敛到极大值附近，也就是最大值附近，但是如果函数存在多个极大值，那么算法的初始值就会影响最后的结果。","link":"/2019/05/10/Article2EM Algorithm/"}],"tags":[{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"AdaBoost","slug":"AdaBoost","link":"/tags/AdaBoost/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/逻辑回归/"},{"name":"奇异值分解","slug":"奇异值分解","link":"/tags/奇异值分解/"},{"name":"SVD","slug":"SVD","link":"/tags/SVD/"},{"name":"EM算法","slug":"EM算法","link":"/tags/EM算法/"}],"categories":[{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"}]}